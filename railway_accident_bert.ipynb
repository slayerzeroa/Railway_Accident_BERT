{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "sbw_4qrPDWUl",
        "IBZ3UXZtjIbA",
        "Js1n6xKKlfxM",
        "Ppm736thjDGk",
        "CvtIIJici-n-"
      ],
      "authorship_tag": "ABX9TyPWReSEjdrCUDTBKNNB1qPx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/slayerzeroa/Railway_Accident_BERT/blob/main/railway_accident_bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MSCK6v6Km9qE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07b06039-7b04-4731-b1c4-479d2801779e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5 MB 13.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.0-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 54.8 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 36.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.0 tokenizers-0.13.2 transformers-4.24.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch-crf\n",
            "  Downloading pytorch_crf-0.7.2-py3-none-any.whl (9.5 kB)\n",
            "Installing collected packages: pytorch-crf\n",
            "Successfully installed pytorch-crf-0.7.2\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install nltk\n",
        "!pip install pytorch-crf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 라이브러리\n",
        "import torch\n",
        "from torch import nn, optim, tensor\n",
        "from torch.utils.data import DataLoader, Dataset,TensorDataset, random_split\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "import json\n",
        "import glob\n",
        "import re\n",
        "import os\n",
        "import ast\n",
        "import html\n",
        "\n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "RH5K6Y2c2UD6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#디바이스 세팅\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "IJwQl0n9F25R"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# google api key\n",
        "key = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "id": "XEIiZO-Yx9iK",
        "outputId": "61e60882-8992-4376-edf7-350c0c2d16b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-68da65c8-b92f-4238-9c3a-aafff83c97f2\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-68da65c8-b92f-4238-9c3a-aafff83c97f2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving railway-tagtog-d24e6901ddb0.json to railway-tagtog-d24e6901ddb0.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sample Annotation : JSON to csv\n",
        "by TAGTOG"
      ],
      "metadata": {
        "id": "sbw_4qrPDWUl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "\n",
        "#로그인 위치\n",
        "url = \"https://tagtog.com/-login\"\n",
        "\n",
        "#다운로드 위치\n",
        "file_url = 'https://www.tagtog.com/slayerzeroa/Railway_BERT/-downloads/dataset-as-anndoc' \n",
        "o_file = 'abc.zip'  \n",
        "if os.path.exists(o_file):\n",
        "    os.remove(o_file)\n",
        "\n",
        "#로그인 정보\n",
        "login_info = {\n",
        "    'loginid' : 'slayerzeroa@gmail.com',\n",
        "    'password' : 'gyysxw5rU3NzrYX',\n",
        "}\n",
        "\n",
        "#로그인\n",
        "with requests.Session() as s:\n",
        "    login_req = s.post(url, data=login_info)\n",
        "    r = s.get(file_url)\n",
        "\n",
        "    with open(o_file,\"wb\") as output:\n",
        "        output.write(r.content)"
      ],
      "metadata": {
        "id": "tEpJ0wJbsfa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#압축파일 풀기\n",
        "import zipfile\n",
        "import shutil\n",
        "folder_path = \"./tagtog_relation_extraction/\"\n",
        "\n",
        "zip_ = zipfile.ZipFile(\"abc.zip\")\n",
        "if os.path.exists(folder_path):\n",
        "    shutil.rmtree(folder_path)\n",
        "zip_.extractall(folder_path)"
      ],
      "metadata": {
        "id": "wvm1sisNsffS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import glob\n",
        "import re\n",
        "import os\n",
        "#폴더 경로\n",
        "folder_name = \"./tagtog_relation_extraction/Railway_BERT/\"\n",
        "\n",
        "#context list\n",
        "context_name_list = os.listdir(folder_name + \"plain.html/pool\")\n",
        "print(context_name_list)\n",
        "\n",
        "#relation 폴더 경로\n",
        "relation_folder_paths = glob.glob(folder_name + \"ann.json/master/pool\")\n",
        "\n",
        "#context 폴더 경로\n",
        "# contexts_folders_paths = glob.glob(folder_name + \"plain.html/pool/*\")\n",
        "contexts_folders_paths = [folder_name + \"plain.html/pool/\" + c for c in context_name_list]\n",
        "\n",
        "print(contexts_folders_paths)\n",
        "\n",
        "#anntation_lenged 정보\n",
        "annotation_legend = folder_name + \"annotations-legend.json\"\n",
        "with open(annotation_legend,\"r\") as f:\n",
        "    annotation_legend = json.load(f)\n",
        "\n",
        "print(annotation_legend)"
      ],
      "metadata": {
        "id": "2VNJGT64sfj_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce8d6f0d-4bb4-4153-ea00-afb08d1faeb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a3DnzcVSvJKG25OeznRpiW5zLgi8-sample.pdf.plain.html']\n",
            "['./tagtog_relation_extraction/Railway_BERT/plain.html/pool/a3DnzcVSvJKG25OeznRpiW5zLgi8-sample.pdf.plain.html']\n",
            "{'e_23': 'SOLUTION', 'r_19': 'mz4f90xj280(e_6|e_4)', 'r_26': '5zjoi2gg5ps(e_6|e_23)', 'e_22': 'ACCIDENT', 'r_25': '6oyjvcblqfe(e_22|e_23)', 'e_6': 'IMPLIED', 'r_27': 'f19f8drqjkn(e_4|e_23)', 'e_4': 'CAUSE', 'r_24': 'yz4ctet7cda(e_4|e_22)'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#relation dictionary 파일로부터 subject와 object의 entity 정보를 추출해주는 함수\n",
        "def get_needed_relation_data(tmp_relation):\n",
        "    subject_token = re.findall(\"\\(+(.+)+\\)\",annotation_legend[tmp_relation[\"relations\"][0]['classId']])[0].split(\"|\")[0]\n",
        "    if subject_token == tmp_relation['entities'][0]['classId']:\n",
        "        cause, implied = tmp_relation['entities']\n",
        "    else:\n",
        "        implied, cause  = tmp_relation['entities']\n",
        "    \n",
        "    # get preprocessed entities\n",
        "    def _get_entity(entity):\n",
        "        outputs = entity['offsets'][0]\n",
        "        outputs['type'] = annotation_legend[entity['classId']].split(\"-\")[0].lower()\n",
        "        return outputs\n",
        "    \n",
        "    output_cause = _get_entity(cause)\n",
        "    output_implied = _get_entity(implied)\n",
        "    return output_cause, output_implied"
      ],
      "metadata": {
        "id": "_22_zwi3nKno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#relation dictionary 파일로부터 title(relation)을 추출해주는 함수\n",
        "def get_label(relation_json):\n",
        "    label_tag = relation_json['relations'][0]['classId'] #r_6\n",
        "    try:\n",
        "        sub_type, label = annotation_legend[re.findall(\"\\(+(.+)+\\|\",annotation_legend[label_tag])[0]].split(\"-\"), label_tag\n",
        "        return f\"{sub_type}:{label}\"\n",
        "    except:\n",
        "        sub_type, = annotation_legend[re.findall(\"\\(+(.+)+\\|\",annotation_legend[label_tag])[0]].split(\"-\")\n",
        "        return f\"{sub_type}:no_relation\""
      ],
      "metadata": {
        "id": "nTlxq-0jnL0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#html 파일로부터 text만 추출해주는 함수\n",
        "def get_context_from_html(html_file):\n",
        "    html_file = re.sub('(<([^>]+)>)', '', html_file)\n",
        "    return html_file"
      ],
      "metadata": {
        "id": "-G02esnhnOAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#entity 정보가 포함된 sentence를 생성해주는 함수\n",
        "def get_sentence_with_entites(subject_entity, object_entity, sentence):\n",
        "    if subject_entity['start'] < object_entity['start']:\n",
        "        entity1,entity2 = subject_entity, object_entity\n",
        "    else:\n",
        "        entity1,entity2 = object_entity, subject_entity\n",
        "    \n",
        "    #entity 시작 위치 및 길이 \n",
        "    ett1_stt, ett1_len = entity1['start'], len(entity1['text'])\n",
        "    ett2_stt, ett2_len = entity2['start'], len(entity2['text'])\n",
        "    \n",
        "    #문장 분리\n",
        "    bf, ett1, mid, ett2, af = sentence[:ett1_stt], \\\n",
        "                            sentence[ett1_stt:ett1_stt+ett1_len], \\\n",
        "                            sentence[ett1_stt+ett1_len:ett2_stt], \\\n",
        "                            sentence[ett2_stt:ett2_stt+ett2_len], \\\n",
        "                            sentence[ett2_stt+ett2_len:]\n",
        "\n",
        "    \n",
        "    if subject_entity['start'] < object_entity['start']:\n",
        "        ett1,ett2 = f\"<sbj:{ett1}>\", f\"<obj:{ett2}>\"\n",
        "    else:\n",
        "        ett1,ett2 = f\"<obj:{ett1}>\", f\"<sbj:{ett2}>\"\n",
        "    \n",
        "    return \"\".join([bf, ett1, mid, ett2, af])"
      ],
      "metadata": {
        "id": "DmOMMJ0jnQNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dataframe column\n",
        "# id : context title(e.g : 카카오게임, 11번가 등)\n",
        "# sentence w/o entity\n",
        "# sentence w entity\n",
        "# subject_entity\n",
        "# object_entity\n",
        "# class\n",
        "\n",
        "id_list = []\n",
        "sentence_list = []\n",
        "sentence_with_entities_list = []\n",
        "subject_entity_list = []\n",
        "object_entity_list = []\n",
        "relation_list = []\n",
        "# tagtog 데이터를 CSV 형태로 변경\n",
        "for context_name, relation_folder, contexts_folder in zip(context_name_list, relation_folder_paths, contexts_folders_paths):\n",
        "    # relation files와 context files 리스트 출력\n",
        "    file_ids = [file_name for file_name in os.listdir(relation_folder)]\n",
        "    file_nums = [ids.split(\"-\")[1] for ids in file_ids]\n",
        "    relation_files = [relation_folder + \"/\"+ file_id for file_id in file_ids]\n",
        "    context_files = [contexts_folder for file_id in file_ids]\n",
        "\n",
        "    #json으로 된 relation data와 html로 된 context 데이터 읽기\n",
        "    for relation_file, context_file, file_num in zip(relation_files,context_files, file_nums):\n",
        "        #subject, object 정보 추출\n",
        "        with open(relation_file, \"r\") as f:\n",
        "            relation_json = json.load(f)\n",
        "            tmp_subject, tmp_object = get_needed_relation_data(relation_json) #subject, object\n",
        "            tmp_label = get_label(relation_json)\n",
        "        try:\n",
        "            tmp_subject, tmp_object = get_needed_relation_data(relation_json) #subject, object\n",
        "            tmp_label = get_label(relation_json)\n",
        "        except:\n",
        "            continue\n",
        "        #sentence, sentence with entities 정보 추출\n",
        "        with open(context_file, \"r\") as f:\n",
        "            context_json = f.read()\n",
        "        tmp_sentence = get_context_from_html(context_json)\n",
        "        tmp_sentence_w_entities = get_sentence_with_entites(tmp_subject,tmp_object,tmp_sentence)\n",
        "        \n",
        "        #각 list에 데이터 저장\n",
        "        id_list.append(f\"{context_name}_{file_num}\")\n",
        "        sentence_list.append(tmp_sentence)\n",
        "        sentence_with_entities_list.append(tmp_sentence_w_entities)\n",
        "        subject_entity_list.append(tmp_subject)\n",
        "        object_entity_list.append(tmp_object)\n",
        "        relation_list.append(tmp_label.lower())"
      ],
      "metadata": {
        "id": "MVB1BCtBnWRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(subject_entity_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YW8gLRzYrOoz",
        "outputId": "af4a6fbb-1904-45df-cf21-e258b6a514d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'start': 3704, 'text': '화차에 컨테이너를 싣고 내릴 때 차축 베어링에 충격이 가해진 것', 'type': 'implied'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 참고 자료 : \n",
        "\n",
        "# !pip install gspread\n",
        "# !pip install oauth2client\n",
        "\n",
        "import gspread\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "\n",
        "scope = ['https://spreadsheets.google.com/feeds',\n",
        "         'https://www.googleapis.com/auth/drive']\n",
        "\n",
        "#json key file 위치\n",
        "json_file_name = './railway-tagtog-d24e6901ddb0.json'\n",
        "\n",
        "# json key file을 이용하여 접속\n",
        "credentials = ServiceAccountCredentials.from_json_keyfile_name(json_file_name, scope)\n",
        "gc = gspread.authorize(credentials)\n",
        "\n",
        "#구글 스프레드 시트 주소\n",
        "spreadsheet_url = \"https://docs.google.com/spreadsheets/d/1cpt2ad2pw0N2tC1H3MPtRAaZzFmgCh2If0gHOQKEIIo/edit#gid=0\"\n",
        "\n",
        "# 스프레드시트 문서 가져오기\n",
        "doc = gc.open_by_url(spreadsheet_url)\n",
        "## gc.create(spreadsheet_name) # 스프레드시트 생성\n",
        "\n",
        "\n",
        "#시트 선택하기\n",
        "sheet_name = \"annotation\"\n",
        "worksheet = doc.worksheet(sheet_name) #해당 시트가 있는 경우 불러오기\n",
        "## 403 error가 뜰 경우, google sheets API를 활성시켜줘야 함"
      ],
      "metadata": {
        "id": "PveqSBqVndUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 시트의 모든 데이터 가져오기\n",
        "values = worksheet.get_all_values()\n",
        "header, rows = values[0], values[1:]\n",
        "data = pd.DataFrame(rows, columns=header)\n",
        "column_list = [\"id\",\"sentence\",\"sentence_with_entity\",\"subject_entity\",\"object_entity\",\"class\"]\n",
        "data = data[column_list]\n",
        "data.head()\n",
        "\n",
        "sen_list =  list(data.sentence_with_entity.values)\n",
        "len(sen_list)\n",
        "\n",
        "worksheet.resize(len(id_list)+1,10)\n",
        "list_range = f\"a2:f{len(id_list)+1}\"\n",
        "cell_list = worksheet.range(list_range)\n",
        "\n",
        "for i in range(len(cell_list)//len(column_list)):\n",
        "    cell_list[(6*i)].value = id_list[i]\n",
        "    cell_list[(6*i)+1].value = sentence_list[i]\n",
        "    cell_list[(6*i)+2].value = sentence_with_entities_list[i]\n",
        "    cell_list[(6*i)+3].value = str(subject_entity_list[i])\n",
        "    cell_list[(6*i)+4].value = str(object_entity_list[i])\n",
        "    cell_list[(6*i)+5].value = relation_list[i]\n",
        "worksheet.update_cells(cell_list)"
      ],
      "metadata": {
        "id": "pCIE2LCqnduR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbeb1b83-9a00-46bc-bf72-18d43c14ed92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'spreadsheetId': '1cpt2ad2pw0N2tC1H3MPtRAaZzFmgCh2If0gHOQKEIIo',\n",
              " 'updatedRange': 'annotation!A2:F2',\n",
              " 'updatedRows': 1,\n",
              " 'updatedColumns': 6,\n",
              " 'updatedCells': 6}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tokenizer Pre-Trained"
      ],
      "metadata": {
        "id": "IBZ3UXZtjIbA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')"
      ],
      "metadata": {
        "id": "tEgwHtcdMOmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizing_(token_list, label_list, tokenizer):\n",
        "  text_seq = []\n",
        "  valid_seq = []\n",
        "  label_seq = label_list\n",
        "  for tokens, label in zip(token_list, label_list):\n",
        "    new_target_text = []\n",
        "    new_valid = []\n",
        "    for i, (t, l) in enumerate(zip(tokens, label)):\n",
        "      tokens_wordpiece = tokenizer.tokenize(t)\n",
        "      new_v = [1] + [0]*(len(tokens_wordpiece)-1)\n",
        "      new_target_text.extend(tokens_wordpiece)\n",
        "      new_valid.extend(new_v)\n",
        "    valid_seq.append(new_valid)\n",
        "    text_seq.append(new_target_text)\n",
        "  return text_seq, valid_seq, label_seq"
      ],
      "metadata": {
        "id": "iz4LXOrQoATP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_seq, valid_seq, label_seq = tokenizing_(token_list, label_list, tokenizer)"
      ],
      "metadata": {
        "id": "KUaZwhsOLqW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text_seq)\n",
        "print(valid_seq)\n",
        "print(label_seq)"
      ],
      "metadata": {
        "id": "_lTQt5FnMbz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizing_rel(token_list, label_list, rel_list, tokenizer):\n",
        "  text_seq = []\n",
        "  valid_seq = []\n",
        "  final_left_seq = []\n",
        "  final_right_seq = []\n",
        "  label_seq = []\n",
        "\n",
        "  for tokens, label, rel in zip(token_list, label_list, rel_list):\n",
        "    lefts, rights, labels = rel\n",
        "\n",
        "    for le, ri, la in zip(lefts, rights, labels):\n",
        "      new_target_text = []\n",
        "      new_valid = []\n",
        "      new_left = []\n",
        "      new_right = []\n",
        "      before_label = -1\n",
        "\n",
        "      for i, (t, l) in enumerate(zip(tokens, label)):\n",
        "        tokens_wordpiece = tokenizer.tokenize(t)\n",
        "        if before_label != l and l != -1:\n",
        "          if l == le:\n",
        "            tokens_wordpiece = [\"*\"] + tokens_wordpiece\n",
        "          elif l == ri:\n",
        "            tokens_wordpiece = [\"#\"] + tokens_wordpiece\n",
        "        \n",
        "        if l != -1 and i+1 != len(label) and l != label[i+1]:\n",
        "          if l == le:\n",
        "            tokens_wordpiece = tokens_wordpiece + [\"*\"]\n",
        "          elif l == ri:\n",
        "            tokens_wordpiece = tokens_wordpiece + [\"#\"]\n",
        "        \n",
        "        if l != -1 and i+1 == len(label):\n",
        "          if l == le:\n",
        "            tokens_wordpiece = tokens_wordpiece + [\"*\"]\n",
        "          elif l == ri:\n",
        "            tokens_wordpiece = tokens_wordpiece + [\"#\"]\n",
        "        \n",
        "        before_label = l\n",
        "        new_v = [l]*(len(tokens_wordpiece))\n",
        "        new_target_text.extend(tokens_wordpiece)\n",
        "        new_valid.extend(new_v)\n",
        "      \n",
        "      leftaa = [float(x == le) for x in new_valid]\n",
        "      rightaa = [float(x == ri) for x in new_valid]\n",
        "      final_left_seq.append(leftaa)\n",
        "      final_right_seq.append(rightaa)\n",
        "      text_seq.append(new_target_text)\n",
        "      label_seq.append(la)\n",
        "    return text_seq, final_left_seq, final_right_seq, label_seq"
      ],
      "metadata": {
        "id": "mFHJ9gXGqzku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_greedy_label(rel_list, dic_rel_label, max_labels):\n",
        "  final_rel_list = []\n",
        "\n",
        "  for rel, max_lab in zip(rel_list, max_labels):\n",
        "    temp_les = []\n",
        "    temp_re =[]\n",
        "    rel_labels = rel[4]\n",
        "    rel_lefts = rel[5]\n",
        "    rel_rights = rel[6]\n",
        "    max_value = max_lab\n",
        "    l, r = make_fair_by_max(max_value)\n",
        "    new_l = []\n",
        "    new_r = []\n",
        "    qomax = len(rel_labels)\n",
        "    z = 0\n",
        "\n",
        "    for ll, rr in zip(l, r):\n",
        "      q = 0\n",
        "      laba = 0\n",
        "      for lf, ri, rel_label in zip(rel_lefts, rel_rights, rel_labels):\n",
        "        if ll == lf and rr == ri:\n",
        "          q = 1\n",
        "          laba = dic_rel_label[rel_label]\n",
        "      \n",
        "      if q == 1:\n",
        "        new_l.append(ll)\n",
        "        new_r.append(rr)\n",
        "        temp_les.append(laba)\n",
        "      elif z < 1.5*qomax:\n",
        "        new_l.append(ll)\n",
        "        new_r.append(rr)\n",
        "        z = z + 1\n",
        "        temp_les.append(0)\n",
        "      \n",
        "    final_rel_list.append((new_l, new_r, temp_les))\n",
        "  return final_rel_list"
      ],
      "metadata": {
        "id": "BpvnNH0Ku9P_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Relation BERT"
      ],
      "metadata": {
        "id": "1P37vclkjQU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "RxK1Mj3IveBf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset = pd.DataFrame(worksheet.get_all_values()).iloc[1:-1]\n",
        "# column = pd.DataFrame(worksheet.get_all_values()).iloc[:1]\n",
        "# Dataset.columns = column.values[0]\n",
        "\n",
        "# print(Dataset)\n"
      ],
      "metadata": {
        "id": "LtKNqxxmuOS8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenizers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AP9cbYcDVwxS",
        "outputId": "98ca8be2-80e7-4dd9-98a3-62248a5d6ddb"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.7/dist-packages (0.13.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import BertWordPieceTokenizer"
      ],
      "metadata": {
        "id": "ucJsV0fiVyHs"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLMTrainDataset_Sik(Dataset):\n",
        "  \n",
        "  def __init__(self, max_len, mask_prob, mask_token, sep_token, cls_token, all_sents,\n",
        "               tokenizer, pad_token):\n",
        "    self.all_sents = all_sents\n",
        "    self.num_vocabs = tokenizer.vocab_size\n",
        "    self.max_len = max_len\n",
        "    self.mask_prob = mask_prob\n",
        "    self.mask_token = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(mask_token))\n",
        "    self.cls_token = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(cls_token))\n",
        "    self.sep_token = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sep_token))\n",
        "    self.tokenizer = tokenizer\n",
        "    self.pad_token = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(pad_token))\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.all_sents)\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    sents = self.all_sents[index]\n",
        "    sents = self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(sents))\n",
        "    tokens = []\n",
        "    sents = np.array(sents)\n",
        "    inputs = sents\n",
        "    labels = inputs.copy()\n",
        "\n",
        "    probability_matrix = torch.full(labels.shape, self.mask_prob)\n",
        "\n",
        "    masked_indices = torch.bernoulli(probability_matrix).bool()\n",
        "    \n",
        "    labels[~masked_indices] = -100\n",
        "\n",
        "    indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
        "\n",
        "    inputs[indices_replaced] = self.tokenzier.convert_tokens_to_ids(self.mask_token)\n",
        "\n",
        "    indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
        "\n",
        "    random_words = torch.randint(len(self.tokenizer), labels.shape, dtype = torch.long)\n",
        "\n",
        "    inputs[indices_random] = random_words[indices_random]\n",
        "    inputs = list(inputs)\n",
        "    labels = list(labels)\n",
        "    valid_length = min(len(inputs), self.max_len-2)\n",
        "    tokens = self.cls_token + inputs[:self.max_len-2] + self.sep_token\n",
        "    labels = [-100] + labels[:self.max_len-2] + [-100]\n",
        "\n",
        "    mask_len = self.max_len - len(tokens)\n",
        "\n",
        "    tokens = tokens + self.pad_token * mask_len\n",
        "    labels = labels + [-100] * mask_len\n",
        "    attention_masks = [0.0]*len(tokens)\n",
        "    for i in range(valid_length):\n",
        "      attention_masks[i] = 1.0\n",
        "    \n",
        "    return torch.LongTensor(tokens), torch.LongTensor(labels), valid_length, torch.LongTensor(attention_masks)"
      ],
      "metadata": {
        "id": "9vfwORZvwL9I"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_mlm = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "id": "8rwflSss4AYb",
        "outputId": "e3811a67-f265-4263-e97f-6b2783c3da27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-37046588-c204-4866-96d1-4d2e09f1899d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-37046588-c204-4866-96d1-4d2e09f1899d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving sample.plain.html to sample.plain.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('sample.plain.html')\n",
        "read = f.read()\n",
        "print(read)\n",
        "print(type(read))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpaP3eThO3fW",
        "outputId": "9214c128-9075-4d8c-c2ef-0e92e56a0152"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<!DOCTYPE html >\n",
            "<html id=\"a3DnzcVSvJKG25OeznRpiW5zLgi8-sample.pdf\" data-origid=\"sample.pdf\" class=\"anndoc\" data-anndoc-version=\"3.6\" lang=\"\" xml:lang=\"\" xmlns=\"http://www.w3.org/1999/xhtml\">\n",
            "  <head>\n",
            "    <meta charset=\"UTF-8\"/>\n",
            "    <meta name=\"generator\" content=\"net.tagtog.anndoc.v3.parsers.general.pdf.common.StubPdfParser_v1_0_0\"/>\n",
            "    <title>a3DnzcVSvJKG25OeznRpiW5zLgi8-sample.pdf</title>\n",
            "  </head>\n",
            "  <body>\n",
            "    <article>\n",
            "      <section data-type=\"\">\n",
            "        <div class=\"content\">\n",
            "          <p id=\"s1p1\">보고서 번호: ARAIB/R 2020 - 3 철도사고 조사보고서</p>\n",
            "          <p id=\"s1p2\">한국철도공사 경전선(완사역→진주역 사이, 삼랑진역 기점 103.286km) 제3081화물열차(디젤 7620호 + 화차 25량) 열차탈선 2019년 2월 18일(월) 21시 56분경 2020.</p>\n",
            "          <p id=\"s1p3\">6.</p>\n",
            "          <p id=\"s1p4\">19.</p>\n",
            "          <p id=\"s1p5\">항공･철도사고조사위원회 이 조사보고서는 「항공․철도사고조사에 관한 법률」제 2조에 의거 사고조사가 이루어졌으며, 제25조에 따라 작 성되었다. 같은 법률 제1조에서 「철도사고 조사는 독립적이고 공정 한 조사를 통하여 사고 원인을 정확하게 규명함으로써 철도 사고의 예방과 안전 확보에 이바지함」을 목적으로 하고 있 다. 또한, 제30조에 따라 사고조사는 민․형사상 책임과 관련된 사 법절차, 행정처분 절차 또는 행정 쟁송 절차와 분리․수행되어 야 하고, 제32조에서위원회에 진술․증언․자료 등의 제출 또는 답변을 한 사람은 이를 이유로 해고․전보․징계․부당한 대우 또 는 그 밖에 신분이나 처우와 관련하여 불이익을 받지 아니 한다.라고 규정하고 있다. 그러므로 이 조사보고서는 철도 분야의 안전을 증진시킬 목 적 이외의 용도로 사용되어서는 아니 된다. 차례 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - i - 차 례 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 ························ 1 개요 ······························································································································· 2 1. 사실 정보 ··············································································································· 3 1.1 사고의 경위 ····································································································· 3 1.2 피해 사항 ········································································································· 4 1.3 인적 정보 및 업무 수행사항 ······································································· 4 1.4 현장 정보 ······································································································· 10 1.5 열차와 차량 정보 ························································································· 11 1.6 차량정비단의 차축 베어링 분해 검수 실태 ··········································· 16 1.7 신호 및 전차선 정보 ··················································································· 18 1.8 기상 정보 ······································································································· 19 2. 분석 ························································································································ 20 2.1 업무 수행사항 분석 ····················································································· 20 2.2 차축 베어링 분해 검사 ··············································································· 22 2.3 컨테이너 상하차 작업 분석 ······································································· 25 2.4 그리스 성분 분석 ························································································· 26 2.5 종합 분석 ······································································································· 29 3. 결론 ························································································································ 30 3.1 조사 결과 ······································································································· 30 3.2 사고 원인 ······································································································· 32 4. 안전 권고 ·············································································································· 33 4.1 한국철도공사에 대하여 ················································································· 33 제목 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 1 - 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 ○ 운영기관 : 한국철도공사 ○ 운행노선 : 경전선 ○ 발생장소 : 완사역∼진주역 사이 (삼랑진역 기점 103.286km 지점) ○ 사고열차 : 제3081화물열차 (황등역16:38→부산항역22:41) [디젤 7620호 + 화차 25량 편성, 환산 33.9량(컨테이너 실은 차)] ○ 사고유형 : 열차탈선 ○ 발생일시 : 2019년 2월 18일(월) 21시 56분경 [그림1] 사고현장 개요 개요 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 2 - 개요 2019년 2월 18일 16:38에 전라선 황등역을 출발하여 부산항역으로 운행 중 이던 제3081호 화물열차(이하 ‘사고열차’라 한다)가 21:56경 경전선 하행선 방 향 완사역과 진주역 사이에서 비상제동이 체결되어 정차하였다. 정차 후 사고 열차 기관사가 확인한 결과 13번째 화차(976296호, 이하 ‘탈선화차’라 한다)의 뒷 대차 첫 번째 차축과 두 번째 차축이 진행방향 좌측으로 탈선되었고, 좌 측 세 번째 차축이 절손되었으며, 탈선화차와 14번째 화차 사이의 공기호스 가 빠져있는 것이 발견되었다. 이 사고로 인명피해는 발생하지 않았으나, 탈선화차의 차축이 절손되고 대 차부품 일부와 선로시설의 PCT침목, 체결구 등이 파손되는 피해가 발생하 였다. 항공·철도사고조사위원회는 이번 사고의 주원인을 ‘차축에 설치되어 있는 차축 베어링 외륜의 내면과 롤러에 결함이 있는 상태에서 운행 도중 차축 베어링이 파손되면서 차축이 절손되어 탈선한 것’으로 결정하였다. 기여요인으로는 ‘화차에 컨테이너를 싣고 내릴 때 차축 베어링에 충격이 가해진 것, 차축 베어링의 분해검수 주기와 교환 주기가 화차에 적재되는 화 물의 종류, 적재방법 및 사용환경의 차이에도 불구하고 동일하게 규정하여 관리한 것’으로 결정하였다. 항공․철도사고조사위원회는 한국철도공사에 5건의 안전권고를 한다. 사실정보 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 3 - 1. 사실 정보 1.1 사고의 경위 2019년 2월 18일 16:38에 황등역을 출발하여 부산항역으로 운행 중이던 사 고열차의 탈선화차 3번째 좌측 차축 베어링이 과열되어 불꽃을 일으키며 진 행하다가 차축 베어링의 과열로 차축이 절손되어 21:52경 신기터널 출구 부 근(삼랑진역 기점 106.135km)에서 엔드캡이 탈락되었으며, 이후 베어링이 분 해되며 롤러 및 베어링 외륜 등 베어링 부품이 선로변으로 흩어졌다. 사고열 차는 완사역과 진주역 사이(삼랑진역 기점 103.286km)에서 탈선화차의 3위축 과 4위축이 진행방향 좌측으로 탈선하여 약 886m를 진행하다 탈선화차와 14 번째 화차 사이의 공기호스가 빠지면서 비상제동이 체결되어 정차(삼랑진 기점 102.400km)하였다. 선로변에서 발견한 차축 베어링 부품의 위치와 탈선위치 등은 [그림2]와 같다. [그림2] 차축 베어링 부품 발견위치와 탈선위치 주행방향 사실정보 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 4 - 1.2. 피해 사항 1.2.1 인명 피해 이 사고로 인명 피해는 발생하지 않았다. 1.2.2 물적 피해 이번 사고로 화차 1량이 탈선되면서 대차 프레임, 브레이크빔, 차축, 스프 링 등이 파손되어 차량 분야에서 약 1,088만원, PC침목 402개, e-코일 1,500 개, 체결구 60세트 등이 파손되어 2,847만원, 솔티터널 복구 공사비 2억6,364 만원 등 선로 분야에서 2억9,211만원의 피해가 발생하여 총 3억299만원의 피해가 발생하였으며 운행에 지장을 받은 열차는 없었다. 1.3 인적 정보 및 업무 수행 사항 1.3.1 사고열차 기관사 사고열차 기관사(58세, 남)는 1980년 11월 1일 (구)철도청 광주기관차사무 소 기관조사로 임용된 후 1987년 12월 31일 기관사로 임용되었으며, 2006년 7월 1일 디젤과 전기 1종 철도차량 운전면허를 취득하여 2018년 7월 1일자 로 지원기관사로 근무하고 있었다. 사고열차 기관사는 2016년 적성검사에 합격하였고, 사고당일 19시경 출근 하여 승무적합성 검사를 시행하였으며 결과는 양호하였다. 사고열차 기관사는 순천역 발차 후 제동 감도 시험 후 광양역, 진상역, 하 동역, 횡천역, 완사역을 차례로 통과하고 8/1000 상구배에 접어들면서 가감 간을 2단으로 운행 중 갑자기 비상제동이 체결되었다고 진술하였다. 사고열 차 기관사는 진상역 진입 시 열차 후부를 본 적이 있으나 이상 상태는 감지 하지 못했고, 이후에는 선로상태가 거의 직선이어서 열차 후부를 보는 것은 사실정보 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 5 - 불가능하였고, MMI1) 화면의 CCTV로 볼 수는 있지만 선명하지 않아 확인할 수 없었다고 진술하였다. 정차 후 비상제동을 완해하기 위해서 1분 이상을 기다린 후 완해하였으나 주공기는 정상이고 제동관 공기가 3.5바(bar) 이상 충기되지 않아 다시 한 번 비상제동을 체결한 후 완해를 시키는 절차를 수행했으나 동일 현상이 발생 되어 진주역을 무전으로 호출하여 열차가 비상제동이 체결되어 정차하고 있 다는 통보를 하였고, 사고열차 보조기관사로 하여 기관차와 화차사이의 제동 관 연결콕크를 잠궈보라고 하니 공기압이 정상 회복되어 화차의 문제라 판 단하고 사고열차를 점검하도록 지시 하였다고 진술하였다. 1.3.2 사고열차 보조기관사 사고열차 보조기관사(59세, 남)는 1985년 12월 19일 (구)철도청 순천기관차 승무사무소에 기관조사로 전입되어 1991년 12월 05일 기관사로 임용되었으 며, 2006년 7월 1일 디젤과 전기 1종 철도차량 운전면허를 취득하여 지원기 관사로 근무하고 있었다. 사고열차 보조기관사는 사고 당일 19시경 출근하여 승무적합성 검사를 시 행하였으며 결과는 양호하였다. 사고열차 보조기관사는 순천역∼마산역 간 사고열차 사업담당으로 순천역 ∼진주역 간은 보조기관사, 진주역에서 마산역까지는 기관사 업무를 수행할 예정으로 완사역을 계획보다 1시간12분 늦게 통과(현 21:53)하여 삼량진역 기점 102.700km 지점(8‰ 상구배) 운행중 갑자기 비상제동이 체결되면서 정차하였다. 비상제동 원인을 찾던 중 사고열차의 13번째 화차 뒷 대차(3,4위)가 탈선 되어 있음을 확인하고 구로교통관제센터와 진주역에 통보(현 23:15)하였다. 1) MMI((Man Machine Interface) : 차량의 상태를 화면으로 보여주는 장치 사실정보 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 6 - 1.3.3 진주역 로컬관제원 진주역 로컬관제원(48세, 남)는 1999년 (구)철도청 역무원으로 임용되어 2015년 1월 1일부터 진주역 로컬관제원으로 근무하고 있었다. 진주역 로컬관제원은 2015년 8월 20일 로컬관제원 보수교육을 수료하였고, 2016년 3월15일 적성검사를 통과하였으며, 2018년 6월25일 신체검사 결과는 양호하였다. 사고당일 진주역 로컬관제원은 야간근무시(21:00~익 02:00) 1231열차의 입 환작업을 준비하던 중 21시57분경 사고열차 기관사로 부터 비상제동이 체결 되어 공기압이 올라가지 않는다는 내용을 무전으로 통보받고, 구로교통관제 센터에 관련 사실을 통보하고, 역무팀장에 보고하였으며, 23시18분 전남본부 ㅇㅇㅇ으로부터 사고열차가 탈선되었다는 통보를 받고 사고열차 복구 관련 업무를 수행하였다고 진술하였다. 1.3.4 북천역 로컬관제원 북천역 로컬관제원은 사고열차가 1시간13분 지연되어 21시46분경 북천역 51호 선로전환기에서 21호 선로전환기까지 3번선 통과 운행시 정상적인 신 호에 의해 운행하였고, CCTV를 통해 확인하였을 때도 다른 이상이 없었다 고 진술하였다. 1.3.5 사고열차(DL7620호) 운행기록 사고열차의 운행기록을 분석해 본 결과 [표1]과 같이 21:54:12경(③) 약 81km/h 속도로 운행 중 기관차 출력을 최대로 상승시켰으나 속도가 74km/h로 떨어졌다. 사실정보 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 7 - 21:54:43경(⑥) 약 73km/h의 속도에서 8단으로 견인 중 제동관 압력 이 급감하면서 비상제동이 체결되었고 약 231m 이동 후 정차되었다.(⑦) [표1] 사고열차의 운행기록 정보 구분 시간 속도(km/h) 거리(m) 차 량 상 태 비고 ① 21:07:01 66 51,869.3 순천역 출발 평균속도 ② 21:54:01 81 247.5 가감간 유전⇒1단 상승 ③ 21:54:12 81 244.4 가감간 1단에서 상승 시작 화차탈선(추정) ④ 21:54:23 79 403.8 가감간 8단 유지 속도가 떨어짐(81km/h⇒74km/h)⑤ 21:54:42 74 20.4 ⑥ 21:54:43 73 231.7 제동관 압력 급강하, 비상체결 약 231.7m 이동 후 정차⑦ 21:55:05 0 0 열차 정지 ⑦⑥④~⑤③ 사실정보 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 8 - 1.3.6 열차무선 녹취록 사고열차의 운행과 관련하여 [표2]와 같이 로컬관제원과 기관사의 무전기 녹취록을 확인한 결과 사고열차는 21:03에 순천역에 도착하여 21:07에 순천 역을 출발하였으며, 21:13경 광양역을 통과하는 과정의 녹취록은 잡음이 심해 통화내용을 확인할 수 없었다. 22:05경 기관사가 진주역을 호출하여 운행 중 갑자기 비상제동이 체결되어 열차가 정지되어 원인을 찾고 있다고 하였고, 이후 현장에서 차량상태를 점검 하여 열차의 탈선을 확인하였다. 사고열차 무전기의 통화내용 녹음 기록 중 탈선화차에 대한 차축 베어링 의 이상(불꽃 등)을 발견하거나 확인한 내용은 없었다. 시 간 사고열차 기관사 로컬관제원 비 고 21:03 철도 3081열차 후부 양호. 순천역 이상! 순천역 21:06 철도 3081 교대 완료, 준비완료! 철도 3081 순천역 4번선에 정차 한 상선 출발확인 발차합시다. 순천역 로컬관제 이상! 21:07 철도 3081 발차, 재확인 이상! 21:13 철도 30**열차 이상!(잡음 심함) 광양역 22:05 철도 진주 3081 ~ 예 지금 102km200부근에 갑자기 비상 제 동이 체결되어 정차해 있습니다. 예, 지금 원인을 찾고 있습니다. 이상감지, 이후 열차 탈선확인 [표2] 열차무선 녹취록 1.3.7 CCTV 확인(역 및 기관차) 사고열차가 순천역을 출발 후 탈선되어 정차할 때까지 [그림3]과 [그림4]와 같이 역사 및 기관차 CCTV에 녹화된 영상을 확인한 결과 광양역과 진상역 간 운행 중 차량 하부에서 최초로 불꽃이 발생되는 것을 확인하였으며, 이후 하동역 북천역 완사역 통과 중에도 불꽃이 발생되는 것을 확인하였다. 사실정보 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 9 - ① 진상역 통과 중(불꽃 발생) ② 하동역 통과 중(불꽃 발생) ③ 북천역 통과 중(불꽃 발생) ④ 완사역 통과 중(불꽃 발생) [그림3] 순천역∼완사역 역사 CCTV 녹화 화면 이상 없음 하부 불꽃 ① 광양역 통과 중(이상 없음) ② 광양역∼진상역 운행 중(불꽃 발생) 하부 불꽃 21:55:05 열차정지 ③ 완사역∼진주역 운행 중(불꽃 발생) ④ 열차 정차 [그림4] 기관차 7620호 CCTV 녹화 화면 사실정보 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 10 - 1.4 현장 정보 1.4.1 최초 탈선위치 및 최종 정차위치 사고열차의 최초 탈선흔적은 [그림5]과 같이 삼랑진역 기점 약 103.286km 지점에서 발견되었으며, 탈선화차의 세 번째 차축의 좌우 차륜이 열차 진행 방향 좌측으로 떨어지면서 자갈에 끌린 흔적을 남겼다. 사고열차는 최초 탈선위치에서 약 886m를 더 진행하여 삼랑진역 기점 약 102.400km 지점에 정차되었다.([그림6]) [그림5] 최초 탈선위치 및 최종 정차위치 [그림6] 사고열차 탈선상태 1.4.2 선로 정보 사고지점(삼량진역 기점103.290km)이 포함된 경전선 진주역∼완사역 간은 2017년 경전선 진주역∼광양역 복선으로 개통되었으며, 사고개소의 본선 선형은 열차진행 방향으로 우곡선(R:3000m, 곡선 시점101.687km-종점 최초 탈선위치 (삼랑진역 기점 103.286km) 최종 정차위치(삼랑진역 기점 102.400km) 사실정보 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 11 - 103. 464km, 연장 1,777m)으로 설정 캔트는 50mm이며, 선로기울기는 내리막 1.0‰이고, 최고속도는 150km/h이다. 사고지점의 궤도는 60kg 장대레일에 PC침목, 코일스프링 크립, 도상자갈 두께 320mm로 부설되어 있었으며, 솔티터널2) 접속부는 콘크리트 도상으로 레일을 침목에 체결하는 형식은 레다20003)과 터널 내부는 시스템300-14)로 구성되어 있었다. 선로유지관리 상태를 보면 궤도검측차 운행은 분기1회 시행하고 있었으며, 2018년 1/4분기(4/5∼4/5), 2/4분기(5/31∼6/1), 3/4분기(9/11∼9/12), 4/4분 기(11/14∼11/15) 4회 검측한 결과는 양호하였다. 또한, 삼랑진역 기점 101.000km∼114.500km 구간에 대하여 2019년 1월 7 일, 1월 14일, 1월 21일, 2월 1일, 2월 7일, 2월 11일, 2월 18일 도보순회 점검을 시행한 결과에도 탈선에 영향을 줄 만한 사항은 없었다. 유지보수 작업은 인력작업과 보선장비 작업을 병행하였으며, 보선장비 작업은 주로 1종 기계작업으로 시행한 것을 확인하였다. 1.5 열차와 차량 정보 1.5.1 사고열차의 조성 사고열차는 [표3]과 같이 디젤기관차(DL7620호)와 컨테이너 화차 25량으로 조성(환산 33.9량, 열차장 30.1량)되어 있었으며 화물이 적재된 컨테이너를 싣 고 있었다. 2) 솔티터널(복선) : 경전선 진주역-완사역간 102km497~102km906 ( L = 409m) 3) 레다2000 : 레티스 철근으로 연결된 RCT Twin Block 침목을 콘크리트 도상에 매립하고 레일과 침 목사이에 탄성재를 삽입 4) 시스템300-1 : 트러프를 시공 후 트러프 내부에 탄성식 베이스 플레이트로 구성된 PCT Mono Block 침목과 침목 관통 종방향 보강 철근과 횡철근을 매입 사실정보 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 12 - ⑬976296 ☜완사(순천)방 향 진주(부산)방향 ☞ ①◯25 열차진행방향 [표3] 사고열차의 조성 현황 1.5.2 사고열차 제원 사고열차에 조성되어 있던 기관차와 화차의 제원은 각각 [표4] 및 [표5]와 같다. 조성순서 차량번호 차종명 차량톤수 하중톤수 공차환산 영차환산 차 길이 기관차 7620 디젤76 132 124 4.2 4.4 1.5</p>\n",
            "          <p id=\"s1p6\">1 70085 컨테2TEU 18.5 50 0.5 1.6 1 2 763025 컨테이너전용 23.1 51.5 0.6 1.7 1.1 3 70911 컨테2TEU 21.4 62.6 0.5 1.9 1 4 76071 컨테2TEU 18.1 50 0.5 1.6 1 5 70965 컨테2TEU 21.4 62.6 0.5 1.9 1 6 70883 컨테2TEU 21.4 62.6 0.5 1.9 1 7 70385 컨테2TEU 17.7 50 0.4 1.6 1 8 976211 컨테3TEU 20 54.5 0.5 1.7 1.4 9 976509 컨테3TEU 20.8 56.5 0.5 1.8 1.4 10 760872 컨겸50 21 50 0.5 1.6 1.1 11 70162 컨테2TEU 17.5 50 0.4 1.6 1 12 70394 컨테2TEU 17.7 50 0.4 1.6 1 13 976296 컨테3TEU 20.6 56.5 0.5 1.8 1.4 14 763349 컨겸평판 22.8 52 0.6 1.7 1.1 15 763359 컨겸평판 22.8 52 0.6 1.7 1.1 16 71020 컨겸평판 22.7 52 0.6 1.7 1.1 17 975209 컨테3TEU 18.2 56.5 0.5 1.7 1.4 18 975276 컨테3TEU 18.2 56.5 0.5 1.7 1.4 19 970505 냉동컨테이너 19.5 48 0.5 1.6 1 20 763071 컨겸평판 23.1 51.5 0.6 1.7 1.1 21 71027 컨겸평판 22.7 52 0.6 1.7 1.1 22 975237 컨테3TEU 18.2 56.5 0.5 1.7 1.4 23 70948 컨테2TEU 21.4 62.6 0.5 1.9 1 24 71033 컨겸평판 22.7 52 0.6 1.7 1.1 25 975262 컨테3TEU 18.2 56.5 0.5 1.7 1.4 사실정보 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 13 - 구 분 제 원 구 분 제 원 기관차번호 7620 최고속도 150km/h 차 종 디젤기관차 자 중 132t 도입일 2014. 6.30. 길 이 21,168㎜ 대차형식 볼스타리스 용접대차 높 이 4,150㎜ 제동장치 발전제동, 답면제동 폭 2,870㎜ 연결기형식 E 형 고정축거 4,200㎜ [표4] 기관차 주요 제원 구 분 제 원 구 분 제 원 차량번호 976296 최고속도 90km 차 종 평판차 자 중 20.6t 세부차종 컨테이너 3T 하 중 56.5t 도입일 1996.01.09 길 이 19,570㎜ 대차형식 바바 높 이 1,065㎜ 제동장치 공기제동(K2) 폭 2,579㎜ 연결기형식 E 형 고정축거 1,676㎜ [표5] 탈선화차의 주요 제원 1.5.3 탈선화차의 정비 현황 탈선화차의 종별 검수 이력은 [표6]과 같다. 검종 시행일 시행 장소 ES(기본정비) ‘19.2.15., ‘19.2.13., ‘19.2.11. 익산차량사업소 LI6(경정비) ‘18.12.19. ‘17.11.23.-11.24. ‘16.9.20.-9.21. 부산신항차량사업소 부산차량사업소 사외정비(고려차량) GI1(중정비1) ‘18.7.5. ‘16.4.4.-4.6. 부곡차량사업소 사외정비(고려차량) GI2(중정비2) ‘17.4.6.-5.4. ‘15.1.12.-1.16. 부산철도차량정비단 사외정비(고려차량) [표6] 탈선화차의 종별 검수 이력 탈선화차는 한국철도공사 객화차유지보수기준 제18조에 따라 주행장치 대차 각부 분해세척, 청소 후 액체침투 탐상, 차축 롤러 베어링 분해세척 후 각 부품의 상태 확인 및 차륜삭정 등을 시행하는 중정비2(GI2)를 2017년 5월 2일 실시하였고 검사 이후 사고당일까지 127,448.6㎞를 주행하였다. 사실정보 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 14 - 탈선화차의 차축에 대한 초음파탐상 및 유간측정 등의 결과는 [표7]과 같다. [표7] 탈선화차 윤축검사 내용 1.5.4 탈선화차의 차축 베어링 탈선화차 차축에는 TIMKEN사와 SKF사의 RCT NFL5) 차축 베어링이 혼 합 설치되어 있었으며, 탈선화차의 절손된 차축에는 SKF사의 차축 베어링이 설치되어 있었다. 이 베어링은 신차 도입 시 최초 설치되었고 차축 베어링의 구조는 [그림7]과 같다. 설치 볼트 엔드캡 씰 마모링 오일 씰잠금 플레이트 베어링 외륜 스페이스 내륜 및 롤러 조립 백킹링오일 씰 씰 마모링 내륜 및 롤러 조립 [그림7] RCT형 NFL 차축 베어링 구조 5) RCT NFL : Rotate end Cap and Tapered roller bearing No Field Lubrication의 약자로서, 정상 운행 중이면 장기간 그리스의 재급유가 필요하지 않은 그리스 윤활방식의 테이퍼 롤 러 베어링 위수 차축탐상 베어링 번호 제작사/년월 축방향유간 내경 감도 주파수 판독 R L R L R L R L 기준값 35 3M PN(양호) 0.25~0.50㎜ 131.75~131.80㎜ 1 35 3M PN 9708 76553 0007 34619 TK 97.08 TK 00.07 0.38 0.32 131.78 131.78 2 35 3M PN 0208 54537 0111 39836 TK 02.08 TK 01.11 0.32 0.32 131.78 131.78 3 35 3M PN 0312 004385 0111 57918 SKF 03.12 SKF 01.11 0.35 0.34 131.78 131.78 4 35 3M PN 0212 91188 0709 559109 TK 02.12 TK 07.09 0.30 0.38 131.78 131.78 사실정보 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 15 - 1.5.5 탈선화차의 축상 및 차륜의 상태 탈선화차의 뒷 대차의 왼쪽 앞 차축은 [그림8]과 같이 절손되었으며, 엔드캡 부분이 절손된 상태의 차축이 사고현장에서 발견되었다. [그림8] 절손된 탈선화차 차축 탈선화차 뒷 대차의 엔드캡 상태는 [그림9]와 같고 차축 베어링 과열 확인 을 위해 붙여 놓은 70도 온도 테이프는 탈락되었거나 변색이 없었다. 또한 모든 베어링에서 그리스 누유 흔적은 발견되지 않았다. [그림9] 탈선화차 차축의 베어링 조립체 3L : 70도 온도 테이프 없음 3R : 70도온도테이프변색없음 4L : 70도 온도 테이프 변색 없음 4R : 70도 온도 테이프 변색 없음 사실정보 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 16 - [그림 10]은 사고현장에서 수거한 탈선화차의 파손된 차축 베어링과 정상 적인 차축 베어링을 비교한 것이다. 파손된 차축 베어링은 내륜, 외륜, 롤러, 케이지 등이 심하게 훼손된 상태였다. [그림10] 정상 차축 베어링(상)과 비교한 파손된 차축 베어링(하) 1.6 차축 베어링 분해 검수 실태 한국철도공사 차량정비단에서는 객화차유지보수기준 에 따라 중정비 중 차축 베어링 분해검수 기준(주행키로 80만km, 회기한도 8년)에 도달하는 차 축 베어링에 대해서는 세척→내륜의 청소→검사→외륜의 검사→ 각부의 측정→표기, 기록→조립→그리스 주입→오일 씰 조립 후 차축에 압입하는 분해검수 과정으로 중정비를 시행하고 있었다. 탈선차량의 경우 사고 이전(‘17.4.28) 중정비2(GI2) 시행 시 차축 베어링을 분해 세척한 후 [그림11] 처럼 외륜과 롤러 표면의 필링(미세 박리), 브리넬 링(표면 변형), 스폴링(피로성 박리) 등을 돋보기와 디지털 현미경으로 확인 하고 있었으며, 차축 베어링 유간 및 내륜 측정기로 치수를 측정하였으며, 탈선화차의 롤러 베어링 분해 및 조립 시 검사항목은 [그림12]과 같으며, 모든 측정값은 모두 기준치 이내로 관리되고 있었다. 사실정보 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 17 - 롤러 표면 육안 검사 롤러 표면 현미경 검사 유간 및 내륜 측정기 베어링 내륜 측정 [그림11] 차량정비단의 베어링 분해검사 장면 [그림12] 차축 베어링 분해검사 및 조립 후 검사기준치 및 측정치 3L (절손) 3R (탈선) 4L (탈선) 4R (탈선) 사실정보 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 18 - 1.7 신호 및 전차선 정보 1.7.1 신호정보 사고구간의 신호설비는 전자연동장치, 복선 자동폐색 5현시, 구내 임펄스 궤도회로, 역간 AF 궤도회로, ATS 방식으로 구성되어 있다. 전기기술지원시스템 기록에는 [그림13] ①②③④와 같이 사고열차가 완사 역 상장내-2번선 진로로 진입하여 완사역 2번선에서 상출발 신호현시 상태에 서 진출한 후 사고지점에 정차할 때까지 신호와 진로는 모두 양호한 상태였 으며, 사고열차는 완사역∼진주역 구간을 운행하다가 탈선되어 886m를 더 진행 후 B8303T와 B8305T 구간에서 비상제동이 체결되어 정차되었다. ③ 사고열차 완사역 2번선-상출발 진출2019. 2.18. 21:53:32 ④ 사고열차 완사-진주간 탈선 8303T 정차2019. 2.18. 21:56:09 [그림13] 완사역∼진주역간 사고열차 운행 기술지원시스템 기록 상황 ① 사고열차 완사역 2번선 통과 현시2019. 2.18 21:47:10 ② 사고열차 완사역 상장내 진입 전2019. 2.18. 21:52:17 사실정보 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 19 - 1.7.2 전차선 정보 사고구간은 복선 비전철 구간으로 이번 사고와 관련이 없다. 1.8 기상정보 기상청 자료에 따르면 사고구간인 진주시의 당시 날씨는 약간의 비가 내 렸고, 기온은 4.9℃, 습도는 59%로 나타났다. 분석 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 20 - 2. 분석(Analysis) 2.1 업무 수행사항 2.1.1 사고열차 기관사 사고열차 기관사는 음주 등 승무적합성 검사에 대하여 위반 사항은 없었 으며, 사고열차 운행 시 운행기록 분석 결과 제한속도를 초과하지 않았으며, 열차에 충격을 줄 만한 기기 취급은 없는 등 적정한 승무업무를 수행하였다. 2.1.2 열차감시의 적정성 사고열차 무전기 녹취록을 확인한 결과 기관사와 로컬관제원 간 통화내용 중‘차량상태 이상 유무’를 확인해 주거나 탈선화차‘차축 베어링의 발열(불꽃 등)을 발견하거나 확인’한 내용은 없는 것으로 조사되었다. 사고열차가 출발 후 탈선되어 정차할 때까지 역사 및 기관차 CCTV 녹화 영상을 확인한 결과 탈선화차의 차축 베어링 발열에 의한 불꽃은 광양역과 진상역 사이 운행 중 차량 하부에서 최초로 불꽃이 발생되는 것을 확인하였 고, 이후 하동역 북천역 완사역 통과 중에도 불꽃이 발생되는 것이 확인되었 다. 당시 사고열차의 운행 속도를 80km/h라고 하고 불꽃발생 지점을 추정해 보면, ① 광양역(삼량진역 기점 148.500km) 지점의 통과 시각 21시17분20초 ② 최초 불꽃 확인된 지점의 통과시각 21시26분29초 ③ ①-②지점 운행시각 = 21시26분29초-21시17분20초 = 9분9초 = 549초 ④ 당시 속도 80km/h를 초속으로 환산하면 22.2m/sec ⑤ 이동거리 22.2m/sec × 549초 ≒ 12,200 m = 12.2km ⑥ 최초 불꽃발생 지점은 148.500km–12.2km = 136.300km 지점으로 추정됨 분석 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 21 - 또한 열차의 감시업무에 대하여 한국철도공사 운전취급규정 (개정 2018. 5. 8.) 37조와 제37조의 2에서 [표8]과 같이 규정하고 있다. 제37조(열차의 감시) ① 열차가 정거장에 도착·출발 또는 통과할 때와 운행 중인 열차 의 감시는 다음 각 호에 따른다. 1. 기관사 가. 견인력 저하 등 차량이상을 감지한 경우 열차상태를 확인 후 운행할 것 나. 열차교행, 역 출발 또는 통과 시 무선전화기 수신에 주의할 것 다. 동력차 2인 승무열차의 기관사(또는 부기관사)는 운행 중 각 정거장간 1회 이상 뒤를 감시하여 열차의 이상 유무를 확인하여야 하며, 열차가 정거장을 출발하거나 통과할 경우 때때로 뒤를 감시하여 열차의 상태와 역장 또는 열차승무원의 동작 에 주의할 것 라. 동력차 1인 승무열차는 뒤감시 생략 제37조의2(영상감시설비에 의한 열차의 감시) ① 열차감시 지정역의 역장은 조작반 취 급, 열차무선교신 등 운전취급에 지장 없는 범위에서 영상감시장비를 활용하여 열차가 정거장에 진입 또는 진출할 때 주행장치 등의 이상 유무를 감시하여야 한다. 다만, 고 정편성 여객열차 및 동력차만으로 조성한 열차는 감시를 생략할 수 있다. ② 영상감시설비에 의한 열차감시 지정역은 별표 26과 같다. 영상감시설비에 의한 열차감시 지정역(제37조의2 [별표 26] 관련) 선 명 대상역 경전선 진영, 마산, 진주, 광양 [표8] 열차감시 관련 한국철도공사 「운전취급규정」 발췌 사고구간의 영상감시설비에 의한 열차의 감시는 [표8]의 [별표26]과 같고 열차감시 지정역은 ‘광양역과 진주역’이며 광양역에서는 광양역과 진상역 진주역에서는 완사역과 진주역의 영상감시설비가 구성되어 있었다. 하동역, 횡천역, 북천역은 무인 또는 1∼2명이 근무하는 역으로 열차 감시 역으로 지정되어 있지 않았다. 광양역과 진주역 로컬관제원은 ‘사고열차에 대해 CCTV 영상설비를 활용하 여 열차감시를 하였고 이상 없었다.’라고 진술하였으나 광양역 근무자는 사 고열차가 진상역 통과 중 차량 하부의 불꽃이 발생된 것을 확인하지 못하였 고 진주역 근무자는 사고열차가 완사역 통과 중 차량 하부의 불꽃이 발생된 것을 발견하지 못한 것으로 확인되었다. 분석 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 22 - 그리고 사고열차의 기관차는 신형기관차로서 일반기관차의 구조와 달리 박 스 형태이며 후부감시용 후사경은 설치되어 있지 않았고 측면 창문은 작게 설치되어 있어 운전실 내부에 운전자용 MMI 화면을 CCTV 화면으로 변경 하여야 열차 후부를 감시할 수 있었다. 사고열차의 기관사는‘정거장 간 후부감시를 기관차 내부 MMI 화면(CCTV) 을 통해 확인했으나 야간 및 우천으로 화질 상태가 불량하여 후부 감시가 어려웠습니다.’라고 진술하였다. 지역본부 지도운영팀장은 본사 운전제도 문답방6)을 활용하여‘신형기관차 (7600호대, 8500호대) 열차 후부감시 방법’을 문의(2015. 9. 28) 한 것 으로 보 아 사고열차와 같은 신형기관차는 기관사가 운전실에서 열차의 후부를 감시 하기에는 적절하지 않은 것으로 조사되었다. 2.2 차축 베어링 분해 검사 차축베어링이 차축 절손에 영향을 주었는지를 알아보기 위하여 탈선화차 에 설치된 총 8개의 차축 베어링 중 절손된 세 번째 왼쪽 차축(3L)에 설치 되었던 차축 베어링은 [그림14]과 같이 심하게 파손되어 사고원인을 밝힐 만 한 단서를 찾을 수 없었다. 그러나 세 번째 왼쪽 차축 베어링과 거의 유사한 하중이력을 가졌다고 판단되는 세 번째 오른쪽 차축 베어링(3R), 동일한 뒷 대차에 있던 차축 베어링(4L, 4R) 및 동일 차량의 앞 대차에 설치되어 있던 차축 베어링(1L, 1R, 2L, 2R)을 검사하여 간접적으로 사고의 원인을 추정하 였다. 6) 문답방 : 한국철도공사 그룹포털(전자문서)에 운전제도에 관한 질의 응답방 베어링 잔해 분석 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 23 - [그림14] 사고현장에서 수거한 베어링 잔해 및 엔드캡 검사 결과 [그림15]와 같이 대부분의 차축 베어링 외륜과 롤러에서 일직선 불량 및 충격흠 등 결함이 발견되었다. 차축 베어링이 파손되고(3L) 탈선한 뒷 대차의 나머지 세 개 차축 베어링(3R, 4L, 4R)에서 발견한 결함은 탈선 충격의 영향을 받았을 수 있지만, 탈선하지 않은 앞 대차의 네 개 차축 베어링 (1L, 1R, 2L, 2R)에서도 결함이 발견된 것으로 보아 탈선화차의 차축 베어링 은 사고 이전부터 결함이 있는 상태로 운행하다가 제 기능을 하지 못하여 차축이 절단되어 탈선된 것으로 판단된다. 차축 베어링 외륜과 롤러에서 발 견된 일직선 불량이나 충격흠 등 결함은 과적 또는 편적한 상태로 운행할 때, 열차 편성 시 충격 또는 고정핀을 풀지 않고 크레인으로 화차에서 컨테이너 를 들어 올렸다 화차가 떨어질 때 발생할 가능성이 있는 것으로 확인되었다. 1L : 외륜의 일직선 불량, 필링 1L : 롤러의 일직선 불량, 필링 1R : 외륜의 일직선 불량, 충격흠 1R : 롤러의 일직선 불량 롤러 엔드캡 분석 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 24 - [그림15] 차축 베어링 표면 검사 결과 (3R과 4L의 외륜은 탈선 충격으로 파손된 것으로 추정됨) 2L : 외륜의 일직선 불량 2L : 롤러의 일직선 불량 2R : 외륜의 일직선 불량 2R : 롤러의 일직선 불량 3R : 외륜의 일직선 불량 및 충격흠 3R : 롤러의 일직선 불량 및 파손 4L : 외륜의 일직선 불량 4L : 롤러의 일직선 불량 4R : 외륜의 일직선 불량 및 충격흠 4R : 롤러의 일직선 불량 분석 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 25 - 또한 화차가 특성이 각각 다른 선로를 운행하거나 작업하는 것을 고려하 지 않고 한국철도공사 객화차유지보수기준(내규 35호) 에서 차축 베어링의 분해검사 주기는 주행거리 80만km 또는 회기한도 8년으로, 교환 주기는 주 행키로 320만km로 동일하게 정하여 검사, 교환하는 것은 차축 베어링 결함 으로 발생할 수 있는 탈선사고를 예방하기 적절하지 못하므로 운행환경· 작 업환경에 따라 그 주기에 차이를 두어 관리할 수 있도록 객화차유지보수기 준 을 개정할 필요가 있을 것으로 판단된다. 2.3 컨테이너 상하차 작업 화차에 컨테이너를 싣고 내리는 작업을 조사한 결과 컨테이너 고정핀을 완 전히 풀지 않고 컨테이너를 들어 올리거나 고정핀을 풀었다 하더라도 컨테 이너가 수직 방향으로 들어 올려지지 않으면 컨테이너가 고정핀에서 원활하 게 빠지지 못하고 화차가 함께 들어 올려지다 떨어지는 경우가 발생할 수 있다. 이러한 경우에 차축 베어링 외륜의 내면과 롤러 사이에 충격이 발생하 여 차축 베어링 외륜의 내면과 롤러에 일직선불량, 충격흠 등의 결함이 생기 는 것으로 조사되었다.([그림16]) 컨테이너 상하차 작업 분석 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 26 - 잠긴 상태 풀린 상태 [그림16] 컨테이너 상하차 작업과 고정핀 2.4 그리스 성분 사고 당시에 파손된 차축 베어링 이외의 차량에 남아 있는 차축 베어링에서 그리스를 채취하여 분석함으로써 탈선화차의 차축에 베어링을 설치할 당시 또는 운행 중에 이물질이 유입되었는지 여부를 간접적으로 확인하였다.([그림17]) [그림17] 탈선화차의 차축 베어링에서 채취한 그리스 고정핀 분석 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 27 - 신품 그리스와 파손된 차축 베어링에서 채취한 7종(1L, 1R, 2L, 2R, 3R, 4L, 4R) 그리스 시료의 구조분석을 위해 적외선 분광기를 사용하였다. 구조 분석의 결과 [그림18]과 같이 적외선 분광기 스펙트럼의 주요 피크가 일치하 기 때문에 신품과 차축 베어링에서 채취한 그리스는 동일한 제품(SCH-100, Mobilith 社)일 것으로 분석되었다. [그림18] 적외선 분광기 구조분석 결과 차축 베어링 마모에 의해 그리스에 철(Fe)성분이 섞여 있는지를 확인하기 위해 유도결합 플라즈마 분광기를 사용하였다. 신품 그리스와 차축 베어링에 서 채취한 그리스의 성분을 분석하여 비교하였고, 그 결과는 [표9]과 같다. 유도결합 플라즈마 분광기의 성분분석 결과, 신품에서는 철 성분이 검출되 지 않았으나 나머지 7종에서는 모두 철 성분이 검출되었다. 이는 탈선 충격 이나 차축 베어링에 결함이 있는 상태에서 차량이 주행하여 차축 베어링 구 성 부품들이 마모되어 그리스에 철 성분이 섞인 것으로 분석되었다. 분석 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 28 - [표9] 그리스 성분 분석 결과 (단위: wt%) 그리스에 함유된 수분을 분석하기 위해 수분함량 측정기를 사용하였다. 신 품 그리스와 차축 베어링에서 채취한 그리스의 수분을 분석하여 비교하였고, 그 결과는 [표10]과 같이 신품 및 차축 베어링에서 채취한 그리스에서 수분 이 0.1% 이내로 검출되어 기준(신간선 0.2%)을 만족하는 것으로 분석되었다. [표10] 수분 함량 측정 결과 (단위: wt%) 분석 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 29 - 2.5 종합 분석 탈선화차는 컨테이너를 운반하는 화차로서 차축 베어링 분해검사 결과 차 축 베어링 외륜의 내면과 롤러에서 일직선 불량과 충격흠 등 결함이 발견되 었으며, 차축 베어링의 분해검사 주기도 운행 및 작업환경 등을 고려하지 않 고 동일(주행거리 80만km, 회기 한도 8년)하게 정해져 있었다. 또한, 화차에 컨테이너를 싣고 내리는 작업을 조사한 결과 화차에서 컨테 이너 고정핀을 완전히 풀지 않고 컨테이너를 들어 올리거나 고정핀을 풀었 다 하더라도 컨테이너가 수직 방향으로 들어 올려지지 않으면 컨테이너가 고정핀에서 원활하게 빠지지 못하고 화차가 함께 들어 올려지다 떨어지는 경우가 발생할 수 있다. 이러한 경우에 차축 베어링 외륜의 내면과 롤러 사 이에 충격이 발생하여 결함이 생기는 것으로 판단된다. 탈선화차는 차축 베어링에 결함이 있는 상태로 주행하다가 진상역 진입하 기 이전부터 차축 베어링 발열에 의한 불꽃이 발생되었으나 로컬관제원은 열차감시를 통해 이를 발견하지 못하였다. 탈선화차는 사고 현장 근처에서 외륜이 파손되었고, 롤러가 차축 베어링 내의 정상 위치로부터 이탈되었으며, 이로 인해 차축 베어링이 과열되며 차 축이 절손되어 완사역과 진주역 사이에서 진행방향 좌측으로 탈선하였다. 결론 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 30 - 3. 결론 3.1 조사 결과 3.1.1 사고열차는 2인 승무열차로 기관사는 운전취급규정 제37조(열차의 감 시) 제1항에 따라 기관차 내부 MMI 화면(CCTV)을 활용하여 열차감시 를 시행하였으나 야간 및 우천으로 인하여 확인에 어려움이 있었다. 3.1.2 사고열차의 기관차는 신형기관차로 일반기관차 구조와 달리 박스 형태 이고, 후부감시용 후사경이 설치되어 있지 않았으며, 측면 창문은 작게 설치되어 있었고, 운전실 내부에 설치된 운전자용 후부감시용 CCTV 화면은 MMI를 CCTV 모드로 변경해야 열차 후부를 감시할 수 있는 어려움이 있으므로 기관사가 차축 베어링 발열 등 열차 후부의 이상을 용이하게 감시하기 위한 개선이 필요할 것으로 조사되었다. 3.1.3 사고열차 무전기 녹취록을 확인한 결과 기관사와 로컬관제원 간 통화 내용 중에서‘차량상태의 이상 유무’를 확인해 주었거나 탈선화차 ‘차축 베 어링의 발열(불꽃 등) 상태를 발견하거나 확인’한 내용은 없었다. 3.1.4 역사 및 기관차 CCTV 녹화영상을 확인한 결과 사고열차는 광양역과 진상역 운행 중 차량 하부에서 최초로 불꽃이 발생되는 것을 확인하였고, 이후 하동역 북천역 완사역 통과 중 불꽃이 확인되어 탈선화차의 차축 베어링 발열에 의한 불꽃은 진상역 진입전(삼랑진역 기점 136.300km 지점)부터 발생된 것으로 조사되었다. 3.1.5 광양역과 진주역 로컬관제원은 운전취급규정 제37조의2에 따른 열차 감시를 시행하였으나 차축 불꽃을 발견하지 못하여 탈선사고를 예방하 지 못한 것으로 분석되었다. 3.1.6 탈선화차 뒷 대차의 엔드캡에 차축 베어링 과열 확인을 위해 붙여 놓 은 70도 온도 테이프는 탈락되었거나 변색이 없어 개선이 필요할 것으 결론 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 31 - 로 조사되었다. 3.1.7 한국철도공사 객화차유지보수기준(내규 35호) 에서 차축 베어링의 분해검사 주기는 주행거리 80만km 또는 회기한도 8년으로, 교환주기는 주행키로 320만km로 동일하게 정하여 검사, 교환하는 것은 차축 베어 링 결함으로 발생할 수 있는 탈선사고를 예방하기에는 적절하지 못하 므로 운행환경·작업환경에 따라 차이를 둘 수 있도록 객화차유지보수 기준 을 개정할 필요가 있을 것으로 판단된다. 3.1.8 탈선화차에 설치되어 있던 차축 베어링 분해검사 결과 대부분의 차축 베어링 외륜과 롤러에서 일직선 불량 및 충격흠 등의 결함이 발견되었 다. 차축 베어링이 파손되고 탈선한 뒷 대차의 나머지 세 개 차축 베어 링에서 발견한 결함은 탈선 충격의 영향을 받았을 수 있지만, 탈선하지 않은 앞 대차의 네 개 차축 베어링에서도 결함이 발견된 것으로 보아 탈선화차의 차축 베어링은 사고 이전부터 결함이 있는 상태로 운행한 것이 탈선의 원인이 된 것으로 판단된다. 3.1.9 차축 베어링 외륜의 내면과 롤러에서 발견된 일직선 불량이나 충격흠 등 결함은 과적 또는 편적한 상태로 운행하거나, 화차에 컨테이너를 싣고 내리는 작업을 할 때 컨테이너 고정핀을 완전히 풀지 않고 컨테 이너를 들어 올리거나 고정핀을 풀었다 하더라도 컨테이너가 수직 방 향으로 들어 올려지지 않으면 컨테이너가 고정핀에서 원활하게 빠지지 못하고 화차가 함께 들어 올려지다 떨어지는 경우가 발생할 수 있다. 이러한 경우에 차축 베어링 외륜의 내면과 롤러 사이에 충격이 발생되 면서 차축 베어링 외륜의 내면과 롤러에 결함이 생길 수 있을 것으로 조사되었다. 3.1.10 적외선 분광기 이용하여 그리스 구조 분석을 수행한 결과 신품과 탈 선화차 차축 베어링에서 채취한 그리스는 동일 제품일 것으로 분석되 었다. 결론 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 32 - 3.1.11 유도결합 플라즈마 분광기의 성분분석 결과, 신품에서는 철 성분이 검 출되지 않았으나 나머지 7종에서는 모두 철 성분이 검출되었다. 이는 탈선 충격이나 차축 베어링에 결함이 있는 상태에서 차량이 주행하여 차축 베어링 구성 부품들이 마모되어 그리스에 철 성분이 섞인 것으로 분석되었다. 3.1.12 그리스 수분함량 측정 결과 신품 및 차축 베어링에서 채취한 그리스 에서는 수분이 0.1% 이내로 검출되어 기준을 만족하는 것으로 분석되 었다. 3.1.13 사고구간의 선로상태 확인을 위한 궤도검측차 검측 결과는 양호하였 으며, 도보순회점검도 주기적으로 시행하였고 특이사항은 없었다. 3.1.14 사고열차는 완사역을 통과하여 진주역 쪽으로 운행하면서 사고지점에 정차할 때까지 신호와 진로는 모두 양호한 상태였다. 3.2 사고 원인 항공·철도사고조사위원회는 이번 사고의 주원인을 ‘차축에 설치되어 있는 차축 베어링 외륜의 내면과 롤러에 결함이 있는 상태에서 운행 도중 차축 베어링이 파손되면서 차축이 절손되어 탈선한 것’으로 결정하였다. 기여요인으로는 ‘화차에 컨테이너를 싣고 내릴 때 차축 베어링에 충격이 가해진 것, 차축 베어링의 분해검수 주기와 교환 주기가 화차에 적재되는 화 물의 종류, 적재방법 및 사용환경의 차이에도 불구하고 동일하게 규정하여 관리한 것’으로 결정하였다. 안전 권고 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 33 - 4. 안전 권고 항공․철도사고조사위원회는「항공․철도사고조사에 관한 법률」제26조에 따라 2019년 2월 18일 경전선 완사역∼진주역 사이에서 발생한 화물열차 탈 선사고에 대하여 다음과 같이 권고한다. 4.1 한국철도공사에 대하여 4.1.1 화차 차축 베어링의 분해검수 주기와 교환 주기를 화차에 적재되는 화 물의 종류, 적재방법 및 사용환경에 따라 차이를 두어 유지보수에 적용 할 수 있도록 연구용역 등을 통하여 차축 베어링의 분해검수 주기와 교환 주기를 단축할 수 있도록 객화차 유지보수 기준 을 개정할 것 4.1.2 컨테이너를 화차에서 들어 올릴 때 화차가 함께 들어 올려지다 떨어지지 않도록 작업 절차를 개선하고 이를 감시할 수 있는 시스템을 갖출 것 4.1.3 화차의 발열 여부를 발견하기 위해 차축에 부착한 온도 테이프는 효과가 없는 것으로 밝혀진 바, 이를 대체할 방법을 수립하고 시행할 것 4.1.4 역근무자가 열차 감시 업무를 적정하게 수행할 수 있도록 열차감시용 영상감시설비의 확대 설치 등 방안을 마련하여 시행할 것 4.1.5 신형기관차로 열차를 편성하여 운행시 열차 후부를 쉽게 감시할 수 있 도록 개선할 것 - 34 - 　　이 보고서는 사고조사 과정에서 관계인들로부터 청취한 진 술 및 개인정보 등이 포함되어 있어, 　『항공․철도사고조사에 관한 법률』제28조(정보의 공개금지) 및 같은 법 시행령 제8조(공개를 금지할 수 있는 정보의 범위)에 의 하여 이 보고서(인쇄본)에 개인정보는 공개하지 않았으며,</p>\n",
            "          <p id=\"s1p7\">국민 여러분의 이해를 돕기 위해 전문 철도용어를 쉽게 풀어서 쓴 점을 양해하여 주시기 바랍니다. 자세한 사항은 항공․철도사고조사위원회로 문의하여 주시기 바랍니다. 항공․철도사고조사위원회 http://www.araib.go.kr 전화 : 044-201-5443 E-mail: mrlee56@korea.kr</p>\n",
            "        </div>\n",
            "      </section>\n",
            "    </article>\n",
            "  </body>\n",
            "</html>\n",
            "\n",
            "<class 'str'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "check = re.sub('(<([^>]+)>)', '', read).strip()\n",
        "check = re.sub('\\n', '', check)\n",
        "check = re.sub(' +', ' ', check)\n",
        "print(check)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwTz4mE4RgX2",
        "outputId": "7d7bcb6f-e62c-47ad-919d-7c9656378f3a"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a3DnzcVSvJKG25OeznRpiW5zLgi8-sample.pdf 보고서 번호: ARAIB/R 2020 - 3 철도사고 조사보고서 한국철도공사 경전선(완사역→진주역 사이, 삼랑진역 기점 103.286km) 제3081화물열차(디젤 7620호 + 화차 25량) 열차탈선 2019년 2월 18일(월) 21시 56분경 2020. 6. 19. 항공･철도사고조사위원회 이 조사보고서는 「항공․철도사고조사에 관한 법률」제 2조에 의거 사고조사가 이루어졌으며, 제25조에 따라 작 성되었다. 같은 법률 제1조에서 「철도사고 조사는 독립적이고 공정 한 조사를 통하여 사고 원인을 정확하게 규명함으로써 철도 사고의 예방과 안전 확보에 이바지함」을 목적으로 하고 있 다. 또한, 제30조에 따라 사고조사는 민․형사상 책임과 관련된 사 법절차, 행정처분 절차 또는 행정 쟁송 절차와 분리․수행되어 야 하고, 제32조에서위원회에 진술․증언․자료 등의 제출 또는 답변을 한 사람은 이를 이유로 해고․전보․징계․부당한 대우 또 는 그 밖에 신분이나 처우와 관련하여 불이익을 받지 아니 한다.라고 규정하고 있다. 그러므로 이 조사보고서는 철도 분야의 안전을 증진시킬 목 적 이외의 용도로 사용되어서는 아니 된다. 차례 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - i - 차 례 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 ························ 1 개요 ······························································································································· 2 1. 사실 정보 ··············································································································· 3 1.1 사고의 경위 ····································································································· 3 1.2 피해 사항 ········································································································· 4 1.3 인적 정보 및 업무 수행사항 ······································································· 4 1.4 현장 정보 ······································································································· 10 1.5 열차와 차량 정보 ························································································· 11 1.6 차량정비단의 차축 베어링 분해 검수 실태 ··········································· 16 1.7 신호 및 전차선 정보 ··················································································· 18 1.8 기상 정보 ······································································································· 19 2. 분석 ························································································································ 20 2.1 업무 수행사항 분석 ····················································································· 20 2.2 차축 베어링 분해 검사 ··············································································· 22 2.3 컨테이너 상하차 작업 분석 ······································································· 25 2.4 그리스 성분 분석 ························································································· 26 2.5 종합 분석 ······································································································· 29 3. 결론 ························································································································ 30 3.1 조사 결과 ······································································································· 30 3.2 사고 원인 ······································································································· 32 4. 안전 권고 ·············································································································· 33 4.1 한국철도공사에 대하여 ················································································· 33 제목 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 1 - 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 ○ 운영기관 : 한국철도공사 ○ 운행노선 : 경전선 ○ 발생장소 : 완사역∼진주역 사이 (삼랑진역 기점 103.286km 지점) ○ 사고열차 : 제3081화물열차 (황등역16:38→부산항역22:41) [디젤 7620호 + 화차 25량 편성, 환산 33.9량(컨테이너 실은 차)] ○ 사고유형 : 열차탈선 ○ 발생일시 : 2019년 2월 18일(월) 21시 56분경 [그림1] 사고현장 개요 개요 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 2 - 개요 2019년 2월 18일 16:38에 전라선 황등역을 출발하여 부산항역으로 운행 중 이던 제3081호 화물열차(이하 ‘사고열차’라 한다)가 21:56경 경전선 하행선 방 향 완사역과 진주역 사이에서 비상제동이 체결되어 정차하였다. 정차 후 사고 열차 기관사가 확인한 결과 13번째 화차(976296호, 이하 ‘탈선화차’라 한다)의 뒷 대차 첫 번째 차축과 두 번째 차축이 진행방향 좌측으로 탈선되었고, 좌 측 세 번째 차축이 절손되었으며, 탈선화차와 14번째 화차 사이의 공기호스 가 빠져있는 것이 발견되었다. 이 사고로 인명피해는 발생하지 않았으나, 탈선화차의 차축이 절손되고 대 차부품 일부와 선로시설의 PCT침목, 체결구 등이 파손되는 피해가 발생하 였다. 항공·철도사고조사위원회는 이번 사고의 주원인을 ‘차축에 설치되어 있는 차축 베어링 외륜의 내면과 롤러에 결함이 있는 상태에서 운행 도중 차축 베어링이 파손되면서 차축이 절손되어 탈선한 것’으로 결정하였다. 기여요인으로는 ‘화차에 컨테이너를 싣고 내릴 때 차축 베어링에 충격이 가해진 것, 차축 베어링의 분해검수 주기와 교환 주기가 화차에 적재되는 화 물의 종류, 적재방법 및 사용환경의 차이에도 불구하고 동일하게 규정하여 관리한 것’으로 결정하였다. 항공․철도사고조사위원회는 한국철도공사에 5건의 안전권고를 한다. 사실정보 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 3 - 1. 사실 정보 1.1 사고의 경위 2019년 2월 18일 16:38에 황등역을 출발하여 부산항역으로 운행 중이던 사 고열차의 탈선화차 3번째 좌측 차축 베어링이 과열되어 불꽃을 일으키며 진 행하다가 차축 베어링의 과열로 차축이 절손되어 21:52경 신기터널 출구 부 근(삼랑진역 기점 106.135km)에서 엔드캡이 탈락되었으며, 이후 베어링이 분 해되며 롤러 및 베어링 외륜 등 베어링 부품이 선로변으로 흩어졌다. 사고열 차는 완사역과 진주역 사이(삼랑진역 기점 103.286km)에서 탈선화차의 3위축 과 4위축이 진행방향 좌측으로 탈선하여 약 886m를 진행하다 탈선화차와 14 번째 화차 사이의 공기호스가 빠지면서 비상제동이 체결되어 정차(삼랑진 기점 102.400km)하였다. 선로변에서 발견한 차축 베어링 부품의 위치와 탈선위치 등은 [그림2]와 같다. [그림2] 차축 베어링 부품 발견위치와 탈선위치 주행방향 사실정보 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 4 - 1.2. 피해 사항 1.2.1 인명 피해 이 사고로 인명 피해는 발생하지 않았다. 1.2.2 물적 피해 이번 사고로 화차 1량이 탈선되면서 대차 프레임, 브레이크빔, 차축, 스프 링 등이 파손되어 차량 분야에서 약 1,088만원, PC침목 402개, e-코일 1,500 개, 체결구 60세트 등이 파손되어 2,847만원, 솔티터널 복구 공사비 2억6,364 만원 등 선로 분야에서 2억9,211만원의 피해가 발생하여 총 3억299만원의 피해가 발생하였으며 운행에 지장을 받은 열차는 없었다. 1.3 인적 정보 및 업무 수행 사항 1.3.1 사고열차 기관사 사고열차 기관사(58세, 남)는 1980년 11월 1일 (구)철도청 광주기관차사무 소 기관조사로 임용된 후 1987년 12월 31일 기관사로 임용되었으며, 2006년 7월 1일 디젤과 전기 1종 철도차량 운전면허를 취득하여 2018년 7월 1일자 로 지원기관사로 근무하고 있었다. 사고열차 기관사는 2016년 적성검사에 합격하였고, 사고당일 19시경 출근 하여 승무적합성 검사를 시행하였으며 결과는 양호하였다. 사고열차 기관사는 순천역 발차 후 제동 감도 시험 후 광양역, 진상역, 하 동역, 횡천역, 완사역을 차례로 통과하고 8/1000 상구배에 접어들면서 가감 간을 2단으로 운행 중 갑자기 비상제동이 체결되었다고 진술하였다. 사고열 차 기관사는 진상역 진입 시 열차 후부를 본 적이 있으나 이상 상태는 감지 하지 못했고, 이후에는 선로상태가 거의 직선이어서 열차 후부를 보는 것은 사실정보 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 5 - 불가능하였고, MMI1) 화면의 CCTV로 볼 수는 있지만 선명하지 않아 확인할 수 없었다고 진술하였다. 정차 후 비상제동을 완해하기 위해서 1분 이상을 기다린 후 완해하였으나 주공기는 정상이고 제동관 공기가 3.5바(bar) 이상 충기되지 않아 다시 한 번 비상제동을 체결한 후 완해를 시키는 절차를 수행했으나 동일 현상이 발생 되어 진주역을 무전으로 호출하여 열차가 비상제동이 체결되어 정차하고 있 다는 통보를 하였고, 사고열차 보조기관사로 하여 기관차와 화차사이의 제동 관 연결콕크를 잠궈보라고 하니 공기압이 정상 회복되어 화차의 문제라 판 단하고 사고열차를 점검하도록 지시 하였다고 진술하였다. 1.3.2 사고열차 보조기관사 사고열차 보조기관사(59세, 남)는 1985년 12월 19일 (구)철도청 순천기관차 승무사무소에 기관조사로 전입되어 1991년 12월 05일 기관사로 임용되었으 며, 2006년 7월 1일 디젤과 전기 1종 철도차량 운전면허를 취득하여 지원기 관사로 근무하고 있었다. 사고열차 보조기관사는 사고 당일 19시경 출근하여 승무적합성 검사를 시 행하였으며 결과는 양호하였다. 사고열차 보조기관사는 순천역∼마산역 간 사고열차 사업담당으로 순천역 ∼진주역 간은 보조기관사, 진주역에서 마산역까지는 기관사 업무를 수행할 예정으로 완사역을 계획보다 1시간12분 늦게 통과(현 21:53)하여 삼량진역 기점 102.700km 지점(8‰ 상구배) 운행중 갑자기 비상제동이 체결되면서 정차하였다. 비상제동 원인을 찾던 중 사고열차의 13번째 화차 뒷 대차(3,4위)가 탈선 되어 있음을 확인하고 구로교통관제센터와 진주역에 통보(현 23:15)하였다. 1) MMI((Man Machine Interface) : 차량의 상태를 화면으로 보여주는 장치 사실정보 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 6 - 1.3.3 진주역 로컬관제원 진주역 로컬관제원(48세, 남)는 1999년 (구)철도청 역무원으로 임용되어 2015년 1월 1일부터 진주역 로컬관제원으로 근무하고 있었다. 진주역 로컬관제원은 2015년 8월 20일 로컬관제원 보수교육을 수료하였고, 2016년 3월15일 적성검사를 통과하였으며, 2018년 6월25일 신체검사 결과는 양호하였다. 사고당일 진주역 로컬관제원은 야간근무시(21:00~익 02:00) 1231열차의 입 환작업을 준비하던 중 21시57분경 사고열차 기관사로 부터 비상제동이 체결 되어 공기압이 올라가지 않는다는 내용을 무전으로 통보받고, 구로교통관제 센터에 관련 사실을 통보하고, 역무팀장에 보고하였으며, 23시18분 전남본부 ㅇㅇㅇ으로부터 사고열차가 탈선되었다는 통보를 받고 사고열차 복구 관련 업무를 수행하였다고 진술하였다. 1.3.4 북천역 로컬관제원 북천역 로컬관제원은 사고열차가 1시간13분 지연되어 21시46분경 북천역 51호 선로전환기에서 21호 선로전환기까지 3번선 통과 운행시 정상적인 신 호에 의해 운행하였고, CCTV를 통해 확인하였을 때도 다른 이상이 없었다 고 진술하였다. 1.3.5 사고열차(DL7620호) 운행기록 사고열차의 운행기록을 분석해 본 결과 [표1]과 같이 21:54:12경(③) 약 81km/h 속도로 운행 중 기관차 출력을 최대로 상승시켰으나 속도가 74km/h로 떨어졌다. 사실정보 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 7 - 21:54:43경(⑥) 약 73km/h의 속도에서 8단으로 견인 중 제동관 압력 이 급감하면서 비상제동이 체결되었고 약 231m 이동 후 정차되었다.(⑦) [표1] 사고열차의 운행기록 정보 구분 시간 속도(km/h) 거리(m) 차 량 상 태 비고 ① 21:07:01 66 51,869.3 순천역 출발 평균속도 ② 21:54:01 81 247.5 가감간 유전⇒1단 상승 ③ 21:54:12 81 244.4 가감간 1단에서 상승 시작 화차탈선(추정) ④ 21:54:23 79 403.8 가감간 8단 유지 속도가 떨어짐(81km/h⇒74km/h)⑤ 21:54:42 74 20.4 ⑥ 21:54:43 73 231.7 제동관 압력 급강하, 비상체결 약 231.7m 이동 후 정차⑦ 21:55:05 0 0 열차 정지 ⑦⑥④~⑤③ 사실정보 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 8 - 1.3.6 열차무선 녹취록 사고열차의 운행과 관련하여 [표2]와 같이 로컬관제원과 기관사의 무전기 녹취록을 확인한 결과 사고열차는 21:03에 순천역에 도착하여 21:07에 순천 역을 출발하였으며, 21:13경 광양역을 통과하는 과정의 녹취록은 잡음이 심해 통화내용을 확인할 수 없었다. 22:05경 기관사가 진주역을 호출하여 운행 중 갑자기 비상제동이 체결되어 열차가 정지되어 원인을 찾고 있다고 하였고, 이후 현장에서 차량상태를 점검 하여 열차의 탈선을 확인하였다. 사고열차 무전기의 통화내용 녹음 기록 중 탈선화차에 대한 차축 베어링 의 이상(불꽃 등)을 발견하거나 확인한 내용은 없었다. 시 간 사고열차 기관사 로컬관제원 비 고 21:03 철도 3081열차 후부 양호. 순천역 이상! 순천역 21:06 철도 3081 교대 완료, 준비완료! 철도 3081 순천역 4번선에 정차 한 상선 출발확인 발차합시다. 순천역 로컬관제 이상! 21:07 철도 3081 발차, 재확인 이상! 21:13 철도 30**열차 이상!(잡음 심함) 광양역 22:05 철도 진주 3081 ~ 예 지금 102km200부근에 갑자기 비상 제 동이 체결되어 정차해 있습니다. 예, 지금 원인을 찾고 있습니다. 이상감지, 이후 열차 탈선확인 [표2] 열차무선 녹취록 1.3.7 CCTV 확인(역 및 기관차) 사고열차가 순천역을 출발 후 탈선되어 정차할 때까지 [그림3]과 [그림4]와 같이 역사 및 기관차 CCTV에 녹화된 영상을 확인한 결과 광양역과 진상역 간 운행 중 차량 하부에서 최초로 불꽃이 발생되는 것을 확인하였으며, 이후 하동역 북천역 완사역 통과 중에도 불꽃이 발생되는 것을 확인하였다. 사실정보 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 9 - ① 진상역 통과 중(불꽃 발생) ② 하동역 통과 중(불꽃 발생) ③ 북천역 통과 중(불꽃 발생) ④ 완사역 통과 중(불꽃 발생) [그림3] 순천역∼완사역 역사 CCTV 녹화 화면 이상 없음 하부 불꽃 ① 광양역 통과 중(이상 없음) ② 광양역∼진상역 운행 중(불꽃 발생) 하부 불꽃 21:55:05 열차정지 ③ 완사역∼진주역 운행 중(불꽃 발생) ④ 열차 정차 [그림4] 기관차 7620호 CCTV 녹화 화면 사실정보 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 10 - 1.4 현장 정보 1.4.1 최초 탈선위치 및 최종 정차위치 사고열차의 최초 탈선흔적은 [그림5]과 같이 삼랑진역 기점 약 103.286km 지점에서 발견되었으며, 탈선화차의 세 번째 차축의 좌우 차륜이 열차 진행 방향 좌측으로 떨어지면서 자갈에 끌린 흔적을 남겼다. 사고열차는 최초 탈선위치에서 약 886m를 더 진행하여 삼랑진역 기점 약 102.400km 지점에 정차되었다.([그림6]) [그림5] 최초 탈선위치 및 최종 정차위치 [그림6] 사고열차 탈선상태 1.4.2 선로 정보 사고지점(삼량진역 기점103.290km)이 포함된 경전선 진주역∼완사역 간은 2017년 경전선 진주역∼광양역 복선으로 개통되었으며, 사고개소의 본선 선형은 열차진행 방향으로 우곡선(R:3000m, 곡선 시점101.687km-종점 최초 탈선위치 (삼랑진역 기점 103.286km) 최종 정차위치(삼랑진역 기점 102.400km) 사실정보 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 11 - 103. 464km, 연장 1,777m)으로 설정 캔트는 50mm이며, 선로기울기는 내리막 1.0‰이고, 최고속도는 150km/h이다. 사고지점의 궤도는 60kg 장대레일에 PC침목, 코일스프링 크립, 도상자갈 두께 320mm로 부설되어 있었으며, 솔티터널2) 접속부는 콘크리트 도상으로 레일을 침목에 체결하는 형식은 레다20003)과 터널 내부는 시스템300-14)로 구성되어 있었다. 선로유지관리 상태를 보면 궤도검측차 운행은 분기1회 시행하고 있었으며, 2018년 1/4분기(4/5∼4/5), 2/4분기(5/31∼6/1), 3/4분기(9/11∼9/12), 4/4분 기(11/14∼11/15) 4회 검측한 결과는 양호하였다. 또한, 삼랑진역 기점 101.000km∼114.500km 구간에 대하여 2019년 1월 7 일, 1월 14일, 1월 21일, 2월 1일, 2월 7일, 2월 11일, 2월 18일 도보순회 점검을 시행한 결과에도 탈선에 영향을 줄 만한 사항은 없었다. 유지보수 작업은 인력작업과 보선장비 작업을 병행하였으며, 보선장비 작업은 주로 1종 기계작업으로 시행한 것을 확인하였다. 1.5 열차와 차량 정보 1.5.1 사고열차의 조성 사고열차는 [표3]과 같이 디젤기관차(DL7620호)와 컨테이너 화차 25량으로 조성(환산 33.9량, 열차장 30.1량)되어 있었으며 화물이 적재된 컨테이너를 싣 고 있었다. 2) 솔티터널(복선) : 경전선 진주역-완사역간 102km497~102km906 ( L = 409m) 3) 레다2000 : 레티스 철근으로 연결된 RCT Twin Block 침목을 콘크리트 도상에 매립하고 레일과 침 목사이에 탄성재를 삽입 4) 시스템300-1 : 트러프를 시공 후 트러프 내부에 탄성식 베이스 플레이트로 구성된 PCT Mono Block 침목과 침목 관통 종방향 보강 철근과 횡철근을 매입 사실정보 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 12 - ⑬976296 ☜완사(순천)방 향 진주(부산)방향 ☞ ①◯25 열차진행방향 [표3] 사고열차의 조성 현황 1.5.2 사고열차 제원 사고열차에 조성되어 있던 기관차와 화차의 제원은 각각 [표4] 및 [표5]와 같다. 조성순서 차량번호 차종명 차량톤수 하중톤수 공차환산 영차환산 차 길이 기관차 7620 디젤76 132 124 4.2 4.4 1.5 1 70085 컨테2TEU 18.5 50 0.5 1.6 1 2 763025 컨테이너전용 23.1 51.5 0.6 1.7 1.1 3 70911 컨테2TEU 21.4 62.6 0.5 1.9 1 4 76071 컨테2TEU 18.1 50 0.5 1.6 1 5 70965 컨테2TEU 21.4 62.6 0.5 1.9 1 6 70883 컨테2TEU 21.4 62.6 0.5 1.9 1 7 70385 컨테2TEU 17.7 50 0.4 1.6 1 8 976211 컨테3TEU 20 54.5 0.5 1.7 1.4 9 976509 컨테3TEU 20.8 56.5 0.5 1.8 1.4 10 760872 컨겸50 21 50 0.5 1.6 1.1 11 70162 컨테2TEU 17.5 50 0.4 1.6 1 12 70394 컨테2TEU 17.7 50 0.4 1.6 1 13 976296 컨테3TEU 20.6 56.5 0.5 1.8 1.4 14 763349 컨겸평판 22.8 52 0.6 1.7 1.1 15 763359 컨겸평판 22.8 52 0.6 1.7 1.1 16 71020 컨겸평판 22.7 52 0.6 1.7 1.1 17 975209 컨테3TEU 18.2 56.5 0.5 1.7 1.4 18 975276 컨테3TEU 18.2 56.5 0.5 1.7 1.4 19 970505 냉동컨테이너 19.5 48 0.5 1.6 1 20 763071 컨겸평판 23.1 51.5 0.6 1.7 1.1 21 71027 컨겸평판 22.7 52 0.6 1.7 1.1 22 975237 컨테3TEU 18.2 56.5 0.5 1.7 1.4 23 70948 컨테2TEU 21.4 62.6 0.5 1.9 1 24 71033 컨겸평판 22.7 52 0.6 1.7 1.1 25 975262 컨테3TEU 18.2 56.5 0.5 1.7 1.4 사실정보 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 13 - 구 분 제 원 구 분 제 원 기관차번호 7620 최고속도 150km/h 차 종 디젤기관차 자 중 132t 도입일 2014. 6.30. 길 이 21,168㎜ 대차형식 볼스타리스 용접대차 높 이 4,150㎜ 제동장치 발전제동, 답면제동 폭 2,870㎜ 연결기형식 E 형 고정축거 4,200㎜ [표4] 기관차 주요 제원 구 분 제 원 구 분 제 원 차량번호 976296 최고속도 90km 차 종 평판차 자 중 20.6t 세부차종 컨테이너 3T 하 중 56.5t 도입일 1996.01.09 길 이 19,570㎜ 대차형식 바바 높 이 1,065㎜ 제동장치 공기제동(K2) 폭 2,579㎜ 연결기형식 E 형 고정축거 1,676㎜ [표5] 탈선화차의 주요 제원 1.5.3 탈선화차의 정비 현황 탈선화차의 종별 검수 이력은 [표6]과 같다. 검종 시행일 시행 장소 ES(기본정비) ‘19.2.15., ‘19.2.13., ‘19.2.11. 익산차량사업소 LI6(경정비) ‘18.12.19. ‘17.11.23.-11.24. ‘16.9.20.-9.21. 부산신항차량사업소 부산차량사업소 사외정비(고려차량) GI1(중정비1) ‘18.7.5. ‘16.4.4.-4.6. 부곡차량사업소 사외정비(고려차량) GI2(중정비2) ‘17.4.6.-5.4. ‘15.1.12.-1.16. 부산철도차량정비단 사외정비(고려차량) [표6] 탈선화차의 종별 검수 이력 탈선화차는 한국철도공사 객화차유지보수기준 제18조에 따라 주행장치 대차 각부 분해세척, 청소 후 액체침투 탐상, 차축 롤러 베어링 분해세척 후 각 부품의 상태 확인 및 차륜삭정 등을 시행하는 중정비2(GI2)를 2017년 5월 2일 실시하였고 검사 이후 사고당일까지 127,448.6㎞를 주행하였다. 사실정보 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 14 - 탈선화차의 차축에 대한 초음파탐상 및 유간측정 등의 결과는 [표7]과 같다. [표7] 탈선화차 윤축검사 내용 1.5.4 탈선화차의 차축 베어링 탈선화차 차축에는 TIMKEN사와 SKF사의 RCT NFL5) 차축 베어링이 혼 합 설치되어 있었으며, 탈선화차의 절손된 차축에는 SKF사의 차축 베어링이 설치되어 있었다. 이 베어링은 신차 도입 시 최초 설치되었고 차축 베어링의 구조는 [그림7]과 같다. 설치 볼트 엔드캡 씰 마모링 오일 씰잠금 플레이트 베어링 외륜 스페이스 내륜 및 롤러 조립 백킹링오일 씰 씰 마모링 내륜 및 롤러 조립 [그림7] RCT형 NFL 차축 베어링 구조 5) RCT NFL : Rotate end Cap and Tapered roller bearing No Field Lubrication의 약자로서, 정상 운행 중이면 장기간 그리스의 재급유가 필요하지 않은 그리스 윤활방식의 테이퍼 롤 러 베어링 위수 차축탐상 베어링 번호 제작사/년월 축방향유간 내경 감도 주파수 판독 R L R L R L R L 기준값 35 3M PN(양호) 0.25~0.50㎜ 131.75~131.80㎜ 1 35 3M PN 9708 76553 0007 34619 TK 97.08 TK 00.07 0.38 0.32 131.78 131.78 2 35 3M PN 0208 54537 0111 39836 TK 02.08 TK 01.11 0.32 0.32 131.78 131.78 3 35 3M PN 0312 004385 0111 57918 SKF 03.12 SKF 01.11 0.35 0.34 131.78 131.78 4 35 3M PN 0212 91188 0709 559109 TK 02.12 TK 07.09 0.30 0.38 131.78 131.78 사실정보 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 15 - 1.5.5 탈선화차의 축상 및 차륜의 상태 탈선화차의 뒷 대차의 왼쪽 앞 차축은 [그림8]과 같이 절손되었으며, 엔드캡 부분이 절손된 상태의 차축이 사고현장에서 발견되었다. [그림8] 절손된 탈선화차 차축 탈선화차 뒷 대차의 엔드캡 상태는 [그림9]와 같고 차축 베어링 과열 확인 을 위해 붙여 놓은 70도 온도 테이프는 탈락되었거나 변색이 없었다. 또한 모든 베어링에서 그리스 누유 흔적은 발견되지 않았다. [그림9] 탈선화차 차축의 베어링 조립체 3L : 70도 온도 테이프 없음 3R : 70도온도테이프변색없음 4L : 70도 온도 테이프 변색 없음 4R : 70도 온도 테이프 변색 없음 사실정보 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 16 - [그림 10]은 사고현장에서 수거한 탈선화차의 파손된 차축 베어링과 정상 적인 차축 베어링을 비교한 것이다. 파손된 차축 베어링은 내륜, 외륜, 롤러, 케이지 등이 심하게 훼손된 상태였다. [그림10] 정상 차축 베어링(상)과 비교한 파손된 차축 베어링(하) 1.6 차축 베어링 분해 검수 실태 한국철도공사 차량정비단에서는 객화차유지보수기준 에 따라 중정비 중 차축 베어링 분해검수 기준(주행키로 80만km, 회기한도 8년)에 도달하는 차 축 베어링에 대해서는 세척→내륜의 청소→검사→외륜의 검사→ 각부의 측정→표기, 기록→조립→그리스 주입→오일 씰 조립 후 차축에 압입하는 분해검수 과정으로 중정비를 시행하고 있었다. 탈선차량의 경우 사고 이전(‘17.4.28) 중정비2(GI2) 시행 시 차축 베어링을 분해 세척한 후 [그림11] 처럼 외륜과 롤러 표면의 필링(미세 박리), 브리넬 링(표면 변형), 스폴링(피로성 박리) 등을 돋보기와 디지털 현미경으로 확인 하고 있었으며, 차축 베어링 유간 및 내륜 측정기로 치수를 측정하였으며, 탈선화차의 롤러 베어링 분해 및 조립 시 검사항목은 [그림12]과 같으며, 모든 측정값은 모두 기준치 이내로 관리되고 있었다. 사실정보 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 17 - 롤러 표면 육안 검사 롤러 표면 현미경 검사 유간 및 내륜 측정기 베어링 내륜 측정 [그림11] 차량정비단의 베어링 분해검사 장면 [그림12] 차축 베어링 분해검사 및 조립 후 검사기준치 및 측정치 3L (절손) 3R (탈선) 4L (탈선) 4R (탈선) 사실정보 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 18 - 1.7 신호 및 전차선 정보 1.7.1 신호정보 사고구간의 신호설비는 전자연동장치, 복선 자동폐색 5현시, 구내 임펄스 궤도회로, 역간 AF 궤도회로, ATS 방식으로 구성되어 있다. 전기기술지원시스템 기록에는 [그림13] ①②③④와 같이 사고열차가 완사 역 상장내-2번선 진로로 진입하여 완사역 2번선에서 상출발 신호현시 상태에 서 진출한 후 사고지점에 정차할 때까지 신호와 진로는 모두 양호한 상태였 으며, 사고열차는 완사역∼진주역 구간을 운행하다가 탈선되어 886m를 더 진행 후 B8303T와 B8305T 구간에서 비상제동이 체결되어 정차되었다. ③ 사고열차 완사역 2번선-상출발 진출2019. 2.18. 21:53:32 ④ 사고열차 완사-진주간 탈선 8303T 정차2019. 2.18. 21:56:09 [그림13] 완사역∼진주역간 사고열차 운행 기술지원시스템 기록 상황 ① 사고열차 완사역 2번선 통과 현시2019. 2.18 21:47:10 ② 사고열차 완사역 상장내 진입 전2019. 2.18. 21:52:17 사실정보 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 19 - 1.7.2 전차선 정보 사고구간은 복선 비전철 구간으로 이번 사고와 관련이 없다. 1.8 기상정보 기상청 자료에 따르면 사고구간인 진주시의 당시 날씨는 약간의 비가 내 렸고, 기온은 4.9℃, 습도는 59%로 나타났다. 분석 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 20 - 2. 분석(Analysis) 2.1 업무 수행사항 2.1.1 사고열차 기관사 사고열차 기관사는 음주 등 승무적합성 검사에 대하여 위반 사항은 없었 으며, 사고열차 운행 시 운행기록 분석 결과 제한속도를 초과하지 않았으며, 열차에 충격을 줄 만한 기기 취급은 없는 등 적정한 승무업무를 수행하였다. 2.1.2 열차감시의 적정성 사고열차 무전기 녹취록을 확인한 결과 기관사와 로컬관제원 간 통화내용 중‘차량상태 이상 유무’를 확인해 주거나 탈선화차‘차축 베어링의 발열(불꽃 등)을 발견하거나 확인’한 내용은 없는 것으로 조사되었다. 사고열차가 출발 후 탈선되어 정차할 때까지 역사 및 기관차 CCTV 녹화 영상을 확인한 결과 탈선화차의 차축 베어링 발열에 의한 불꽃은 광양역과 진상역 사이 운행 중 차량 하부에서 최초로 불꽃이 발생되는 것을 확인하였 고, 이후 하동역 북천역 완사역 통과 중에도 불꽃이 발생되는 것이 확인되었 다. 당시 사고열차의 운행 속도를 80km/h라고 하고 불꽃발생 지점을 추정해 보면, ① 광양역(삼량진역 기점 148.500km) 지점의 통과 시각 21시17분20초 ② 최초 불꽃 확인된 지점의 통과시각 21시26분29초 ③ ①-②지점 운행시각 = 21시26분29초-21시17분20초 = 9분9초 = 549초 ④ 당시 속도 80km/h를 초속으로 환산하면 22.2m/sec ⑤ 이동거리 22.2m/sec × 549초 ≒ 12,200 m = 12.2km ⑥ 최초 불꽃발생 지점은 148.500km–12.2km = 136.300km 지점으로 추정됨 분석 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 21 - 또한 열차의 감시업무에 대하여 한국철도공사 운전취급규정 (개정 2018. 5. 8.) 37조와 제37조의 2에서 [표8]과 같이 규정하고 있다. 제37조(열차의 감시) ① 열차가 정거장에 도착·출발 또는 통과할 때와 운행 중인 열차 의 감시는 다음 각 호에 따른다. 1. 기관사 가. 견인력 저하 등 차량이상을 감지한 경우 열차상태를 확인 후 운행할 것 나. 열차교행, 역 출발 또는 통과 시 무선전화기 수신에 주의할 것 다. 동력차 2인 승무열차의 기관사(또는 부기관사)는 운행 중 각 정거장간 1회 이상 뒤를 감시하여 열차의 이상 유무를 확인하여야 하며, 열차가 정거장을 출발하거나 통과할 경우 때때로 뒤를 감시하여 열차의 상태와 역장 또는 열차승무원의 동작 에 주의할 것 라. 동력차 1인 승무열차는 뒤감시 생략 제37조의2(영상감시설비에 의한 열차의 감시) ① 열차감시 지정역의 역장은 조작반 취 급, 열차무선교신 등 운전취급에 지장 없는 범위에서 영상감시장비를 활용하여 열차가 정거장에 진입 또는 진출할 때 주행장치 등의 이상 유무를 감시하여야 한다. 다만, 고 정편성 여객열차 및 동력차만으로 조성한 열차는 감시를 생략할 수 있다. ② 영상감시설비에 의한 열차감시 지정역은 별표 26과 같다. 영상감시설비에 의한 열차감시 지정역(제37조의2 [별표 26] 관련) 선 명 대상역 경전선 진영, 마산, 진주, 광양 [표8] 열차감시 관련 한국철도공사 「운전취급규정」 발췌 사고구간의 영상감시설비에 의한 열차의 감시는 [표8]의 [별표26]과 같고 열차감시 지정역은 ‘광양역과 진주역’이며 광양역에서는 광양역과 진상역 진주역에서는 완사역과 진주역의 영상감시설비가 구성되어 있었다. 하동역, 횡천역, 북천역은 무인 또는 1∼2명이 근무하는 역으로 열차 감시 역으로 지정되어 있지 않았다. 광양역과 진주역 로컬관제원은 ‘사고열차에 대해 CCTV 영상설비를 활용하 여 열차감시를 하였고 이상 없었다.’라고 진술하였으나 광양역 근무자는 사 고열차가 진상역 통과 중 차량 하부의 불꽃이 발생된 것을 확인하지 못하였 고 진주역 근무자는 사고열차가 완사역 통과 중 차량 하부의 불꽃이 발생된 것을 발견하지 못한 것으로 확인되었다. 분석 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 22 - 그리고 사고열차의 기관차는 신형기관차로서 일반기관차의 구조와 달리 박 스 형태이며 후부감시용 후사경은 설치되어 있지 않았고 측면 창문은 작게 설치되어 있어 운전실 내부에 운전자용 MMI 화면을 CCTV 화면으로 변경 하여야 열차 후부를 감시할 수 있었다. 사고열차의 기관사는‘정거장 간 후부감시를 기관차 내부 MMI 화면(CCTV) 을 통해 확인했으나 야간 및 우천으로 화질 상태가 불량하여 후부 감시가 어려웠습니다.’라고 진술하였다. 지역본부 지도운영팀장은 본사 운전제도 문답방6)을 활용하여‘신형기관차 (7600호대, 8500호대) 열차 후부감시 방법’을 문의(2015. 9. 28) 한 것 으로 보 아 사고열차와 같은 신형기관차는 기관사가 운전실에서 열차의 후부를 감시 하기에는 적절하지 않은 것으로 조사되었다. 2.2 차축 베어링 분해 검사 차축베어링이 차축 절손에 영향을 주었는지를 알아보기 위하여 탈선화차 에 설치된 총 8개의 차축 베어링 중 절손된 세 번째 왼쪽 차축(3L)에 설치 되었던 차축 베어링은 [그림14]과 같이 심하게 파손되어 사고원인을 밝힐 만 한 단서를 찾을 수 없었다. 그러나 세 번째 왼쪽 차축 베어링과 거의 유사한 하중이력을 가졌다고 판단되는 세 번째 오른쪽 차축 베어링(3R), 동일한 뒷 대차에 있던 차축 베어링(4L, 4R) 및 동일 차량의 앞 대차에 설치되어 있던 차축 베어링(1L, 1R, 2L, 2R)을 검사하여 간접적으로 사고의 원인을 추정하 였다. 6) 문답방 : 한국철도공사 그룹포털(전자문서)에 운전제도에 관한 질의 응답방 베어링 잔해 분석 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 23 - [그림14] 사고현장에서 수거한 베어링 잔해 및 엔드캡 검사 결과 [그림15]와 같이 대부분의 차축 베어링 외륜과 롤러에서 일직선 불량 및 충격흠 등 결함이 발견되었다. 차축 베어링이 파손되고(3L) 탈선한 뒷 대차의 나머지 세 개 차축 베어링(3R, 4L, 4R)에서 발견한 결함은 탈선 충격의 영향을 받았을 수 있지만, 탈선하지 않은 앞 대차의 네 개 차축 베어링 (1L, 1R, 2L, 2R)에서도 결함이 발견된 것으로 보아 탈선화차의 차축 베어링 은 사고 이전부터 결함이 있는 상태로 운행하다가 제 기능을 하지 못하여 차축이 절단되어 탈선된 것으로 판단된다. 차축 베어링 외륜과 롤러에서 발 견된 일직선 불량이나 충격흠 등 결함은 과적 또는 편적한 상태로 운행할 때, 열차 편성 시 충격 또는 고정핀을 풀지 않고 크레인으로 화차에서 컨테이너 를 들어 올렸다 화차가 떨어질 때 발생할 가능성이 있는 것으로 확인되었다. 1L : 외륜의 일직선 불량, 필링 1L : 롤러의 일직선 불량, 필링 1R : 외륜의 일직선 불량, 충격흠 1R : 롤러의 일직선 불량 롤러 엔드캡 분석 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 24 - [그림15] 차축 베어링 표면 검사 결과 (3R과 4L의 외륜은 탈선 충격으로 파손된 것으로 추정됨) 2L : 외륜의 일직선 불량 2L : 롤러의 일직선 불량 2R : 외륜의 일직선 불량 2R : 롤러의 일직선 불량 3R : 외륜의 일직선 불량 및 충격흠 3R : 롤러의 일직선 불량 및 파손 4L : 외륜의 일직선 불량 4L : 롤러의 일직선 불량 4R : 외륜의 일직선 불량 및 충격흠 4R : 롤러의 일직선 불량 분석 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 25 - 또한 화차가 특성이 각각 다른 선로를 운행하거나 작업하는 것을 고려하 지 않고 한국철도공사 객화차유지보수기준(내규 35호) 에서 차축 베어링의 분해검사 주기는 주행거리 80만km 또는 회기한도 8년으로, 교환 주기는 주 행키로 320만km로 동일하게 정하여 검사, 교환하는 것은 차축 베어링 결함 으로 발생할 수 있는 탈선사고를 예방하기 적절하지 못하므로 운행환경· 작 업환경에 따라 그 주기에 차이를 두어 관리할 수 있도록 객화차유지보수기 준 을 개정할 필요가 있을 것으로 판단된다. 2.3 컨테이너 상하차 작업 화차에 컨테이너를 싣고 내리는 작업을 조사한 결과 컨테이너 고정핀을 완 전히 풀지 않고 컨테이너를 들어 올리거나 고정핀을 풀었다 하더라도 컨테 이너가 수직 방향으로 들어 올려지지 않으면 컨테이너가 고정핀에서 원활하 게 빠지지 못하고 화차가 함께 들어 올려지다 떨어지는 경우가 발생할 수 있다. 이러한 경우에 차축 베어링 외륜의 내면과 롤러 사이에 충격이 발생하 여 차축 베어링 외륜의 내면과 롤러에 일직선불량, 충격흠 등의 결함이 생기 는 것으로 조사되었다.([그림16]) 컨테이너 상하차 작업 분석 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 26 - 잠긴 상태 풀린 상태 [그림16] 컨테이너 상하차 작업과 고정핀 2.4 그리스 성분 사고 당시에 파손된 차축 베어링 이외의 차량에 남아 있는 차축 베어링에서 그리스를 채취하여 분석함으로써 탈선화차의 차축에 베어링을 설치할 당시 또는 운행 중에 이물질이 유입되었는지 여부를 간접적으로 확인하였다.([그림17]) [그림17] 탈선화차의 차축 베어링에서 채취한 그리스 고정핀 분석 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 27 - 신품 그리스와 파손된 차축 베어링에서 채취한 7종(1L, 1R, 2L, 2R, 3R, 4L, 4R) 그리스 시료의 구조분석을 위해 적외선 분광기를 사용하였다. 구조 분석의 결과 [그림18]과 같이 적외선 분광기 스펙트럼의 주요 피크가 일치하 기 때문에 신품과 차축 베어링에서 채취한 그리스는 동일한 제품(SCH-100, Mobilith 社)일 것으로 분석되었다. [그림18] 적외선 분광기 구조분석 결과 차축 베어링 마모에 의해 그리스에 철(Fe)성분이 섞여 있는지를 확인하기 위해 유도결합 플라즈마 분광기를 사용하였다. 신품 그리스와 차축 베어링에 서 채취한 그리스의 성분을 분석하여 비교하였고, 그 결과는 [표9]과 같다. 유도결합 플라즈마 분광기의 성분분석 결과, 신품에서는 철 성분이 검출되 지 않았으나 나머지 7종에서는 모두 철 성분이 검출되었다. 이는 탈선 충격 이나 차축 베어링에 결함이 있는 상태에서 차량이 주행하여 차축 베어링 구 성 부품들이 마모되어 그리스에 철 성분이 섞인 것으로 분석되었다. 분석 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 28 - [표9] 그리스 성분 분석 결과 (단위: wt%) 그리스에 함유된 수분을 분석하기 위해 수분함량 측정기를 사용하였다. 신 품 그리스와 차축 베어링에서 채취한 그리스의 수분을 분석하여 비교하였고, 그 결과는 [표10]과 같이 신품 및 차축 베어링에서 채취한 그리스에서 수분 이 0.1% 이내로 검출되어 기준(신간선 0.2%)을 만족하는 것으로 분석되었다. [표10] 수분 함량 측정 결과 (단위: wt%) 분석 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 29 - 2.5 종합 분석 탈선화차는 컨테이너를 운반하는 화차로서 차축 베어링 분해검사 결과 차 축 베어링 외륜의 내면과 롤러에서 일직선 불량과 충격흠 등 결함이 발견되 었으며, 차축 베어링의 분해검사 주기도 운행 및 작업환경 등을 고려하지 않 고 동일(주행거리 80만km, 회기 한도 8년)하게 정해져 있었다. 또한, 화차에 컨테이너를 싣고 내리는 작업을 조사한 결과 화차에서 컨테 이너 고정핀을 완전히 풀지 않고 컨테이너를 들어 올리거나 고정핀을 풀었 다 하더라도 컨테이너가 수직 방향으로 들어 올려지지 않으면 컨테이너가 고정핀에서 원활하게 빠지지 못하고 화차가 함께 들어 올려지다 떨어지는 경우가 발생할 수 있다. 이러한 경우에 차축 베어링 외륜의 내면과 롤러 사 이에 충격이 발생하여 결함이 생기는 것으로 판단된다. 탈선화차는 차축 베어링에 결함이 있는 상태로 주행하다가 진상역 진입하 기 이전부터 차축 베어링 발열에 의한 불꽃이 발생되었으나 로컬관제원은 열차감시를 통해 이를 발견하지 못하였다. 탈선화차는 사고 현장 근처에서 외륜이 파손되었고, 롤러가 차축 베어링 내의 정상 위치로부터 이탈되었으며, 이로 인해 차축 베어링이 과열되며 차 축이 절손되어 완사역과 진주역 사이에서 진행방향 좌측으로 탈선하였다. 결론 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 30 - 3. 결론 3.1 조사 결과 3.1.1 사고열차는 2인 승무열차로 기관사는 운전취급규정 제37조(열차의 감 시) 제1항에 따라 기관차 내부 MMI 화면(CCTV)을 활용하여 열차감시 를 시행하였으나 야간 및 우천으로 인하여 확인에 어려움이 있었다. 3.1.2 사고열차의 기관차는 신형기관차로 일반기관차 구조와 달리 박스 형태 이고, 후부감시용 후사경이 설치되어 있지 않았으며, 측면 창문은 작게 설치되어 있었고, 운전실 내부에 설치된 운전자용 후부감시용 CCTV 화면은 MMI를 CCTV 모드로 변경해야 열차 후부를 감시할 수 있는 어려움이 있으므로 기관사가 차축 베어링 발열 등 열차 후부의 이상을 용이하게 감시하기 위한 개선이 필요할 것으로 조사되었다. 3.1.3 사고열차 무전기 녹취록을 확인한 결과 기관사와 로컬관제원 간 통화 내용 중에서‘차량상태의 이상 유무’를 확인해 주었거나 탈선화차 ‘차축 베 어링의 발열(불꽃 등) 상태를 발견하거나 확인’한 내용은 없었다. 3.1.4 역사 및 기관차 CCTV 녹화영상을 확인한 결과 사고열차는 광양역과 진상역 운행 중 차량 하부에서 최초로 불꽃이 발생되는 것을 확인하였고, 이후 하동역 북천역 완사역 통과 중 불꽃이 확인되어 탈선화차의 차축 베어링 발열에 의한 불꽃은 진상역 진입전(삼랑진역 기점 136.300km 지점)부터 발생된 것으로 조사되었다. 3.1.5 광양역과 진주역 로컬관제원은 운전취급규정 제37조의2에 따른 열차 감시를 시행하였으나 차축 불꽃을 발견하지 못하여 탈선사고를 예방하 지 못한 것으로 분석되었다. 3.1.6 탈선화차 뒷 대차의 엔드캡에 차축 베어링 과열 확인을 위해 붙여 놓 은 70도 온도 테이프는 탈락되었거나 변색이 없어 개선이 필요할 것으 결론 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 31 - 로 조사되었다. 3.1.7 한국철도공사 객화차유지보수기준(내규 35호) 에서 차축 베어링의 분해검사 주기는 주행거리 80만km 또는 회기한도 8년으로, 교환주기는 주행키로 320만km로 동일하게 정하여 검사, 교환하는 것은 차축 베어 링 결함으로 발생할 수 있는 탈선사고를 예방하기에는 적절하지 못하 므로 운행환경·작업환경에 따라 차이를 둘 수 있도록 객화차유지보수 기준 을 개정할 필요가 있을 것으로 판단된다. 3.1.8 탈선화차에 설치되어 있던 차축 베어링 분해검사 결과 대부분의 차축 베어링 외륜과 롤러에서 일직선 불량 및 충격흠 등의 결함이 발견되었 다. 차축 베어링이 파손되고 탈선한 뒷 대차의 나머지 세 개 차축 베어 링에서 발견한 결함은 탈선 충격의 영향을 받았을 수 있지만, 탈선하지 않은 앞 대차의 네 개 차축 베어링에서도 결함이 발견된 것으로 보아 탈선화차의 차축 베어링은 사고 이전부터 결함이 있는 상태로 운행한 것이 탈선의 원인이 된 것으로 판단된다. 3.1.9 차축 베어링 외륜의 내면과 롤러에서 발견된 일직선 불량이나 충격흠 등 결함은 과적 또는 편적한 상태로 운행하거나, 화차에 컨테이너를 싣고 내리는 작업을 할 때 컨테이너 고정핀을 완전히 풀지 않고 컨테 이너를 들어 올리거나 고정핀을 풀었다 하더라도 컨테이너가 수직 방 향으로 들어 올려지지 않으면 컨테이너가 고정핀에서 원활하게 빠지지 못하고 화차가 함께 들어 올려지다 떨어지는 경우가 발생할 수 있다. 이러한 경우에 차축 베어링 외륜의 내면과 롤러 사이에 충격이 발생되 면서 차축 베어링 외륜의 내면과 롤러에 결함이 생길 수 있을 것으로 조사되었다. 3.1.10 적외선 분광기 이용하여 그리스 구조 분석을 수행한 결과 신품과 탈 선화차 차축 베어링에서 채취한 그리스는 동일 제품일 것으로 분석되 었다. 결론 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 32 - 3.1.11 유도결합 플라즈마 분광기의 성분분석 결과, 신품에서는 철 성분이 검 출되지 않았으나 나머지 7종에서는 모두 철 성분이 검출되었다. 이는 탈선 충격이나 차축 베어링에 결함이 있는 상태에서 차량이 주행하여 차축 베어링 구성 부품들이 마모되어 그리스에 철 성분이 섞인 것으로 분석되었다. 3.1.12 그리스 수분함량 측정 결과 신품 및 차축 베어링에서 채취한 그리스 에서는 수분이 0.1% 이내로 검출되어 기준을 만족하는 것으로 분석되 었다. 3.1.13 사고구간의 선로상태 확인을 위한 궤도검측차 검측 결과는 양호하였 으며, 도보순회점검도 주기적으로 시행하였고 특이사항은 없었다. 3.1.14 사고열차는 완사역을 통과하여 진주역 쪽으로 운행하면서 사고지점에 정차할 때까지 신호와 진로는 모두 양호한 상태였다. 3.2 사고 원인 항공·철도사고조사위원회는 이번 사고의 주원인을 ‘차축에 설치되어 있는 차축 베어링 외륜의 내면과 롤러에 결함이 있는 상태에서 운행 도중 차축 베어링이 파손되면서 차축이 절손되어 탈선한 것’으로 결정하였다. 기여요인으로는 ‘화차에 컨테이너를 싣고 내릴 때 차축 베어링에 충격이 가해진 것, 차축 베어링의 분해검수 주기와 교환 주기가 화차에 적재되는 화 물의 종류, 적재방법 및 사용환경의 차이에도 불구하고 동일하게 규정하여 관리한 것’으로 결정하였다. 안전 권고 한국철도공사 경전선 완사역∼진주역간 화물열차 탈선사고 조사보고서 - 33 - 4. 안전 권고 항공․철도사고조사위원회는「항공․철도사고조사에 관한 법률」제26조에 따라 2019년 2월 18일 경전선 완사역∼진주역 사이에서 발생한 화물열차 탈 선사고에 대하여 다음과 같이 권고한다. 4.1 한국철도공사에 대하여 4.1.1 화차 차축 베어링의 분해검수 주기와 교환 주기를 화차에 적재되는 화 물의 종류, 적재방법 및 사용환경에 따라 차이를 두어 유지보수에 적용 할 수 있도록 연구용역 등을 통하여 차축 베어링의 분해검수 주기와 교환 주기를 단축할 수 있도록 객화차 유지보수 기준 을 개정할 것 4.1.2 컨테이너를 화차에서 들어 올릴 때 화차가 함께 들어 올려지다 떨어지지 않도록 작업 절차를 개선하고 이를 감시할 수 있는 시스템을 갖출 것 4.1.3 화차의 발열 여부를 발견하기 위해 차축에 부착한 온도 테이프는 효과가 없는 것으로 밝혀진 바, 이를 대체할 방법을 수립하고 시행할 것 4.1.4 역근무자가 열차 감시 업무를 적정하게 수행할 수 있도록 열차감시용 영상감시설비의 확대 설치 등 방안을 마련하여 시행할 것 4.1.5 신형기관차로 열차를 편성하여 운행시 열차 후부를 쉽게 감시할 수 있 도록 개선할 것 - 34 - 　　이 보고서는 사고조사 과정에서 관계인들로부터 청취한 진 술 및 개인정보 등이 포함되어 있어, 　『항공․철도사고조사에 관한 법률』제28조(정보의 공개금지) 및 같은 법 시행령 제8조(공개를 금지할 수 있는 정보의 범위)에 의 하여 이 보고서(인쇄본)에 개인정보는 공개하지 않았으며, 국민 여러분의 이해를 돕기 위해 전문 철도용어를 쉽게 풀어서 쓴 점을 양해하여 주시기 바랍니다. 자세한 사항은 항공․철도사고조사위원회로 문의하여 주시기 바랍니다. 항공․철도사고조사위원회 http://www.araib.go.kr 전화 : 044-201-5443 E-mail: mrlee56@korea.kr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sep_token=\"[SEP]\"\n",
        "pad_token=\"[PAD]\"\n",
        "cls_token=\"[CLS]\"\n",
        "mask_token=\"[MASK]\"\n",
        "max_len = 60\n",
        "mask_prob = 0.15\n",
        "\n",
        "\n",
        "test_instance = MLMTrainDataset_Sik(max_len, mask_prob, mask_token, sep_token, cls_token, check, BertTokenizer, pad_token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "id": "pHn9s_CHuIn5",
        "outputId": "4c355576-62f6-4dd6-f272-32671ef4da66"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-cd2164440b4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtest_instance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLMTrainDataset_Sik\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-f4d39c4c4e18>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, max_len, mask_prob, mask_token, sep_token, cls_token, all_sents, tokenizer, pad_token)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask_prob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcls_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msep_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msep_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: tokenize() missing 1 required positional argument: 'text'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BertTokenizer.tokenize()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "id": "KHryjrnwWO_H",
        "outputId": "0e6a384c-9a44-42e3-f910-5ec866899f23"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-307823f7a0b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: tokenize() missing 2 required positional arguments: 'self' and 'text'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchcrf import CRF\n",
        "\n",
        "# 개체명 인식 모델 Class 코드\n",
        "class Bert_entity(nn.Module):\n",
        "  def __init__(self, model, num_labels):\n",
        "    super().__init__()\n",
        "    self.model = model\n",
        "    self.hidden_size = 768\n",
        "    self.block_size = 64\n",
        "    self.hidden_layer = nn.Linear(768, 768)\n",
        "    self.classifier = nn.Linear(768, num_labels)\n",
        "    self.dropout = nn.Dropout(0.4)\n",
        "    self.num_labels = num_labels\n",
        "    self.crf = CRF(num_tags = num_labels, batch_first=True)\n",
        "  \n",
        "  def forward(self,\n",
        "              input_ids = None,\n",
        "              attention_mask = None,\n",
        "              labels = None,\n",
        "              valid_ids = None,\n",
        "              label_mask = None):\n",
        "    \n",
        "    output = self.model(input_ids = input_ids, attention_mask = attention_mask)\n",
        "    last_hidden_state = output.last_hidden_state\n",
        "    valid_output = torch.zeros(last_hidden_state.size()[0], last_hidden_state.size()[1],\n",
        "                               last_hidden_state.size()[2], dtype=torch.float32, device='cuda:1')\n",
        "    valid_masks = torch.zeros(last_hidden_state.size()[0], last_hidden_state.size()[1],\n",
        "                              dtype=torch.bool, device='cuda:1')\n",
        "    \n",
        "    for i in range(last_hidden_state.size()[0]):\n",
        "      jj = -1\n",
        "      for j in range(last_hidden_state.size()[1]):\n",
        "        if valid_ids[i][j].item() == 1:\n",
        "          jj += 1\n",
        "          valid_masks[i][jj] = 1\n",
        "          valid_output[i][jj] = last_hidden_state[i][j]\n",
        "      \n",
        "      relations = self.dropout(valid_output)\n",
        "      logits = self.classifier(relations)\n",
        "      loss = None\n",
        "      if labels is not None:\n",
        "        if attention_mask is not None:\n",
        "          loss, logits = self.crf(logits, labels, mask = valid_masks, reduction = 'token_mean'), self.crf.decode(logits, mask=valid_masks)\n",
        "        else:\n",
        "          loss, logits = self.crf(logits, labels, mask = valid_masks, reduction = 'token_mean'), self.crf.decode(logits, mask=valid_masks)\n",
        "      return -loss, logits, last_hidden_state, valid_output, output.attentions\n"
      ],
      "metadata": {
        "id": "fJpEt0Mb2kJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 개체간 관계 추출 모델 Class 코드\n",
        "class Bert_relation(nn.Module):\n",
        "  def __init__(self, model, num_labels):\n",
        "    super().__init__()\n",
        "    self.model = model\n",
        "    self.hidden_size =768\n",
        "    self.block_size = 64\n",
        "\n",
        "    self.head_extractor = nn.Linear(768, 768)\n",
        "    self.tail_extractor = nn.Linear(768, 768)\n",
        "\n",
        "    self.bilinear_classifier = nn.Linear(768 * 3, num_labels)\n",
        "\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "    self.num_labels = num_labels\n",
        "  \n",
        "  def get_hrts(self, valid_output, left_ids, right_ids):\n",
        "    left_masks = left_ids.unsqueeze(-1).expand(valid_output.size()).float()\n",
        "    right_masks = right_ids.unsqueeze(-1).expand(valid_output.size()).float()\n",
        "\n",
        "    left_rep = torch.logsumexp(valid_output * left_masks, dim=1)\n",
        "    right_rep = torch.logsumexp(valid_output * right_masks, dim=1)\n",
        "\n",
        "    return left_rep, right_rep\n",
        "  \n",
        "  def forward(self,\n",
        "              input_ids=None,\n",
        "              attention_mask=None,\n",
        "              labels=None,\n",
        "              valid_ids=None,\n",
        "              label_mask=None,\n",
        "              left_ids=None,\n",
        "              right_ids=None):\n",
        "    output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    last_hidden_state = output.last_hidden_state\n",
        "    pooled_output = output.pooler_output\n",
        "    context = self.dropout(pooled_output)\n",
        "    valid_output = last_hidden_state\n",
        "\n",
        "    left_rep, right_rep = self.get_hrts(valid_output, left_ids, right_ids)\n",
        "\n",
        "    left_rep = self.head_extractor(left_rep)\n",
        "    right_rep = self.head_extractor(right_rep)\n",
        "\n",
        "    bl = torch.cat([context, left_rep, right_rep], dim=-1)\n",
        "\n",
        "    logits = self.bilinear_classifier(bl)\n",
        "    loss = None\n",
        "\n",
        "    if labels is None:\n",
        "      loss_fct = torch.nn.BCEWithLogitsLoss()\n",
        "      loss = loss_fct(logits, labels.float())\n",
        "\n",
        "    output = logits\n",
        "\n",
        "    return loss, output if loss is not None else output"
      ],
      "metadata": {
        "id": "d1RycRmj8Qfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_most_similar_words(word, total_words):\n",
        "  q = []\n",
        "\n",
        "  for a in total_words:\n",
        "    k = jaccard_similarity(word, a[0])\n",
        "    q.append((a, k))\n",
        "  q.sort(key=lambda x:x[1])\n",
        "  qq = q[-20:]\n",
        "  new_q = []\n",
        "  for qa in qq:\n",
        "    if qa[1] > 0.05:\n",
        "      new_q.append(qa)\n",
        "  \n",
        "  return new_q"
      ],
      "metadata": {
        "id": "hdVMwKQhJgFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def jaccard_similarity(word1, word2):\n",
        "  word1 = word1.lower()\n",
        "  word2 = word2.lower()\n",
        "  trigram1 = trigram_from_word(word1)\n",
        "  trigram2 = trigram_from_word(word2)\n",
        "\n",
        "  set_trigram = set(trigram1)\n",
        "  set_trigram = set_trigram.union(set(trigram2))\n",
        "  con_trigram = [t in trigram2 for t in trigram1]\n",
        "  qq = sum(con_trigram)\n",
        "  la = 0.0\n",
        "  la = float(float(qq)/float(len(set_trigram)+1e-9))\n",
        "\n",
        "  return la"
      ],
      "metadata": {
        "id": "SfeWOBqwQkq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trigram_from_word(word):\n",
        "  if len(word) < 3:\n",
        "    return [word]\n",
        "  \n",
        "  else:\n",
        "    trigram = [word[i:i+3] for i in range(len(word)-3)]\n",
        "    return trigram"
      ],
      "metadata": {
        "id": "4Hxea9nARNYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sample : PDF to TXT"
      ],
      "metadata": {
        "id": "Js1n6xKKlfxM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "!pip install pdfminer.six"
      ],
      "metadata": {
        "id": "A8oWUsXKlkHd",
        "cellView": "code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "import pdfminer\n",
        "from pdfminer.high_level import extract_text\n",
        "from google.colab import files\n",
        "\n",
        "pdf = files.upload()"
      ],
      "metadata": {
        "id": "ldJUFR7npkkC",
        "cellView": "code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "print(pdf)"
      ],
      "metadata": {
        "id": "HwL0cPhrpsOh",
        "cellView": "code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "file_path = \"sample.pdf\"\n",
        "\n",
        "text = extract_text(pdf_file=file_path)\n"
      ],
      "metadata": {
        "id": "PNxzYfKbp0GA",
        "cellView": "code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "from io import StringIO\n",
        "\n",
        "from pdfminer.converter import TextConverter\n",
        "from pdfminer.layout import LAParams\n",
        "from pdfminer.pdfdocument import PDFDocument\n",
        "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
        "from pdfminer.pdfpage import PDFPage\n",
        "from pdfminer.pdfparser import PDFParser\n",
        "\n",
        "output_string = StringIO()\n",
        "with open(file_path, 'rb') as in_file:\n",
        "    parser = PDFParser(in_file)\n",
        "    doc = PDFDocument(parser)\n",
        "    rsrcmgr = PDFResourceManager()\n",
        "    device = TextConverter(rsrcmgr, output_string, laparams=LAParams())\n",
        "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
        "    for page in PDFPage.create_pages(doc):\n",
        "        interpreter.process_page(page)\n",
        "\n",
        "print(output_string.getvalue())"
      ],
      "metadata": {
        "id": "AKYgFGUVq4R-",
        "cellView": "code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tokenizing"
      ],
      "metadata": {
        "id": "Ppm736thjDGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "str = output_string.getvalue()\n",
        "new_str = re.sub(\"\\n\", \"\", str)\n",
        "new_str = re.sub(\"·\", \"\", new_str)\n",
        "\n",
        "print(new_str)"
      ],
      "metadata": {
        "id": "YSdmiVZD5kKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "BlCkotjt-g6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "word_tokens = word_tokenize(new_str)\n",
        "sent_tokens = sent_tokenize(new_str)\n",
        "\n",
        "print(sent_tokens)"
      ],
      "metadata": {
        "id": "Ea49FUcJ-NEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy"
      ],
      "metadata": {
        "id": "e8H2nVuq5w1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install collections"
      ],
      "metadata": {
        "id": "RVFpSvysAN4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt\n",
        "from collections import Counter\n",
        "okt = Okt()\n",
        "noun = okt.nouns(text)\n",
        "for i, v in enumerate(noun):\n",
        "  if len(v) < 2:\n",
        "    noun.pop(i)\n",
        "\n",
        "count = Counter(noun)"
      ],
      "metadata": {
        "id": "Uik8A28t87PB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 빈도 카운트\n",
        "noun_list = count.most_common(100)\n",
        "for v in noun_list:\n",
        "  print(v)"
      ],
      "metadata": {
        "id": "wduRrIYk9lKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#BERT Training"
      ],
      "metadata": {
        "id": "CvtIIJici-n-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "model = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')"
      ],
      "metadata": {
        "id": "1LfjYJn-DnMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = bert_tokenizer(text, return_tensors='pt')\n",
        "print(inputs)"
      ],
      "metadata": {
        "id": "bus7ncLBD98B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs['labels'] = inputs.input_ids.detach().clone()"
      ],
      "metadata": {
        "id": "RVnFUnA9GtTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rand = torch.rand(inputs.input_ids.shape)\n",
        "\n",
        "mask_arr = (rand < 0.15) * (inputs.input_ids != 101) * (inputs.input_ids != 102) # 101, 102번 토큰 제외하고 15% 위치 선별\n",
        "\n",
        "### padding이 포함된 경우(0번도 추가 제외) ###\n",
        "# mask_arr = (rand < 0.15) * (inputs.input_ids != 101) * (inputs.input_ids != 102) * (inputs.input_ids != 0)\n",
        "\n",
        "selection = torch.flatten((mask_arr[0]).nonzero())"
      ],
      "metadata": {
        "id": "ogURvy6ZG8HZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "selection_val = np.random.random(len(selection)) # selection의 위치마다 0~1 값 부여\n",
        "\n",
        "mask_selection = selection[np.where(selection_val >= 0.2)[0]] # 80% : Mask 토큰 대체\n",
        "random_selection = selection[np.where(selection_val < 0.1)[0]] # 10% : 랜덤 토큰 대체\n",
        "\n",
        "print(random_selection) # tensor([ 30,  95, 143])\n",
        "print(mask_selection)"
      ],
      "metadata": {
        "id": "rCvxrEcVG-gf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs.input_ids[0, mask_selection] = 103\n",
        "inputs.input_ids[0, random_selection] = torch.randint(0, 30522, size = random_selection.shape)\n",
        "\n",
        "print(inputs)"
      ],
      "metadata": {
        "id": "nXDL2jAjHOPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model(**inputs)"
      ],
      "metadata": {
        "id": "EArKkfn3HQP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")"
      ],
      "metadata": {
        "id": "1FTgrbxVHR2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = tokenizer.tokenize(new_str)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "rvHyzuzzyrxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CS3N5T_nzyZb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}