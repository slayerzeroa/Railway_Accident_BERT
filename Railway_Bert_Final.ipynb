{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "gKbTGIGeUWSq",
        "zapSz8M995hT"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a3d86b6e709a40ebbce170ed200679e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eff3624d36984219bf0e59184914dd4a",
              "IPY_MODEL_07707d3196a44c7d85b80f1c58ea7d95",
              "IPY_MODEL_f4c8687ab3484a25826d1af127a62316"
            ],
            "layout": "IPY_MODEL_313a1d36c9b7409c8c6ea34d4701930e"
          }
        },
        "eff3624d36984219bf0e59184914dd4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8c0afac9081461593e75af50ea35c5f",
            "placeholder": "​",
            "style": "IPY_MODEL_196aba78853b4109b8428ccf40c881b1",
            "value": "Downloading (…)&quot;pytorch_model.bin&quot;;: 100%"
          }
        },
        "07707d3196a44c7d85b80f1c58ea7d95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2fef0799d88044fb9dd4640f64e08096",
            "max": 714314041,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4aed4f1702474566a9db0a616a71cad7",
            "value": 714314041
          }
        },
        "f4c8687ab3484a25826d1af127a62316": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00b4c76bb5fc4429b80100900a37395c",
            "placeholder": "​",
            "style": "IPY_MODEL_095f0ed0f23b4c60938f1dd77f4a1ac3",
            "value": " 714M/714M [00:05&lt;00:00, 129MB/s]"
          }
        },
        "313a1d36c9b7409c8c6ea34d4701930e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8c0afac9081461593e75af50ea35c5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "196aba78853b4109b8428ccf40c881b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2fef0799d88044fb9dd4640f64e08096": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4aed4f1702474566a9db0a616a71cad7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "00b4c76bb5fc4429b80100900a37395c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "095f0ed0f23b4c60938f1dd77f4a1ac3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f66ac09c8cfb4708af10917f7fdacb15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_493af20055594f1e8e9e40a6a078b813",
              "IPY_MODEL_c155e9aeca8b4746a555b70ace98fc94",
              "IPY_MODEL_b55a28e8a7854976891a6f7d2f3e8036"
            ],
            "layout": "IPY_MODEL_114333d4d17f4d029d401eeb8fe37996"
          }
        },
        "493af20055594f1e8e9e40a6a078b813": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94dc662b53b94253acab388e9f76ce7e",
            "placeholder": "​",
            "style": "IPY_MODEL_d09cb8a6a4104764812d30e0410c2527",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "c155e9aeca8b4746a555b70ace98fc94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c6a1830374c41c5b8541343a84822bc",
            "max": 29,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4a08944894d34e0bac947a11a38f93bf",
            "value": 29
          }
        },
        "b55a28e8a7854976891a6f7d2f3e8036": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08485424b7bf4fcdb919cedacb858ef5",
            "placeholder": "​",
            "style": "IPY_MODEL_0107b9aabf0940c0acf7cbfd1afb3f36",
            "value": " 29.0/29.0 [00:00&lt;00:00, 1.17kB/s]"
          }
        },
        "114333d4d17f4d029d401eeb8fe37996": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94dc662b53b94253acab388e9f76ce7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d09cb8a6a4104764812d30e0410c2527": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7c6a1830374c41c5b8541343a84822bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a08944894d34e0bac947a11a38f93bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "08485424b7bf4fcdb919cedacb858ef5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0107b9aabf0940c0acf7cbfd1afb3f36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6ec1e88922ab460786b182178d02e93d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fb794452ee4b44b1a1d7088f003903d1",
              "IPY_MODEL_5ebbccb4828a4657a6456fc8d77cd8aa",
              "IPY_MODEL_17d4c9cc32a740f4857c9ddd84010593"
            ],
            "layout": "IPY_MODEL_a9d1665d99374c0bb7e9420a0c128c0a"
          }
        },
        "fb794452ee4b44b1a1d7088f003903d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa986e675b9b4fac9fac9e9efb4c604b",
            "placeholder": "​",
            "style": "IPY_MODEL_10fa3ebc13204e138d0e51e34195875e",
            "value": "Downloading (…)solve/main/vocab.txt: 100%"
          }
        },
        "5ebbccb4828a4657a6456fc8d77cd8aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6b6a87bf1e44a158cf9db3e2caf6bea",
            "max": 995526,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cf32440576f34183b299a281a5b4cc8a",
            "value": 995526
          }
        },
        "17d4c9cc32a740f4857c9ddd84010593": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bfc98c5914c743d880e7e74122e0e6d0",
            "placeholder": "​",
            "style": "IPY_MODEL_3b9c35627b8641a0a9877fbc7c499bfc",
            "value": " 996k/996k [00:00&lt;00:00, 2.67MB/s]"
          }
        },
        "a9d1665d99374c0bb7e9420a0c128c0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa986e675b9b4fac9fac9e9efb4c604b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10fa3ebc13204e138d0e51e34195875e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a6b6a87bf1e44a158cf9db3e2caf6bea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf32440576f34183b299a281a5b4cc8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bfc98c5914c743d880e7e74122e0e6d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b9c35627b8641a0a9877fbc7c499bfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8126bbffac2a4e4e98d0202871125e38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_41f6b314642c4c8188276463e7474741",
              "IPY_MODEL_467c3e218bae410cb47f0998d508a056",
              "IPY_MODEL_87c022fdd14c4b7292e6cde128830c9a"
            ],
            "layout": "IPY_MODEL_eafd8dcb5b024536a5dd62643a0d88c0"
          }
        },
        "41f6b314642c4c8188276463e7474741": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d29f73aaf06440cb283b313ccc841da",
            "placeholder": "​",
            "style": "IPY_MODEL_409abab7df504477bce5f0002158e342",
            "value": "Downloading (…)/main/tokenizer.json: 100%"
          }
        },
        "467c3e218bae410cb47f0998d508a056": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0576c6ff56e4ebcb83d512b95d80475",
            "max": 1961828,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_68ed37fe4ce7402283dec27e7640e6da",
            "value": 1961828
          }
        },
        "87c022fdd14c4b7292e6cde128830c9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c7db7f9ddbb479cbc3e9db96dbf716f",
            "placeholder": "​",
            "style": "IPY_MODEL_cff3961012ca48b8897dc8e23921a469",
            "value": " 1.96M/1.96M [00:00&lt;00:00, 4.52MB/s]"
          }
        },
        "eafd8dcb5b024536a5dd62643a0d88c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d29f73aaf06440cb283b313ccc841da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "409abab7df504477bce5f0002158e342": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b0576c6ff56e4ebcb83d512b95d80475": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68ed37fe4ce7402283dec27e7640e6da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0c7db7f9ddbb479cbc3e9db96dbf716f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cff3961012ca48b8897dc8e23921a469": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "23789a534b8e4df59c440d6527930bfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b97f7222e0344d2683add96af38c5d40",
              "IPY_MODEL_afbf3075610640739bf4ed847dd8857e",
              "IPY_MODEL_5b04abc3dec240b09340298d231304e7"
            ],
            "layout": "IPY_MODEL_52efeae017a0419781dbddbd8e327c66"
          }
        },
        "b97f7222e0344d2683add96af38c5d40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a51c893cb5e436990c190676e37f7e4",
            "placeholder": "​",
            "style": "IPY_MODEL_ad0eb1b7832e45b5a6bcd5ab9e7800be",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "afbf3075610640739bf4ed847dd8857e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4ae22ad0075470598b9055873da7275",
            "max": 625,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7bba817da07549ffa7bc966140189604",
            "value": 625
          }
        },
        "5b04abc3dec240b09340298d231304e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7dde091e5c084ff98a1ddca3fc9c7d87",
            "placeholder": "​",
            "style": "IPY_MODEL_bcc3a12fe7af42fcba3a42d2337c3f54",
            "value": " 625/625 [00:00&lt;00:00, 19.5kB/s]"
          }
        },
        "52efeae017a0419781dbddbd8e327c66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a51c893cb5e436990c190676e37f7e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad0eb1b7832e45b5a6bcd5ab9e7800be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f4ae22ad0075470598b9055873da7275": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7bba817da07549ffa7bc966140189604": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7dde091e5c084ff98a1ddca3fc9c7d87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bcc3a12fe7af42fcba3a42d2337c3f54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/slayerzeroa/Railway_Accident_BERT/blob/main/Railway_Bert_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Railway Bert\n",
        "Target Paper: https://www.ntis.go.kr/outcomes/popup/srchTotlRschRpt.do?cmd=get_contents&rstId=REP-2022-01112390782&pageCode=TH_RCNT_RPT_DTL \n",
        "<br/><br/>Model using BERT for\n",
        "- 철도 사고 사례 Name Entity Recognition\n",
        "- 철도 사고 사례 Relation Extraction\n",
        "\n",
        "<br/>*{개발중: 완료 X, 디벨롭 필요: 완성O[보수 필요], 주석없음: 완성 O}*\n",
        "<br/>Railway_Bert\n",
        "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |ㅡ Environment_Setting\n",
        "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |ㅡ Load_Dataset ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ Railway_Data_Preprocessor [분리]\n",
        "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |ㅡ Tokenizer_Tuning // 개발중&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|ㅡ Tagtog_Data_Loader\n",
        "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |ㅡ model &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|ㅡ NER_Preprocessor // 디벨롭 필요\n",
        "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|ㅡ NER-Bert // 디벨롭 필요&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|ㅡ REL_Preprocessor\n",
        "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|ㅡ R-Bert // 디벨롭 필요&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|ㅡ Send_to_Sheet<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|ㅡ MLM"
      ],
      "metadata": {
        "id": "Fh0v6uBLdno5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment Setting\n",
        "+ Installing\n",
        "+ Call Library\n",
        "+ Set Cuda\n",
        "+ Google Api Key"
      ],
      "metadata": {
        "id": "WLSmrz7S99aX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSCK6v6Km9qE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12ab7896-f146-4d4f-8c7d-73dc17a66f0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.26.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m118.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.12.0 tokenizers-0.13.2 transformers-4.26.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk) (2022.6.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch-crf\n",
            "  Downloading pytorch_crf-0.7.2-py3-none-any.whl (9.5 kB)\n",
            "Installing collected packages: pytorch-crf\n",
            "Successfully installed pytorch-crf-0.7.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.4.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.6/465.6 KB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.8/dist-packages (from konlpy) (1.21.6)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from konlpy) (4.9.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from JPype1>=0.7.0->konlpy) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->JPype1>=0.7.0->konlpy) (3.0.9)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.1 konlpy-0.6.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement collections (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for collections\u001b[0m\u001b[31m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gspread in /usr/local/lib/python3.8/dist-packages (3.4.2)\n",
            "Requirement already satisfied: requests>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from gspread) (2.25.1)\n",
            "Requirement already satisfied: google-auth in /usr/local/lib/python3.8/dist-packages (from gspread) (2.16.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.2.1->gspread) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.2.1->gspread) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.2.1->gspread) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.2.1->gspread) (1.24.3)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from google-auth->gspread) (1.15.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth->gspread) (5.2.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth->gspread) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth->gspread) (0.2.8)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth->gspread) (0.4.8)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.8/dist-packages (4.1.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.8/dist-packages (from oauth2client) (0.2.8)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.8/dist-packages (from oauth2client) (0.4.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from oauth2client) (4.9)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from oauth2client) (0.17.4)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.8/dist-packages (from oauth2client) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.8.0+cu111\n",
            "  Downloading https://download.pytorch.org/whl/cu111/torch-1.8.0%2Bcu111-cp38-cp38-linux_x86_64.whl (1982.2 MB)\n",
            "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.0/2.0 GB\u001b[0m \u001b[31m156.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0mtcmalloc: large alloc 1982242816 bytes == 0x25be000 @  0x7f3f5c1f5680 0x7f3f5c216824 0x5b3128 0x5bbc90 0x5f714c 0x64d800 0x527022 0x504866 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x5f5ee6 0x56bbe1 0x569d8a 0x5f60c3 0x56cc92 0x569d8a 0x5f60c3\n",
            "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.0/2.0 GB\u001b[0m \u001b[31m179.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0mtcmalloc: large alloc 2477809664 bytes == 0x78828000 @  0x7f3f5c1f5680 0x7f3f5c215da2 0x5f714c 0x64d800 0x527022 0x504866 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x5f5ee6 0x56bbe1 0x569d8a 0x5f60c3 0x56cc92 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a\n",
            "tcmalloc: large alloc 1982242816 bytes == 0x25be000 @  0x7f3f5c1f5680 0x7f3f5c216824 0x5f97c1 0x5f8ecc 0x504866 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a 0x5f60c3 0x50b32c 0x5f6b7b 0x66731d 0x5f6706 0x571143 0x50b22e 0x570b82 0x569d8a 0x50b3a0 0x570b82 0x569d8a 0x50b3a0 0x56cc92 0x501044 0x56be83 0x501044 0x56be83 0x501044 0x56be83 0x5f5ee6\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 GB\u001b[0m \u001b[31m885.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.9.0+cu111\n",
            "  Downloading https://download.pytorch.org/whl/cu111/torchvision-0.9.0%2Bcu111-cp38-cp38-linux_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==0.8.0\n",
            "  Downloading torchaudio-0.8.0-cp38-cp38-manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.8.0+cu111) (4.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torch==1.8.0+cu111) (1.21.6)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.8/dist-packages (from torchvision==0.9.0+cu111) (7.1.2)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.1+cu116\n",
            "    Uninstalling torch-1.13.1+cu116:\n",
            "      Successfully uninstalled torch-1.13.1+cu116\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.14.1+cu116\n",
            "    Uninstalling torchvision-0.14.1+cu116:\n",
            "      Successfully uninstalled torchvision-0.14.1+cu116\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 0.13.1+cu116\n",
            "    Uninstalling torchaudio-0.13.1+cu116:\n",
            "      Successfully uninstalled torchaudio-0.13.1+cu116\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.14.1 requires torch==1.13.1, but you have torch 1.8.0+cu111 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.8.0+cu111 torchaudio-0.8.0 torchvision-0.9.0+cu111\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install nltk\n",
        "!pip install pytorch-crf\n",
        "!pip install konlpy\n",
        "!pip install collections\n",
        "\n",
        "!pip install gspread\n",
        "!pip install oauth2client\n",
        "!pip install torch==1.8.0+cu111 torchvision==0.9.0+cu111 torchaudio==0.8.0 -f https://download.pytorch.org/whl/torch_stable.html"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 라이브러리\n",
        "import torch\n",
        "from torch import nn, optim, tensor\n",
        "from torch.utils.data import DataLoader, Dataset,TensorDataset, random_split\n",
        "import torch.nn.functional as F\n",
        "from torchcrf import CRF\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error\n",
        "\n",
        "from transformers import BertTokenizer, BertForMaskedLM, AdamW, get_linear_schedule_with_warmup, BertConfig, BertForTokenClassification, BertModel, BertTokenizerFast\n",
        "\n",
        "import json\n",
        "import glob\n",
        "import re\n",
        "import os\n",
        "import ast\n",
        "import html\n",
        "\n",
        "import re\n",
        "import requests\n",
        "\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "import nltk\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from konlpy.tag import Okt\n",
        "from collections import Counter\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "RH5K6Y2c2UD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#punkt 다운로드\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YDLOwQ1UozS",
        "outputId": "78583739-dad0-4da0-a8bd-4935d5768722"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#디바이스 세팅\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "IJwQl0n9F25R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased')"
      ],
      "metadata": {
        "id": "R19nGUlQqlWy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "f66ac09c8cfb4708af10917f7fdacb15",
            "493af20055594f1e8e9e40a6a078b813",
            "c155e9aeca8b4746a555b70ace98fc94",
            "b55a28e8a7854976891a6f7d2f3e8036",
            "114333d4d17f4d029d401eeb8fe37996",
            "94dc662b53b94253acab388e9f76ce7e",
            "d09cb8a6a4104764812d30e0410c2527",
            "7c6a1830374c41c5b8541343a84822bc",
            "4a08944894d34e0bac947a11a38f93bf",
            "08485424b7bf4fcdb919cedacb858ef5",
            "0107b9aabf0940c0acf7cbfd1afb3f36",
            "6ec1e88922ab460786b182178d02e93d",
            "fb794452ee4b44b1a1d7088f003903d1",
            "5ebbccb4828a4657a6456fc8d77cd8aa",
            "17d4c9cc32a740f4857c9ddd84010593",
            "a9d1665d99374c0bb7e9420a0c128c0a",
            "aa986e675b9b4fac9fac9e9efb4c604b",
            "10fa3ebc13204e138d0e51e34195875e",
            "a6b6a87bf1e44a158cf9db3e2caf6bea",
            "cf32440576f34183b299a281a5b4cc8a",
            "bfc98c5914c743d880e7e74122e0e6d0",
            "3b9c35627b8641a0a9877fbc7c499bfc",
            "8126bbffac2a4e4e98d0202871125e38",
            "41f6b314642c4c8188276463e7474741",
            "467c3e218bae410cb47f0998d508a056",
            "87c022fdd14c4b7292e6cde128830c9a",
            "eafd8dcb5b024536a5dd62643a0d88c0",
            "2d29f73aaf06440cb283b313ccc841da",
            "409abab7df504477bce5f0002158e342",
            "b0576c6ff56e4ebcb83d512b95d80475",
            "68ed37fe4ce7402283dec27e7640e6da",
            "0c7db7f9ddbb479cbc3e9db96dbf716f",
            "cff3961012ca48b8897dc8e23921a469",
            "23789a534b8e4df59c440d6527930bfb",
            "b97f7222e0344d2683add96af38c5d40",
            "afbf3075610640739bf4ed847dd8857e",
            "5b04abc3dec240b09340298d231304e7",
            "52efeae017a0419781dbddbd8e327c66",
            "3a51c893cb5e436990c190676e37f7e4",
            "ad0eb1b7832e45b5a6bcd5ab9e7800be",
            "f4ae22ad0075470598b9055873da7275",
            "7bba817da07549ffa7bc966140189604",
            "7dde091e5c084ff98a1ddca3fc9c7d87",
            "bcc3a12fe7af42fcba3a42d2337c3f54"
          ]
        },
        "outputId": "a47d98e1-8722-475b-f197-95de4689698c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f66ac09c8cfb4708af10917f7fdacb15"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6ec1e88922ab460786b182178d02e93d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8126bbffac2a4e4e98d0202871125e38"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "23789a534b8e4df59c440d6527930bfb"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load_Dataset\n",
        "\n",
        "(Annotation Tool) Tagtog: https://www.tagtog.com/\n",
        "\n",
        "(Dataset) Google Spreadsheet: https://docs.google.com/spreadsheets/d/1cpt2ad2pw0N2tC1H3MPtRAaZzFmgCh2If0gHOQKEIIo/edit#gid=0\n"
      ],
      "metadata": {
        "id": "TE1R9WctSCkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GOOGLE API KEY: json\n",
        "key = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 76
        },
        "id": "XEIiZO-Yx9iK",
        "outputId": "20193238-21ea-408d-e896-e016abf69c93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b03c5c57-c545-4e1c-a964-5edd0b4ea4ae\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b03c5c57-c545-4e1c-a964-5edd0b4ea4ae\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving railway-tagtog-d24e6901ddb0.json to railway-tagtog-d24e6901ddb0.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#로그인 위치\n",
        "url = \"https://tagtog.com/-login\"\n",
        "\n",
        "#다운로드 위치\n",
        "file_url = 'https://www.tagtog.com/slayerzeroa/Railway_BERT/-downloads/dataset-as-anndoc' \n",
        "o_file = 'Railway_Annotation.zip'  \n",
        "if os.path.exists(o_file):\n",
        "    os.remove(o_file)\n",
        "\n",
        "#로그인 정보\n",
        "login_info = {\n",
        "    'loginid' : 'slayerzeroa@gmail.com',\n",
        "    'password' : 'gyysxw5rU3NzrYX',\n",
        "}\n",
        "\n",
        "#로그인\n",
        "with requests.Session() as s:\n",
        "    login_req = s.post(url, data=login_info)\n",
        "    r = s.get(file_url)\n",
        "\n",
        "    with open(o_file,\"wb\") as output:\n",
        "        output.write(r.content)"
      ],
      "metadata": {
        "id": "MjgJQ4xYvb_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gspread\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "\n",
        "scope = ['https://spreadsheets.google.com/feeds',\n",
        "         'https://www.googleapis.com/auth/drive']\n",
        "\n",
        "#json key file 위치\n",
        "json_file_name = './railway-tagtog-d24e6901ddb0.json'\n",
        "\n",
        "# json key file을 이용하여 접속\n",
        "credentials = ServiceAccountCredentials.from_json_keyfile_name(json_file_name, scope)\n",
        "gc = gspread.authorize(credentials)\n",
        "\n",
        "#구글 스프레드 시트 주소\n",
        "spreadsheet_url = \"https://docs.google.com/spreadsheets/d/1cpt2ad2pw0N2tC1H3MPtRAaZzFmgCh2If0gHOQKEIIo/edit#gid=0\"\n",
        "\n",
        "# 스프레드시트 문서 가져오기\n",
        "doc = gc.open_by_url(spreadsheet_url)\n",
        "## gc.create(spreadsheet_name) # 스프레드시트 생성\n",
        "\n",
        "\n",
        "# entity dataset\n",
        "#시트 선택하기\n",
        "sheet_name = \"context\"\n",
        "worksheet = doc.worksheet(sheet_name) #해당 시트가 있는 경우 불러오기\n",
        "## 403 error가 뜰 경우, google sheets API를 활성시켜줘야 함\n",
        "\n",
        "# 시트의 모든 데이터 가져오기\n",
        "values = worksheet.get_all_values()\n",
        "\n",
        "# Pandas Dataframe화\n",
        "header, rows = values[0], values[1:]\n",
        "entity_data = pd.DataFrame(rows, columns=header)\n",
        "column_list = ['text', 'label']\n",
        "entity_data.columns = column_list\n",
        "print(entity_data)\n",
        "\n",
        "\n",
        "# rel dataset\n",
        "sheet_name = \"annotation\"\n",
        "worksheet = doc.worksheet(sheet_name)\n",
        "\n",
        "values = worksheet.get_all_values()\n",
        "\n",
        "header, rows = values[0], values[1:]\n",
        "rel_data = pd.DataFrame(rows, columns=header)\n",
        "column_list = ['rel', 'text']\n",
        "rel_data.columns = column_list\n",
        "print(rel_data)"
      ],
      "metadata": {
        "id": "lzDF7PWYSSe9",
        "outputId": "fe161758-d485-43ab-998f-12ba65a14394",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text  \\\n",
            "0  열차의 맨 후부가 53A/B호 분기기를 지날 때 마지막 차량 후부대차 4축 우측 차...   \n",
            "1  기관사가 정지신호를 무시하고 운행하다가 선로전환기를 할출한 후 관제원의 되돌이 운전...   \n",
            "2  차축에 설치되어 있는 차축 베어링 외륜의 내면과 롤러에 결함이 있는 상태에서 운행 ...   \n",
            "3  21A호 선로전환기가 장애이더라도 21B호 선로전환기기 정상이면 신호기는 진행으로 ...   \n",
            "4  출발신호기 진행 상태에서 21B2호 선로전환기를 통과 중 선두 동력차가 탈선하고 이...   \n",
            "5  청량신호소 21B호 선로전환기 첨단부가 서울방향으로 밀착되지 못하고 벌어진 장애 상...   \n",
            "6             5번째 차량 ( 3513호 ) 앞 대차가 선로 우측으로 탈선하였다 .   \n",
            "7  ITX 새마을열차가 10:16경 경부선 하선 밀양역 구내 진입 중 서울기점 383....   \n",
            "\n",
            "                                               label  \n",
            "0  O O O O IMPLIED O O O O O O O O IMPLIED O O O ...  \n",
            "1  O O IMPLIED O O O O O O O O O O O O CAUSE CAUS...  \n",
            "2  O O O O O O O O IMPLIED O O O O O O CAUSE O O ...  \n",
            "3                 O O IMPLIED O CAUSE O O O OBJECT O  \n",
            "4  O O O O O O O CAUSE CAUSE O O ACCIDENT ACCIDEN...  \n",
            "5  O O IMPLIED IMPLIED O O O O IMPLIED IMPLIED O ...  \n",
            "6     O OBJECT O O O O IMPLIED SOLUTION O ACCIDENT O  \n",
            "7  O O O O O O O O O O O O O O O O O O O O O O O ...  \n",
            "                      rel                                               text\n",
            "0    mz4f90xj280(e_6|e_4)  열차의 맨 후부가 53A/B호 <e1> 분기기를 </e1> 지날 때 마지막 차량 후...\n",
            "1    mz4f90xj280(e_6|e_4)  열차의 맨 후부가 53A/B호 분기기를 지날 때 마지막 차량 후부대차 4축 우측 차...\n",
            "2   yz4ctet7cda(e_4|e_22)  열차의 맨 후부가 53A/B호 분기기를 지날 때 마지막 차량 후부대차 4축 우측 차...\n",
            "3   yz4ctet7cda(e_4|e_22)  열차의 맨 후부가 53A/B호 분기기를 지날 때 마지막 차량 후부대차 4축 우측 차...\n",
            "4    mz4f90xj280(e_6|e_4)  기관사가 정지신호를 <e2> 무시하고 </e2> 운행하다가 선로전환기를 할출한 후 ...\n",
            "5    mz4f90xj280(e_6|e_4)  기관사가 정지신호를 <e2> 무시하고 </e2> 운행하다가 선로전환기를 할출한 후 ...\n",
            "6   yz4ctet7cda(e_4|e_22)  차축에 설치되어 있는 차축 베어링 외륜의 내면과 롤러에 <e2> 결함이 </e2> ...\n",
            "7   yz4ctet7cda(e_4|e_22)  차축에 설치되어 있는 차축 베어링 외륜의 내면과 롤러에 결함이 있는 상태에서 운행 ...\n",
            "8    mz4f90xj280(e_6|e_4)  21A호 선로전환기가 <e1> 장애이더라도 </e1> 21B호 <e2> 선로전환기기...\n",
            "9    mz4f90xj280(e_6|e_4)  출발신호기 진행 상태에서 21B2호 선로전환기를 통과 중 <e1> 선두 </e1> ...\n",
            "10  yz4ctet7cda(e_4|e_22)  출발신호기 진행 상태에서 21B2호 선로전환기를 통과 중 선두 <e1> 동력차가 <...\n",
            "11  yz4ctet7cda(e_4|e_22)  출발신호기 진행 상태에서 21B2호 선로전환기를 통과 중 선두 <e2> 동력차가 <...\n",
            "12   mz4f90xj280(e_6|e_4)  청량신호소 21B호 선로전환기 첨단부가 서울방향으로 밀착되지 못하고 벌어진 장애 상...\n",
            "13   mz4f90xj280(e_6|e_4)  청량신호소 21B호 선로전환기 첨단부가 서울방향으로 밀착되지 못하고 벌어진 장애 상...\n",
            "14  yz4ctet7cda(e_4|e_22)  5번째 차량 ( 3513호 ) 앞 대차가 <e2> 선로 </e2> 우측으로 <e1>...\n",
            "15  yz4ctet7cda(e_4|e_22)  5번째 차량 ( 3513호 ) 앞 <e1> 대차가 </e1> <e2> 선로 </e2...\n",
            "16   mz4f90xj280(e_6|e_4)  ITX 새마을열차가 10:16경 경부선 하선 밀양역 구내 진입 중 서울기점 383....\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MLM(Masked Language Model)\n",
        "- Unsupervised Learning (비지도 학습)\n",
        "- Pretraining Bert Model for unique task (ex. Finance, Architect, Art)"
      ],
      "metadata": {
        "id": "gKbTGIGeUWSq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgi-_y8msBe6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123,
          "referenced_widgets": [
            "a3d86b6e709a40ebbce170ed200679e2",
            "eff3624d36984219bf0e59184914dd4a",
            "07707d3196a44c7d85b80f1c58ea7d95",
            "f4c8687ab3484a25826d1af127a62316",
            "313a1d36c9b7409c8c6ea34d4701930e",
            "f8c0afac9081461593e75af50ea35c5f",
            "196aba78853b4109b8428ccf40c881b1",
            "2fef0799d88044fb9dd4640f64e08096",
            "4aed4f1702474566a9db0a616a71cad7",
            "00b4c76bb5fc4429b80100900a37395c",
            "095f0ed0f23b4c60938f1dd77f4a1ac3"
          ]
        },
        "outputId": "6a72f93b-76a5-4a8f-e52e-d6e1a7c1052d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/714M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a3d86b6e709a40ebbce170ed200679e2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "model = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#압축파일 풀기\n",
        "folder_path = \"./tagtog_relation_extraction/\"\n",
        "\n",
        "zip_ = zipfile.ZipFile(\"Railway_Annotation.zip\")\n",
        "if os.path.exists(folder_path):\n",
        "    shutil.rmtree(folder_path)\n",
        "zip_.extractall(folder_path)\n",
        "\n",
        "#폴더 경로\n",
        "folder_name = \"./tagtog_relation_extraction/Railway_BERT/\"\n",
        "\n",
        "#context list   # context == plain.html\n",
        "context_name_list = os.listdir(folder_name + \"plain.html/pool\")\n",
        "print(context_name_list)\n",
        "\n",
        "#relation 폴더 경로   # relation == ann.json\n",
        "relation_folder_paths = glob.glob(folder_name + \"ann.json/master/pool\")\n",
        "\n",
        "#context 폴더 경로\n",
        "# contexts_folders_paths = glob.glob(folder_name + \"plain.html/pool/*\")\n",
        "contexts_folders_paths = [folder_name + \"plain.html/pool/\" + c for c in context_name_list]\n",
        "\n",
        "#anntation_lenged 정보\n",
        "annotations_legend = folder_name + \"annotations-legend.json\"\n",
        "with open(annotations_legend,\"r\") as f:\n",
        "    annotations_legend = json.load(f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxnyvE9sWtYN",
        "outputId": "cc12e602-6c22-4ead-e4f4-3e8539bb60b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['aT0v7lsjlValvCzGFIBlxAutdrrG-text_7.txt.plain.html', 'aYqDitzamGH6gW4K6Tv_k3cHoVgW-text.txt.plain.html', 'a7njLl5A3qbkk787l1KnKTod9AHy-text_2.txt.plain.html', 'a2nq4ZkAtmxrdM.yECpQx1Ae4rYu-text_4.txt.plain.html', 'aFBtrQNRHHFnpD56uimygcqw5tAW-text_1.txt.plain.html', 'aXdTlZzcuUIsWDg1KxZzhYiaHUWC-text_6.txt.plain.html', 'aVLI6NO6mNSFompKzlsJc0skVUo8-text_5.txt.plain.html', 'agCjSEwp8H0mHv_BeVgYapyRM80m-text_3.txt.plain.html']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_tag(html_text):\n",
        "  check = html_text\n",
        "  check = re.sub('(<([^>]+)>)', '', check).strip()\n",
        "  check = re.sub('\\n', '', check)\n",
        "  check = re.sub(' +', ' ', check)\n",
        "  check = check[41:]\n",
        "  check = re.sub('·','', check)\n",
        "  check = re.sub('○', '', check)\n",
        "  return check"
      ],
      "metadata": {
        "id": "qQ80uYqsWwtt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check = ''\n",
        "\n",
        "for context_path in contexts_folders_paths:\n",
        "  with open(context_path, \"r\") as f:\n",
        "    sample = f.read()\n",
        "    sample = remove_tag(sample)\n",
        "  print(sample)\n",
        "  check += sample\n",
        "\n",
        "MLM_test = check\n",
        "check = sent_tokenize(check)\n",
        "\n",
        "print(check)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-vLSPaiWyaP",
        "outputId": "e334d109-b5ef-4db8-8688-19e3d29c24fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1A호 선로전환기가 장애이더라도 21B호 선로전환기기 정상이면 신호기는 진행으로 현시된다.\n",
            " 차량 ( 3513호 ) 앞 대차가 선로 우측으로 탈선하였다.\n",
            "관사가 정지신호를 무시하고 운행하다가 선로전환기를 할출한 후 관제원의 되돌이 운전 지시를 받고, 선로전환기 상태를 확인하지 않고 되돌이 운전을 시행한 것\n",
            "차의 맨 후부가 53A/B호 분기기를 지날 때 마지막 차량 후부대차 4축 우측 차륜이 레일위로 타고 오른 후 우측으로 탈선되어 다른 차량과 분리되면서 우측으로 전도되었다.\n",
            "축에 설치되어 있는 차축 베어링 외륜의 내면과 롤러에 결함이 있는 상태에서 운행 도중 차축 베어링이 파손되면서 차축이 절손되어 탈선한 것\n",
            "량신호소 21B호 선로전환기 첨단부가 서울방향으로 밀착되지 못하고 벌어진 장애 상태에서 청량신호소 출발신호기에 정지 신호가 현시되어야 함에도 불구하고 진행신호가 현시되도록 신호기계실내 분선반 단자대의 21A호, 21B호 선로전환기 배선을 반대로 시공한 것\n",
            "발신호기 진행 상태에서 21B2호 선로전환기를 통과 중 선두 동력차가 탈선하고 이어 모든 차량이 탈선하였다.\n",
            "TX 새마을열차가 10:16경 경부선 하선 밀양역 구내 진입 중 서울기점 383.622km 지점에서 작업 중이던 선로보수 작업원을 약 42m 전방에서 발견하고 기적과 비상제동을 동시 취급하였으나 제동거리가 미치지 못하여 직원 교통사상사고가 발생\n",
            "['1A호 선로전환기가 장애이더라도 21B호 선로전환기기 정상이면 신호기는 진행으로 현시된다.', '차량 ( 3513호 ) 앞 대차가 선로 우측으로 탈선하였다.관사가 정지신호를 무시하고 운행하다가 선로전환기를 할출한 후 관제원의 되돌이 운전 지시를 받고, 선로전환기 상태를 확인하지 않고 되돌이 운전을 시행한 것차의 맨 후부가 53A/B호 분기기를 지날 때 마지막 차량 후부대차 4축 우측 차륜이 레일위로 타고 오른 후 우측으로 탈선되어 다른 차량과 분리되면서 우측으로 전도되었다.축에 설치되어 있는 차축 베어링 외륜의 내면과 롤러에 결함이 있는 상태에서 운행 도중 차축 베어링이 파손되면서 차축이 절손되어 탈선한 것량신호소 21B호 선로전환기 첨단부가 서울방향으로 밀착되지 못하고 벌어진 장애 상태에서 청량신호소 출발신호기에 정지 신호가 현시되어야 함에도 불구하고 진행신호가 현시되도록 신호기계실내 분선반 단자대의 21A호, 21B호 선로전환기 배선을 반대로 시공한 것발신호기 진행 상태에서 21B2호 선로전환기를 통과 중 선두 동력차가 탈선하고 이어 모든 차량이 탈선하였다.TX 새마을열차가 10:16경 경부선 하선 밀양역 구내 진입 중 서울기점 383.622km 지점에서 작업 중이던 선로보수 작업원을 약 42m 전방에서 발견하고 기적과 비상제동을 동시 취급하였으나 제동거리가 미치지 못하여 직원 교통사상사고가 발생']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = check[0]\n",
        "inputs = bert_tokenizer(text, return_tensors='pt')\n",
        "\n",
        "### max length에 대한 padding을 포함한 경우 ###\n",
        "# inputs = bert_tokenizer(text, return_tensors='pt', max_length=512, truncation=True, padding='max_length')\n",
        "\n",
        "print(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zz5AEm1NROXe",
        "outputId": "86eccbc5-c476-40dc-e4c2-15f63f8e3dc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[   101,  74903,  20309,   9428,  11261,  16617,  51745,  47869,   9657,\n",
            "         119121,  10739,  54141,  17342,  12092,  10296,  11274,  20309,   9428,\n",
            "          11261,  16617,  51745,  12310,  12310,   9670,  83902,  14867,   9487,\n",
            "          20309,  46216,   9708,  25549,  11467,   9978,  14040,  22096,    119,\n",
            "            102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs['labels'] = inputs.input_ids.detach().clone()"
      ],
      "metadata": {
        "id": "YYYAysBLRWw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "rand = torch.rand(inputs.input_ids.shape)\n",
        "\n",
        "mask_arr = (rand < 0.15) * (inputs.input_ids != 101) * (inputs.input_ids != 102) # 101, 102번 토큰 제외하고 15% 위치 선별\n",
        "\n",
        "### padding이 포함된 경우(0번도 추가 제외) ###\n",
        "# mask_arr = (rand < 0.15) * (inputs.input_ids != 101) * (inputs.input_ids != 102) * (inputs.input_ids != 0)\n",
        "\n",
        "selection = torch.flatten((mask_arr[0]).nonzero())\n",
        "\n",
        "print(selection) # 선별된 위치의 인덱스 번호"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdG7QyUJRbq-",
        "outputId": "629cdf8f-2553-4d13-f13b-aedfa21c3ca4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 5, 14, 19, 24, 25])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "selection_val = np.random.random(len(selection)) # selection의 위치마다 0~1 값 부여\n",
        "\n",
        "mask_selection = selection[np.where(selection_val >= 0.2)[0]] # 80% : Mask 토큰 대체\n",
        "random_selection = selection[np.where(selection_val < 0.1)[0]] # 10% : 랜덤 토큰 대체\n",
        "\n",
        "print(random_selection)\n",
        "print(mask_selection)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7uGGUM4Rdj0",
        "outputId": "7b03c561-7a24-4f45-fc80-096eea2972a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([], dtype=torch.int64)\n",
            "tensor([ 5, 14, 19, 24, 25])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs.input_ids[0, mask_selection] = 103\n",
        "inputs.input_ids[0, random_selection] = torch.randint(0, 30522, size = random_selection.shape)\n",
        "\n",
        "print(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6EwoMfxeRfkI",
        "outputId": "50133991-253a-48ab-c332-aff8936865f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[   101,  74903,  20309,   9428,  11261,    103,  51745,  47869,   9657,\n",
            "         119121,  10739,  54141,  17342,  12092,    103,  11274,  20309,   9428,\n",
            "          11261,    103,  51745,  12310,  12310,   9670,    103,    103,   9487,\n",
            "          20309,  46216,   9708,  25549,  11467,   9978,  14040,  22096,    119,\n",
            "            102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[   101,  74903,  20309,   9428,  11261,  16617,  51745,  47869,   9657,\n",
            "         119121,  10739,  54141,  17342,  12092,  10296,  11274,  20309,   9428,\n",
            "          11261,  16617,  51745,  12310,  12310,   9670,  83902,  14867,   9487,\n",
            "          20309,  46216,   9708,  25549,  11467,   9978,  14040,  22096,    119,\n",
            "            102]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model(**inputs)\n",
        "\n",
        "outputs.loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtDGlB91Rh7t",
        "outputId": "6737a148-217b-45e4-9806-e83213db1555"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.5076, grad_fn=<NllLossBackward>)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 셋 클래스\n",
        "class MLMDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "    def __len__(self):\n",
        "        return len(self.encodings.input_ids)\n",
        "\n",
        "from transformers import TrainingArguments\n",
        "from transformers import Trainer\n",
        "\n",
        "# 위에서 설정한 inputs를 데이터 셋 클래스로 지정\n",
        "dataset = MLMDataset(inputs)\n",
        "\n",
        "# argument 설정 예시\n",
        "args = TrainingArguments(\n",
        "    output_dir='result', # 결과 정보를 받아볼 디렉토리\n",
        "    per_device_train_batch_size=16, # gpu 당 batch size 수\n",
        "    num_train_epochs=2 # epoch 수\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model, # BERT 모델이 저장된 변수\n",
        "    args=args, # 지정한 argument\n",
        "    train_dataset=dataset # 데이터셋\n",
        ")"
      ],
      "metadata": {
        "id": "JToacAxoRj7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n",
        "trainer.save_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "id": "fkuspZlXRl00",
        "outputId": "a2ce14e3-15c9-4a7f-b3b3-d4804f51caf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 1\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 2\n",
            "  Number of trainable parameters = 177974523\n",
            "<ipython-input-19-0b5abc2d62c4>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2/2 00:00, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Saving model checkpoint to result\n",
            "Configuration saved in result/config.json\n",
            "Configuration saved in result/generation_config.json\n",
            "Model weights saved in result/pytorch_model.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Name Entity Recognition\n",
        "\n",
        "Target Model:  \n",
        "\n",
        "NER-Bert\n",
        "<br/>&nbsp;&nbsp;&nbsp;&nbsp; |ㅡ main\n",
        "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |ㅡ data_preprocessing\n",
        "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |ㅡ data_loader\n",
        "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |ㅡ model (module: BertForTokenClassification) -> (bert-base-multilingual-case)\n",
        "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|ㅡ trainer\n",
        "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|ㅡ evaluate"
      ],
      "metadata": {
        "id": "zapSz8M995hT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Data Preprocessing"
      ],
      "metadata": {
        "id": "FVePYrMQ4LBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split labels based on whitespace and turn them into a list\n",
        "labels = [i.split() for i in entity_data['label'].values.tolist()]\n",
        "# Check how many labels are there in the dataset\n",
        "unique_labels = set()\n",
        "\n",
        "for lb in labels:\n",
        "  [unique_labels.add(i) for i in lb if i not in unique_labels]\n",
        " \n",
        "print(unique_labels)\n",
        "\n",
        "# Map each label into its id representation and vice versa\n",
        "labels_to_ids = {k: v for v, k in enumerate(sorted(unique_labels))}\n",
        "ids_to_labels = {v: k for v, k in enumerate(sorted(unique_labels))}\n",
        "print(labels_to_ids)\n",
        "\n",
        "print(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3UMz3vviyNu",
        "outputId": "3619c50b-26e6-4bbd-d8cc-0055c232b7d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'IMPLIED', 'ACCIDENT', 'OBJECT', 'SOLUTION', 'O', 'CAUSE'}\n",
            "{'ACCIDENT': 0, 'CAUSE': 1, 'IMPLIED': 2, 'O': 3, 'OBJECT': 4, 'SOLUTION': 5}\n",
            "[['O', 'O', 'O', 'O', 'IMPLIED', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'IMPLIED', 'O', 'O', 'O', 'O', 'CAUSE', 'O', 'O', 'ACCIDENT', 'O', 'ACCIDENT', 'O'], ['O', 'O', 'IMPLIED', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'CAUSE', 'CAUSE', 'O', 'O', 'OBJECT', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'IMPLIED', 'O', 'O', 'O', 'O', 'O', 'O', 'CAUSE', 'O', 'O', 'ACCIDENT', 'O'], ['O', 'O', 'IMPLIED', 'O', 'CAUSE', 'O', 'O', 'O', 'OBJECT', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'CAUSE', 'CAUSE', 'O', 'O', 'ACCIDENT', 'ACCIDENT', 'O', 'O'], ['O', 'O', 'IMPLIED', 'IMPLIED', 'O', 'O', 'O', 'O', 'IMPLIED', 'IMPLIED', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'IMPLIED', 'O', 'IMPLIED', 'CAUSE', 'O'], ['O', 'OBJECT', 'O', 'O', 'O', 'O', 'IMPLIED', 'SOLUTION', 'O', 'ACCIDENT', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'CAUSE', 'O', 'O', 'O', 'ACCIDENT', 'O']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example = entity_data['text'][0]\n",
        "print(example)\n",
        "\n",
        "label = entity_data['label'][0]\n",
        "print(label)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-hI1Q2bKvD3",
        "outputId": "5d25dcf8-e093-40ef-996e-b4658c2593f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "열차의 맨 후부가 53A/B호 분기기를 지날 때 마지막 차량 후부대차 4축 우측 차륜이 레일위로 타고 오른 후 우측으로 탈선되어 다른 차량과 분리되면서 우측으로 전도되었다 .\n",
            "O O O O IMPLIED O O O O O O O O IMPLIED O O O O CAUSE O O ACCIDENT O ACCIDENT O\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizerFast\n",
        "\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased')\n",
        "text_tokenized = tokenizer(example, padding='max_length', max_length=512, truncation=True, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "eDJ4qr5LK5dJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77ba6ec8-0253-4133-8e30-9c5cf8c00ba3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/vocab.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/tokenizer_config.json\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text_tokenized)\n",
        "print(tokenizer.decode(text_tokenized.input_ids[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PP2ZQh0kK_Xw",
        "outputId": "9ce34951-60e1-4a4e-de28-d538cb1e0282"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[   101,   9569,  23466,  10459,   9260,  10003,  81896,  11756,  10738,\n",
            "            120,    139,  20309,   9367,  12310,  29669,   9706,  41919,   9137,\n",
            "          67313,   9730,  44321,  10003,  14646,  14423,  23466,    125,  70122,\n",
            "           9604, 119281,   9730, 118899,  10739,   9186,  18392,  92454,   9845,\n",
            "          11664,   9580,  37819,  10003,   9604, 119281,  11467,   9848,  18471,\n",
            "          16855,  19709,   9730,  44321,  11882,   9367,  12692,  96377,   9604,\n",
            "         119281,  11467,   9665,  12092,  13628,    119,    102,      0,      0,\n",
            "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "              0,      0,      0,      0,      0,      0,      0,      0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0]])}\n",
            "[CLS] 열차의 맨 후부가 53A / B호 분기기를 지날 때 마지막 차량 후부대차 4축 우측 차륜이 레일위로 타고 오른 후 우측으로 탈선되어 다른 차량과 분리되면서 우측으로 전도되었다. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.convert_ids_to_tokens(text_tokenized[\"input_ids\"][0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "furIZprjLCV9",
        "outputId": "5ed7cff8-716d-496a-bb69-e858ef5abed7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', '열', '##차', '##의', '맨', '후', '##부가', '53', '##A', '/', 'B', '##호', '분', '##기', '##기를', '지', '##날', '때', '마지막', '차', '##량', '후', '##부', '##대', '##차', '4', '##축', '우', '##측', '차', '##륜', '##이', '레', '##일', '##위로', '타', '##고', '오', '##른', '후', '우', '##측', '##으로', '탈', '##선', '##되어', '다른', '차', '##량', '##과', '분', '##리', '##되면서', '우', '##측', '##으로', '전', '##도', '##되었다', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_ids = text_tokenized.word_ids()\n",
        "print(tokenizer.convert_ids_to_tokens(text_tokenized[\"input_ids\"][0]))\n",
        "print(word_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrqI8BrwLGNY",
        "outputId": "8b15c211-4eae-4797-8c7b-5b265a1ac5ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', '열', '##차', '##의', '맨', '후', '##부가', '53', '##A', '/', 'B', '##호', '분', '##기', '##기를', '지', '##날', '때', '마지막', '차', '##량', '후', '##부', '##대', '##차', '4', '##축', '우', '##측', '차', '##륜', '##이', '레', '##일', '##위로', '타', '##고', '오', '##른', '후', '우', '##측', '##으로', '탈', '##선', '##되어', '다른', '차', '##량', '##과', '분', '##리', '##되면서', '우', '##측', '##으로', '전', '##도', '##되었다', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
            "[None, 0, 0, 0, 1, 2, 2, 3, 3, 4, 5, 5, 6, 6, 6, 7, 7, 8, 9, 10, 10, 11, 11, 11, 11, 12, 12, 13, 13, 14, 14, 14, 15, 15, 15, 16, 16, 17, 17, 18, 19, 19, 19, 20, 20, 20, 21, 22, 22, 22, 23, 23, 23, 24, 24, 24, 25, 25, 25, 26, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def align_label_example(tokenized_input, labels):\n",
        "\n",
        "        word_ids = tokenized_input.word_ids()\n",
        "\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "   \n",
        "        for word_idx in word_ids:\n",
        "\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "                \n",
        "            elif word_idx != previous_word_idx:\n",
        "                try:\n",
        "                  label_ids.append(labels_to_ids[labels[word_idx]])\n",
        "                except:\n",
        "                  label_ids.append(-100)\n",
        "        \n",
        "            else:\n",
        "                label_ids.append(labels_to_ids[labels[word_idx]] if label_all_tokens else -100)\n",
        "            previous_word_idx = word_idx\n",
        "      \n",
        "\n",
        "        return label_ids"
      ],
      "metadata": {
        "id": "4_2BrKc7ifJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label = labels[0]\n",
        "print(label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLXfv3lqjeG9",
        "outputId": "ee6db4ee-ba5b-4df2-fc65-bbf798ee9938"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['O', 'O', 'O', 'O', 'IMPLIED', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'IMPLIED', 'O', 'O', 'O', 'O', 'CAUSE', 'O', 'O', 'ACCIDENT', 'O', 'ACCIDENT', 'O']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_all_tokens = True\n",
        "\n",
        "new_label = align_label_example(text_tokenized, label)\n",
        "print(new_label)\n",
        "print(tokenizer.convert_ids_to_tokens(text_tokenized[\"input_ids\"][0]))\n",
        "\n",
        "label_all_tokens = False\n",
        "\n",
        "new_label = align_label_example(text_tokenized, label)\n",
        "print(new_label)\n",
        "print(tokenizer.convert_ids_to_tokens(text_tokenized[\"input_ids\"][0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "id": "mJSWItXyihTZ",
        "outputId": "c75bb5ba-13c4-4331-f4e1-2a714243eb97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-af46744efc22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlabel_all_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnew_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malign_label_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_tokenized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_ids_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_tokenized\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-ee9ab37366f7>\u001b[0m in \u001b[0;36malign_label_example\u001b[0;34m(tokenized_input, labels)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                 \u001b[0mlabel_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_to_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlabel_all_tokens\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mprevious_word_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Data_Loader"
      ],
      "metadata": {
        "id": "jdM6JwO5jn-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def align_label(texts, labels):\n",
        "    tokenized_inputs = tokenizer(texts, padding='max_length', max_length=512, truncation=True)\n",
        "\n",
        "    word_ids = tokenized_inputs.word_ids()\n",
        "\n",
        "    previous_word_idx = None\n",
        "    label_ids = []\n",
        "\n",
        "    for word_idx in word_ids:\n",
        "\n",
        "        if word_idx is None:\n",
        "            label_ids.append(-100)\n",
        "\n",
        "        elif word_idx != previous_word_idx:\n",
        "            try:\n",
        "                label_ids.append(labels_to_ids[labels[word_idx]])\n",
        "            except:\n",
        "                label_ids.append(-100)\n",
        "        else:\n",
        "            try:\n",
        "                label_ids.append(labels_to_ids[labels[word_idx]] if label_all_tokens else -100)\n",
        "            except:\n",
        "                label_ids.append(-100)\n",
        "        previous_word_idx = word_idx\n",
        "\n",
        "    return label_ids\n",
        "\n",
        "class DataSequence(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, df):\n",
        "\n",
        "        lb = [i.split() for i in df['label'].values.tolist()]\n",
        "        txt = df['text'].values.tolist()\n",
        "        self.texts = [tokenizer(str(i),\n",
        "                               padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\") for i in txt]\n",
        "        self.labels = [align_label(i,j) for i,j in zip(txt, lb)]\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.labels)\n",
        "\n",
        "    def get_batch_data(self, idx):\n",
        "\n",
        "        return self.texts[idx]\n",
        "\n",
        "    def get_batch_labels(self, idx):\n",
        "\n",
        "        return torch.LongTensor(self.labels[idx])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        batch_data = self.get_batch_data(idx)\n",
        "        batch_labels = self.get_batch_labels(idx)\n",
        "\n",
        "        return batch_data, batch_labels"
      ],
      "metadata": {
        "id": "BtlhIaZyjtux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(entity_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0MsXP775_tK",
        "outputId": "973db133-f390-4d08-a857-61d0de55e94b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text  \\\n",
            "0  열차의 맨 후부가 53A/B호 분기기를 지날 때 마지막 차량 후부대차 4축 우측 차...   \n",
            "1  기관사가 정지신호를 무시하고 운행하다가 선로전환기를 할출한 후 관제원의 되돌이 운전...   \n",
            "2  차축에 설치되어 있는 차축 베어링 외륜의 내면과 롤러에 결함이 있는 상태에서 운행 ...   \n",
            "3  21A호 선로전환기가 장애이더라도 21B호 선로전환기기 정상이면 신호기는 진행으로 ...   \n",
            "4  출발신호기 진행 상태에서 21B2호 선로전환기를 통과 중 선두 동력차가 탈선하고 이...   \n",
            "5  청량신호소 21B호 선로전환기 첨단부가 서울방향으로 밀착되지 못하고 벌어진 장애 상...   \n",
            "6             5번째 차량 ( 3513호 ) 앞 대차가 선로 우측으로 탈선하였다 .   \n",
            "7  ITX 새마을열차가 10:16경 경부선 하선 밀양역 구내 진입 중 서울기점 383....   \n",
            "\n",
            "                                               label  \n",
            "0  O O O O IMPLIED O O O O O O O O IMPLIED O O O ...  \n",
            "1  O O IMPLIED O O O O O O O O O O O O CAUSE CAUS...  \n",
            "2  O O O O O O O O IMPLIED O O O O O O CAUSE O O ...  \n",
            "3                 O O IMPLIED O CAUSE O O O OBJECT O  \n",
            "4  O O O O O O O CAUSE CAUSE O O ACCIDENT ACCIDEN...  \n",
            "5  O O IMPLIED IMPLIED O O O O IMPLIED IMPLIED O ...  \n",
            "6     O OBJECT O O O O IMPLIED SOLUTION O ACCIDENT O  \n",
            "7  O O O O O O O O O O O O O O O O O O O O O O O ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = entity_data.loc[:,[\"text\", \"label\"]]\n",
        "df.columns = [\"text\", \"label\"]\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFcyQa5I5xo4",
        "outputId": "daf9b8e8-da28-4c1d-bb26-b6104d368196"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text  \\\n",
            "0  열차의 맨 후부가 53A/B호 분기기를 지날 때 마지막 차량 후부대차 4축 우측 차...   \n",
            "1  기관사가 정지신호를 무시하고 운행하다가 선로전환기를 할출한 후 관제원의 되돌이 운전...   \n",
            "2  차축에 설치되어 있는 차축 베어링 외륜의 내면과 롤러에 결함이 있는 상태에서 운행 ...   \n",
            "3  21A호 선로전환기가 장애이더라도 21B호 선로전환기기 정상이면 신호기는 진행으로 ...   \n",
            "4  출발신호기 진행 상태에서 21B2호 선로전환기를 통과 중 선두 동력차가 탈선하고 이...   \n",
            "5  청량신호소 21B호 선로전환기 첨단부가 서울방향으로 밀착되지 못하고 벌어진 장애 상...   \n",
            "6             5번째 차량 ( 3513호 ) 앞 대차가 선로 우측으로 탈선하였다 .   \n",
            "7  ITX 새마을열차가 10:16경 경부선 하선 밀양역 구내 진입 중 서울기점 383....   \n",
            "\n",
            "                                               label  \n",
            "0  O O O O IMPLIED O O O O O O O O IMPLIED O O O ...  \n",
            "1  O O IMPLIED O O O O O O O O O O O O CAUSE CAUS...  \n",
            "2  O O O O O O O O IMPLIED O O O O O O CAUSE O O ...  \n",
            "3                 O O IMPLIED O CAUSE O O O OBJECT O  \n",
            "4  O O O O O O O CAUSE CAUSE O O ACCIDENT ACCIDEN...  \n",
            "5  O O IMPLIED IMPLIED O O O O IMPLIED IMPLIED O ...  \n",
            "6     O OBJECT O O O O IMPLIED SOLUTION O ACCIDENT O  \n",
            "7  O O O O O O O O O O O O O O O O O O O O O O O ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "df = df[0:1000]\n",
        "df_train, df_val, df_test = np.split(df.sample(frac=1, random_state=42),\n",
        "                            [int(.8 * len(df)), int(.9 * len(df))])"
      ],
      "metadata": {
        "id": "27qP3CVfj1Wj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  Model"
      ],
      "metadata": {
        "id": "ZIYp3tPCOJOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForTokenClassification\n",
        "\n",
        "class BertModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        super(BertModel, self).__init__()\n",
        "\n",
        "        self.bert = BertForTokenClassification.from_pretrained('bert-base-multilingual-cased', num_labels=len(unique_labels))\n",
        "\n",
        "    def forward(self, input_id, mask, label):\n",
        "\n",
        "        output = self.bert(input_ids=input_id, attention_mask=mask, labels=label, return_dict=False)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "MXDGYbnsj1gu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Trainer"
      ],
      "metadata": {
        "id": "F33AYNAwOYRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(model, df_train, df_val):\n",
        "\n",
        "    train_dataset = DataSequence(df_train)\n",
        "    val_dataset = DataSequence(df_val)\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, num_workers=4, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_dataloader = DataLoader(val_dataset, num_workers=4, batch_size=BATCH_SIZE)\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    if use_cuda:\n",
        "        model = model.cuda()\n",
        "\n",
        "    best_acc = 0\n",
        "    best_loss = 1000\n",
        "\n",
        "    for epoch_num in range(EPOCHS):\n",
        "\n",
        "        total_acc_train = 0\n",
        "        total_loss_train = 0\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        for train_data, train_label in tqdm(train_dataloader):\n",
        "\n",
        "            train_label = train_label.to(device)\n",
        "            mask = train_data['attention_mask'].squeeze(1).to(device)\n",
        "            input_id = train_data['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss, logits = model(input_id, mask, train_label)\n",
        "\n",
        "            for i in range(logits.shape[0]):\n",
        "\n",
        "              logits_clean = logits[i][train_label[i] != -100]\n",
        "              label_clean = train_label[i][train_label[i] != -100]\n",
        "\n",
        "              predictions = logits_clean.argmax(dim=1)\n",
        "              acc = (predictions == label_clean).float().mean()\n",
        "              total_acc_train += acc\n",
        "              total_loss_train += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        total_acc_val = 0\n",
        "        total_loss_val = 0\n",
        "\n",
        "        for val_data, val_label in val_dataloader:\n",
        "\n",
        "            val_label = val_label.to(device)\n",
        "            mask = val_data['attention_mask'].squeeze(1).to(device)\n",
        "            input_id = val_data['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "            loss, logits = model(input_id, mask, val_label)\n",
        "\n",
        "            for i in range(logits.shape[0]):\n",
        "\n",
        "              logits_clean = logits[i][val_label[i] != -100]\n",
        "              label_clean = val_label[i][val_label[i] != -100]\n",
        "\n",
        "              predictions = logits_clean.argmax(dim=1)\n",
        "              acc = (predictions == label_clean).float().mean()\n",
        "              total_acc_val += acc\n",
        "              total_loss_val += loss.item()\n",
        "\n",
        "        val_accuracy = total_acc_val / len(df_val)\n",
        "        val_loss = total_loss_val / len(df_val)\n",
        "\n",
        "        print(\n",
        "            f'Epochs: {epoch_num + 1} | Loss: {total_loss_train / len(df_train): .3f} | Accuracy: {total_acc_train / len(df_train): .3f} | Val_Loss: {total_loss_val / len(df_val): .3f} | Accuracy: {total_acc_val / len(df_val): .3f}')\n",
        "\n",
        "LEARNING_RATE = 5e-3\n",
        "EPOCHS = 1\n",
        "BATCH_SIZE = 2\n",
        "\n",
        "model = BertModel()\n",
        "train_loop(model, df_train, df_val)"
      ],
      "metadata": {
        "id": "zwmQ5lvBj1mC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c700682c-53e7-4d95-8059-cd01ddf597df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_5\": 5\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.26.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/fdfce55e83dbed325647a63e7e1f5de19f0382ba/pytorch_model.bin\n",
            "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:474: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "100%|██████████| 3/3 [00:01<00:00,  1.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 1 | Loss:  3.851 | Accuracy:  0.307 | Val_Loss:  4.074 | Accuracy:  0.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Evaluate"
      ],
      "metadata": {
        "id": "vQtPxddGOavf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, df_test):\n",
        "\n",
        "    test_dataset = DataSequence(df_test)\n",
        "\n",
        "    test_dataloader = DataLoader(test_dataset, num_workers=4, batch_size=1)\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    if use_cuda:\n",
        "        model = model.cuda()\n",
        "\n",
        "    total_acc_test = 0.0\n",
        "\n",
        "    for test_data, test_label in test_dataloader:\n",
        "\n",
        "            test_label = test_label.to(device)\n",
        "            mask = test_data['attention_mask'].squeeze(1).to(device)\n",
        "\n",
        "            input_id = test_data['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "            loss, logits = model(input_id, mask, test_label)\n",
        "\n",
        "            for i in range(logits.shape[0]):\n",
        "\n",
        "              logits_clean = logits[i][test_label[i] != -100]\n",
        "              label_clean = test_label[i][test_label[i] != -100]\n",
        "\n",
        "              predictions = logits_clean.argmax(dim=1)\n",
        "              acc = (predictions == label_clean).float().mean()\n",
        "              total_acc_test += acc\n",
        "\n",
        "    val_accuracy = total_acc_test / len(df_test)\n",
        "    print(f'Test Accuracy: {total_acc_test / len(df_test): .3f}')\n",
        "\n",
        "\n",
        "evaluate(model, df_test)"
      ],
      "metadata": {
        "id": "obXKerRi8UYb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2341730d-15e7-414b-9211-877a5458fa74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy:  0.136\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def align_word_ids(texts):\n",
        "  \n",
        "    tokenized_inputs = tokenizer(texts, padding='max_length', max_length=512, truncation=True)\n",
        "\n",
        "    word_ids = tokenized_inputs.word_ids()\n",
        "\n",
        "    previous_word_idx = None\n",
        "    label_ids = []\n",
        "\n",
        "    for word_idx in word_ids:\n",
        "\n",
        "        if word_idx is None:\n",
        "            label_ids.append(-100)\n",
        "\n",
        "        elif word_idx != previous_word_idx:\n",
        "            try:\n",
        "                label_ids.append(1)\n",
        "            except:\n",
        "                label_ids.append(-100)\n",
        "        else:\n",
        "            try:\n",
        "                label_ids.append(1 if label_all_tokens else -100)\n",
        "            except:\n",
        "                label_ids.append(-100)\n",
        "        previous_word_idx = word_idx\n",
        "\n",
        "    return label_ids\n",
        "\n",
        "\n",
        "def evaluate_one_text(model, sentence):\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    if use_cuda:\n",
        "        model = model.cuda()\n",
        "\n",
        "    text = tokenizer(sentence, padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "    mask = text['attention_mask'].to(device)\n",
        "    input_id = text['input_ids'].to(device)\n",
        "    label_ids = torch.Tensor(align_word_ids(sentence)).unsqueeze(0).to(device)\n",
        "\n",
        "    logits = model(input_id, mask, None)\n",
        "    logits_clean = logits[0][label_ids != -100]\n",
        "\n",
        "    predictions = logits_clean.argmax(dim=1).tolist()\n",
        "    prediction_label = [ids_to_labels[i] for i in predictions]\n",
        "    print(tokenizer.tokenize(sentence))\n",
        "    print(len(tokenizer.tokenize(sentence)))\n",
        "    print(prediction_label)\n",
        "    print(len(prediction_label))\n",
        "\n",
        "evaluate_one_text(model, '열차가 부산역 진입 철도에서 탈선함')"
      ],
      "metadata": {
        "id": "sxAcYS938WNe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab14e240-67f4-4a7f-a38b-76816e38155a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['열', '##차', '##가', '부', '##산', '##역', '진', '##입', '철도', '##에서', '탈', '##선', '##함']\n",
            "13\n",
            "['ACCIDENT', 'ACCIDENT', 'ACCIDENT', 'ACCIDENT', 'ACCIDENT', 'ACCIDENT', 'ACCIDENT', 'ACCIDENT', 'ACCIDENT', 'ACCIDENT', 'ACCIDENT', 'ACCIDENT', 'ACCIDENT']\n",
            "13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Relation Extraction\n",
        "Target Model: https://github.com/monologg/R-BERT\n",
        "\n",
        "※data, eval 폴더 추가!!\n",
        "\n",
        "R-Bert\n",
        "<br/>&nbsp;&nbsp;&nbsp;&nbsp; |ㅡ main\n",
        "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |ㅡ official_eval\n",
        "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |ㅡ utils\n",
        "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |ㅡ data_loader\n",
        "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |ㅡ model(bert-base-multilingual-case)\n",
        "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|ㅡ trainer\n",
        "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|ㅡ predict\n",
        "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|ㅡ evaluate"
      ],
      "metadata": {
        "id": "BRNlWX_lYKuC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Official Eval"
      ],
      "metadata": {
        "id": "R4V4dCV_InVR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "EVAL_DIR = \"eval\"\n",
        "\n",
        "\n",
        "def official_f1():\n",
        "    # Run the perl script\n",
        "    try:\n",
        "        cmd = \"perl {0}/semeval2010_task8_scorer-v1.2.pl {0}/proposed_answers.txt {0}/answer_keys.txt > {0}/result.txt\".format(\n",
        "            EVAL_DIR\n",
        "        )\n",
        "        os.system(cmd)\n",
        "    except:\n",
        "        raise Exception(\"perl is not installed or proposed_answers.txt is missing\")\n",
        "\n",
        "    with open(os.path.join(EVAL_DIR, \"result.txt\"), \"r\", encoding=\"utf-8\") as f:\n",
        "        macro_result = list(f)[-1]\n",
        "        macro_result = macro_result.split(\":\")[1].replace(\">>>\", \"\").strip()\n",
        "        macro_result = macro_result.split(\"=\")[1].strip().replace(\"%\", \"\")\n",
        "        macro_result = float(macro_result) / 100\n",
        "\n",
        "    return macro_result\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     print(\"macro-averaged F1 = {}%\".format(official_f1() * 100))"
      ],
      "metadata": {
        "id": "0GzskfPiHCiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Utils"
      ],
      "metadata": {
        "id": "_RkM0BsNG1iM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "\n",
        "ADDITIONAL_SPECIAL_TOKENS = [\"<e1>\", \"</e1>\", \"<e2>\", \"</e2>\"]\n",
        "\n",
        "\n",
        "def get_label(args):\n",
        "    return [label.strip() for label in open(os.path.join(args.data_dir, args.label_file), \"r\", encoding=\"utf-8\")]\n",
        "\n",
        "\n",
        "def load_tokenizer(args):\n",
        "    tokenizer = BertTokenizer.from_pretrained(args.model_name_or_path)\n",
        "    tokenizer.add_special_tokens({\"additional_special_tokens\": ADDITIONAL_SPECIAL_TOKENS})\n",
        "    return tokenizer\n",
        "\n",
        "\n",
        "def write_prediction(args, output_file, preds):\n",
        "    relation_labels = get_label(args)\n",
        "    \n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        for idx, pred in enumerate(preds):\n",
        "            print('\\n', idx, \"predict result is \", relation_labels[pred])\n",
        "            f.write(\"{}\\t{}\\n\".format(8001 + idx, relation_labels[pred]))\n",
        "            \n",
        "\n",
        "\n",
        "def init_logger():\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO,\n",
        "    )\n",
        "\n",
        "\n",
        "def set_seed(args):\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    if not args.no_cuda and torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "\n",
        "def compute_metrics(preds, labels):\n",
        "    assert len(preds) == len(labels)\n",
        "    return acc_and_f1(preds, labels)\n",
        "\n",
        "\n",
        "def simple_accuracy(preds, labels):\n",
        "    return (preds == labels).mean()\n",
        "\n",
        "\n",
        "def acc_and_f1(preds, labels, average=\"macro\"):\n",
        "    acc = simple_accuracy(preds, labels)\n",
        "    return {\n",
        "        \"acc\": acc,\n",
        "        \"f1\": 100\n",
        "        #\"f1\": official_f1(),\n",
        "    }"
      ],
      "metadata": {
        "id": "mxp0o5R2G0Ui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Data Loader"
      ],
      "metadata": {
        "id": "RZ-50mAbGL3g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANq0ukPCFE79"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import csv\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class InputExample(object):\n",
        "    \"\"\"\n",
        "    A single training/test example for simple sequence classification.\n",
        "    Args:\n",
        "        guid: Unique id for the example.\n",
        "        text_a: string. The untokenized text of the first sequence. For single\n",
        "        sequence tasks, only this sequence must be specified.\n",
        "        label: (Optional) string. The label of the example. This should be\n",
        "        specified for train and dev examples, but not for test examples.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, guid, text_a, label):\n",
        "        self.guid = guid\n",
        "        self.text_a = text_a\n",
        "        self.label = label\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.to_json_string())\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
        "        output = copy.deepcopy(self.__dict__)\n",
        "        return output\n",
        "\n",
        "    def to_json_string(self):\n",
        "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
        "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n",
        "\n",
        "\n",
        "class InputFeatures(object):\n",
        "    \"\"\"\n",
        "    A single set of features of data.\n",
        "    Args:\n",
        "        input_ids: Indices of input sequence tokens in the vocabulary.\n",
        "        attention_mask: Mask to avoid performing attention on padding token indices.\n",
        "            Mask values selected in ``[0, 1]``:\n",
        "            Usually  ``1`` for tokens that are NOT MASKED, ``0`` for MASKED (padded) tokens.\n",
        "        token_type_ids: Segment token indices to indicate first and second portions of the inputs.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_ids, attention_mask, token_type_ids, label_id, e1_mask, e2_mask):\n",
        "        self.input_ids = input_ids\n",
        "        self.attention_mask = attention_mask\n",
        "        self.token_type_ids = token_type_ids\n",
        "        self.label_id = label_id\n",
        "        self.e1_mask = e1_mask\n",
        "        self.e2_mask = e2_mask\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.to_json_string())\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
        "        output = copy.deepcopy(self.__dict__)\n",
        "        return output\n",
        "\n",
        "    def to_json_string(self):\n",
        "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
        "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n",
        "\n",
        "\n",
        "class SemEvalProcessor(object):\n",
        "    \"\"\"Processor for the Semeval data set \"\"\"\n",
        "\n",
        "    def __init__(self, args):\n",
        "        self.args = args\n",
        "        self.relation_labels = get_label(args)\n",
        "\n",
        "    @classmethod\n",
        "    def _read_tsv(cls, input_file, quotechar=None):\n",
        "        \"\"\"Reads a tab separated value file.\"\"\"\n",
        "        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
        "            lines = []\n",
        "            for line in reader:\n",
        "                lines.append(line)\n",
        "            return lines\n",
        "\n",
        "    def _create_examples(self, lines, set_type):\n",
        "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "        examples = []\n",
        "        for (i, line) in enumerate(lines):\n",
        "            guid = \"%s-%s\" % (set_type, i)\n",
        "            text_a = line[1]\n",
        "            label = self.relation_labels.index(line[0])\n",
        "            if i % 1000 == 0:\n",
        "                logger.info(line)\n",
        "            examples.append(InputExample(guid=guid, text_a=text_a, label=label))\n",
        "        return examples\n",
        "\n",
        "    def get_examples(self, mode):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            mode: train, dev, test\n",
        "        \"\"\"\n",
        "        file_to_read = None\n",
        "        if mode == \"train\":\n",
        "            file_to_read = self.args.train_file\n",
        "        elif mode == \"dev\":\n",
        "            file_to_read = self.args.dev_file\n",
        "        elif mode == \"test\":\n",
        "            file_to_read = self.args.test_file\n",
        "\n",
        "        logger.info(\"LOOKING AT {}\".format(os.path.join(self.args.data_dir, file_to_read)))\n",
        "        return self._create_examples(self._read_tsv(os.path.join(self.args.data_dir, file_to_read)), mode)\n",
        "\n",
        "\n",
        "processors = {\"semeval\": SemEvalProcessor}\n",
        "\n",
        "\n",
        "def convert_examples_to_features(\n",
        "    examples,\n",
        "    max_seq_len,\n",
        "    tokenizer,\n",
        "    cls_token=\"[CLS]\",\n",
        "    cls_token_segment_id=0,\n",
        "    sep_token=\"[SEP]\",\n",
        "    pad_token=0,\n",
        "    pad_token_segment_id=0,\n",
        "    sequence_a_segment_id=0,\n",
        "    add_sep_token=False,\n",
        "    mask_padding_with_zero=True,\n",
        "):\n",
        "    features = []\n",
        "    for (ex_index, example) in enumerate(examples):\n",
        "        if ex_index % 5000 == 0:\n",
        "            logger.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
        "\n",
        "        tokens_a = tokenizer.tokenize(example.text_a)\n",
        "\n",
        "        e11_p = tokens_a.index(\"<e1>\")  # the start position of entity1\n",
        "        e12_p = tokens_a.index(\"</e1>\")  # the end position of entity1\n",
        "        e21_p = tokens_a.index(\"<e2>\")  # the start position of entity2\n",
        "        e22_p = tokens_a.index(\"</e2>\")  # the end position of entity2\n",
        "\n",
        "        # Replace the token\n",
        "        tokens_a[e11_p] = \"$\"\n",
        "        tokens_a[e12_p] = \"$\"\n",
        "        tokens_a[e21_p] = \"#\"\n",
        "        tokens_a[e22_p] = \"#\"\n",
        "\n",
        "        # Add 1 because of the [CLS] token\n",
        "        e11_p += 1\n",
        "        e12_p += 1\n",
        "        e21_p += 1\n",
        "        e22_p += 1\n",
        "\n",
        "        # Account for [CLS] and [SEP] with \"- 2\" and with \"- 3\" for RoBERTa.\n",
        "        if add_sep_token:\n",
        "            special_tokens_count = 2\n",
        "        else:\n",
        "            special_tokens_count = 1\n",
        "        if len(tokens_a) > max_seq_len - special_tokens_count:\n",
        "            tokens_a = tokens_a[: (max_seq_len - special_tokens_count)]\n",
        "\n",
        "        tokens = tokens_a\n",
        "        if add_sep_token:\n",
        "            tokens += [sep_token]\n",
        "\n",
        "        token_type_ids = [sequence_a_segment_id] * len(tokens)\n",
        "\n",
        "        tokens = [cls_token] + tokens\n",
        "        token_type_ids = [cls_token_segment_id] + token_type_ids\n",
        "\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # The mask has 1 for real tokens and 0 for padding tokens. Only real tokens are attended to.\n",
        "        attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
        "\n",
        "        # Zero-pad up to the sequence length.\n",
        "        padding_length = max_seq_len - len(input_ids)\n",
        "        input_ids = input_ids + ([pad_token] * padding_length)\n",
        "        attention_mask = attention_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n",
        "        token_type_ids = token_type_ids + ([pad_token_segment_id] * padding_length)\n",
        "\n",
        "        # e1 mask, e2 mask\n",
        "        e1_mask = [0] * len(attention_mask)\n",
        "        e2_mask = [0] * len(attention_mask)\n",
        "\n",
        "        for i in range(e11_p, e12_p + 1):\n",
        "            e1_mask[i] = 1\n",
        "        for i in range(e21_p, e22_p + 1):\n",
        "            e2_mask[i] = 1\n",
        "\n",
        "        assert len(input_ids) == max_seq_len, \"Error with input length {} vs {}\".format(len(input_ids), max_seq_len)\n",
        "        assert len(attention_mask) == max_seq_len, \"Error with attention mask length {} vs {}\".format(\n",
        "            len(attention_mask), max_seq_len\n",
        "        )\n",
        "        assert len(token_type_ids) == max_seq_len, \"Error with token type length {} vs {}\".format(\n",
        "            len(token_type_ids), max_seq_len\n",
        "        )\n",
        "\n",
        "        label_id = int(example.label)\n",
        "\n",
        "        if ex_index < 5:\n",
        "            logger.info(\"*** Example ***\")\n",
        "            logger.info(\"guid: %s\" % example.guid)\n",
        "            logger.info(\"tokens: %s\" % \" \".join([str(x) for x in tokens]))\n",
        "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
        "            logger.info(\"attention_mask: %s\" % \" \".join([str(x) for x in attention_mask]))\n",
        "            logger.info(\"token_type_ids: %s\" % \" \".join([str(x) for x in token_type_ids]))\n",
        "            logger.info(\"label: %s (id = %d)\" % (example.label, label_id))\n",
        "            logger.info(\"e1_mask: %s\" % \" \".join([str(x) for x in e1_mask]))\n",
        "            logger.info(\"e2_mask: %s\" % \" \".join([str(x) for x in e2_mask]))\n",
        "\n",
        "        features.append(\n",
        "            InputFeatures(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                token_type_ids=token_type_ids,\n",
        "                label_id=label_id,\n",
        "                e1_mask=e1_mask,\n",
        "                e2_mask=e2_mask,\n",
        "            )\n",
        "        )\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "def load_and_cache_examples(args, tokenizer, mode):\n",
        "    processor = processors[args.task](args)\n",
        "\n",
        "    # Load data features from cache or dataset file\n",
        "    cached_features_file = os.path.join(\n",
        "        args.data_dir,\n",
        "        \"cached_{}_{}_{}_{}\".format(\n",
        "            mode,\n",
        "            args.task,\n",
        "            list(filter(None, args.model_name_or_path.split(\"/\"))).pop(),\n",
        "            args.max_seq_len,\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    if os.path.exists(cached_features_file):\n",
        "        logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
        "        features = torch.load(cached_features_file)\n",
        "    else:\n",
        "        logger.info(\"Creating features from dataset file at %s\", args.data_dir)\n",
        "        if mode == \"train\":\n",
        "            examples = processor.get_examples(\"train\")\n",
        "        elif mode == \"dev\":\n",
        "            examples = processor.get_examples(\"dev\")\n",
        "        elif mode == \"test\":\n",
        "            examples = processor.get_examples(\"test\")\n",
        "        else:\n",
        "            raise Exception(\"For mode, Only train, dev, test is available\")\n",
        "\n",
        "        features = convert_examples_to_features(\n",
        "            examples, args.max_seq_len, tokenizer, add_sep_token=args.add_sep_token\n",
        "        )\n",
        "        logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
        "        torch.save(features, cached_features_file)\n",
        "\n",
        "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
        "    all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n",
        "    all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n",
        "    all_e1_mask = torch.tensor([f.e1_mask for f in features], dtype=torch.long)  # add e1 mask\n",
        "    all_e2_mask = torch.tensor([f.e2_mask for f in features], dtype=torch.long)  # add e2 mask\n",
        "\n",
        "    all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n",
        "\n",
        "    dataset = TensorDataset(\n",
        "        all_input_ids,\n",
        "        all_attention_mask,\n",
        "        all_token_type_ids,\n",
        "        all_label_ids,\n",
        "        all_e1_mask,\n",
        "        all_e2_mask,\n",
        "    )\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Model"
      ],
      "metadata": {
        "id": "hu8M5Jw4GUK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel, BertPreTrainedModel\n",
        "\n",
        "\n",
        "class FCLayer(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, dropout_rate=0.0, use_activation=True):\n",
        "        super(FCLayer, self).__init__()\n",
        "        self.use_activation = use_activation\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.linear = nn.Linear(input_dim, output_dim)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dropout(x)\n",
        "        if self.use_activation:\n",
        "            x = self.tanh(x)\n",
        "        return self.linear(x)\n",
        "\n",
        "\n",
        "class RBERT(BertPreTrainedModel):\n",
        "    def __init__(self, config, args):\n",
        "        super(RBERT, self).__init__(config)\n",
        "        self.bert = BertModel(config=config)  # Load pretrained bert\n",
        "\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        self.cls_fc_layer = FCLayer(config.hidden_size, config.hidden_size, args.dropout_rate)\n",
        "        self.entity_fc_layer = FCLayer(config.hidden_size, config.hidden_size, args.dropout_rate)\n",
        "        self.label_classifier = FCLayer(\n",
        "            config.hidden_size * 3,\n",
        "            config.num_labels,\n",
        "            args.dropout_rate,\n",
        "            use_activation=False,\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def entity_average(hidden_output, e_mask):\n",
        "        \"\"\"\n",
        "        Average the entity hidden state vectors (H_i ~ H_j)\n",
        "        :param hidden_output: [batch_size, j-i+1, dim]\n",
        "        :param e_mask: [batch_size, max_seq_len]\n",
        "                e.g. e_mask[0] == [0, 0, 0, 1, 1, 1, 0, 0, ... 0]\n",
        "        :return: [batch_size, dim]\n",
        "        \"\"\"\n",
        "        e_mask_unsqueeze = e_mask.unsqueeze(1)  # [b, 1, j-i+1]\n",
        "        length_tensor = (e_mask != 0).sum(dim=1).unsqueeze(1)  # [batch_size, 1]\n",
        "\n",
        "        # [b, 1, j-i+1] * [b, j-i+1, dim] = [b, 1, dim] -> [b, dim]\n",
        "        sum_vector = torch.bmm(e_mask_unsqueeze.float(), hidden_output).squeeze(1)\n",
        "        avg_vector = sum_vector.float() / length_tensor.float()  # broadcasting\n",
        "        return avg_vector\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids, labels, e1_mask, e2_mask):\n",
        "        outputs = self.bert(\n",
        "            input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids\n",
        "        )  # sequence_output, pooled_output, (hidden_states), (attentions)\n",
        "        sequence_output = outputs[0]\n",
        "        pooled_output = outputs[1]  # [CLS]\n",
        "\n",
        "        # Average\n",
        "        e1_h = self.entity_average(sequence_output, e1_mask)\n",
        "        e2_h = self.entity_average(sequence_output, e2_mask)\n",
        "\n",
        "        # Dropout -> tanh -> fc_layer (Share FC layer for e1 and e2)\n",
        "        pooled_output = self.cls_fc_layer(pooled_output)\n",
        "        e1_h = self.entity_fc_layer(e1_h)\n",
        "        e2_h = self.entity_fc_layer(e2_h)\n",
        "\n",
        "        # Concat -> fc_layer\n",
        "        concat_h = torch.cat([pooled_output, e1_h, e2_h], dim=-1)\n",
        "        logits = self.label_classifier(concat_h)\n",
        "\n",
        "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
        "\n",
        "        # Softmax\n",
        "        if labels is not None:\n",
        "            if self.num_labels == 1:\n",
        "                loss_fct = nn.MSELoss()\n",
        "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
        "            else:\n",
        "                loss_fct = nn.CrossEntropyLoss()\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "\n",
        "            outputs = (loss,) + outputs\n",
        "\n",
        "        return outputs  # (loss), logits, (hidden_states), (attentions)"
      ],
      "metadata": {
        "id": "0TsF1HyvGgDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trainer & Evaluate"
      ],
      "metadata": {
        "id": "Ng1ULrHTJoGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from tqdm import tqdm, trange\n",
        "from transformers import AdamW, BertConfig, get_linear_schedule_with_warmup\n",
        "import easydict\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class Trainer(object):\n",
        "    def __init__(self, args, train_dataset=None, dev_dataset=None, test_dataset=None):\n",
        "        self.args = args\n",
        "        self.train_dataset = train_dataset\n",
        "        self.dev_dataset = dev_dataset\n",
        "        self.test_dataset = test_dataset\n",
        "\n",
        "        self.label_lst = get_label(args)\n",
        "        self.num_labels = len(self.label_lst)\n",
        "\n",
        "        self.config = BertConfig.from_pretrained(\n",
        "            args.model_name_or_path,\n",
        "            num_labels=self.num_labels,\n",
        "            finetuning_task=args.task,\n",
        "            id2label={str(i): label for i, label in enumerate(self.label_lst)},\n",
        "            label2id={label: i for i, label in enumerate(self.label_lst)},\n",
        "        )\n",
        "        self.model = RBERT.from_pretrained(args.model_name_or_path, config=self.config, args=args)\n",
        "\n",
        "        # GPU or CPU\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\"\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def train(self):\n",
        "        train_sampler = RandomSampler(self.train_dataset)\n",
        "        train_dataloader = DataLoader(\n",
        "            self.train_dataset,\n",
        "            sampler=train_sampler,\n",
        "            batch_size=self.args.train_batch_size,\n",
        "        )\n",
        "\n",
        "        if self.args.max_steps > 0:\n",
        "            t_total = self.args.max_steps\n",
        "            self.args.num_train_epochs = (\n",
        "                self.args.max_steps // (len(train_dataloader) // self.args.gradient_accumulation_steps) + 1\n",
        "            )\n",
        "        else:\n",
        "            t_total = len(train_dataloader) // self.args.gradient_accumulation_steps * self.args.num_train_epochs\n",
        "\n",
        "        # Prepare optimizer and schedule (linear warmup and decay)\n",
        "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "        optimizer_grouped_parameters = [\n",
        "            {\n",
        "                \"params\": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "                \"weight_decay\": self.args.weight_decay,\n",
        "            },\n",
        "            {\n",
        "                \"params\": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "                \"weight_decay\": 0.0,\n",
        "            },\n",
        "        ]\n",
        "        optimizer = AdamW(\n",
        "            optimizer_grouped_parameters,\n",
        "            lr=self.args.learning_rate,\n",
        "            eps=self.args.adam_epsilon,\n",
        "        )\n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            optimizer,\n",
        "            num_warmup_steps=self.args.warmup_steps,\n",
        "            num_training_steps=t_total,\n",
        "        )\n",
        "\n",
        "        # Train!\n",
        "        logger.info(\"***** Running training *****\")\n",
        "        logger.info(\"  Num examples = %d\", len(self.train_dataset))\n",
        "        logger.info(\"  Num Epochs = %d\", self.args.num_train_epochs)\n",
        "        logger.info(\"  Total train batch size = %d\", self.args.train_batch_size)\n",
        "        logger.info(\"  Gradient Accumulation steps = %d\", self.args.gradient_accumulation_steps)\n",
        "        logger.info(\"  Total optimization steps = %d\", t_total)\n",
        "        logger.info(\"  Logging steps = %d\", self.args.logging_steps)\n",
        "        logger.info(\"  Save steps = %d\", self.args.save_steps)\n",
        "        \n",
        "        global_step = 0\n",
        "        tr_loss = 0.0\n",
        "        self.model.zero_grad()\n",
        "\n",
        "        train_iterator = trange(int(self.args.num_train_epochs), desc=\"Epoch\")\n",
        "\n",
        "        for _ in train_iterator:\n",
        "            epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\")\n",
        "            for step, batch in enumerate(epoch_iterator):\n",
        "                self.model.train()\n",
        "                batch = tuple(t.to(self.device) for t in batch)  # GPU or CPU\n",
        "                inputs = {\n",
        "                    \"input_ids\": batch[0],\n",
        "                    \"attention_mask\": batch[1],\n",
        "                    \"token_type_ids\": batch[2],\n",
        "                    \"labels\": batch[3],\n",
        "                    \"e1_mask\": batch[4],\n",
        "                    \"e2_mask\": batch[5],\n",
        "                }\n",
        "                outputs = self.model(**inputs)\n",
        "                loss = outputs[0]\n",
        "\n",
        "                if self.args.gradient_accumulation_steps > 1:\n",
        "                    loss = loss / self.args.gradient_accumulation_steps\n",
        "\n",
        "                loss.backward()\n",
        "\n",
        "                tr_loss += loss.item()\n",
        "                if (step + 1) % self.args.gradient_accumulation_steps == 0:\n",
        "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.args.max_grad_norm)\n",
        "\n",
        "                    optimizer.step()\n",
        "                    scheduler.step()  # Update learning rate schedule\n",
        "                    self.model.zero_grad()\n",
        "                    global_step += 1\n",
        "\n",
        "                    if self.args.logging_steps > 0 and global_step % self.args.logging_steps == 0:\n",
        "                        self.evaluate(\"test\")  # There is no dev set for semeval task\n",
        "\n",
        "                    if self.args.save_steps > 0 and global_step % self.args.save_steps == 0:\n",
        "                        self.save_model()\n",
        "\n",
        "                if 0 < self.args.max_steps < global_step:\n",
        "                    epoch_iterator.close()\n",
        "                    break\n",
        "\n",
        "            if 0 < self.args.max_steps < global_step:\n",
        "                train_iterator.close()\n",
        "                break\n",
        "\n",
        "        return global_step, tr_loss / global_step\n",
        "\n",
        "    def evaluate(self, mode):\n",
        "        # We use test dataset because semeval doesn't have dev dataset\n",
        "        if mode == \"test\":\n",
        "            dataset = self.test_dataset\n",
        "        elif mode == \"dev\":\n",
        "            dataset = self.dev_dataset\n",
        "        else:\n",
        "            raise Exception(\"Only dev and test dataset available\")\n",
        "\n",
        "        eval_sampler = SequentialSampler(dataset)\n",
        "        eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=self.args.eval_batch_size)\n",
        "\n",
        "        # Eval!\n",
        "        logger.info(\"***** Running evaluation on %s dataset *****\", mode)\n",
        "        logger.info(\"  Num examples = %d\", len(dataset))\n",
        "        logger.info(\"  Batch size = %d\", self.args.eval_batch_size)\n",
        "        eval_loss = 0.0\n",
        "        nb_eval_steps = 0\n",
        "        preds = None\n",
        "        out_label_ids = None\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "        for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
        "            batch = tuple(t.to(self.device) for t in batch)\n",
        "            with torch.no_grad():\n",
        "                inputs = {\n",
        "                    \"input_ids\": batch[0],\n",
        "                    \"attention_mask\": batch[1],\n",
        "                    \"token_type_ids\": batch[2],\n",
        "                    \"labels\": batch[3],\n",
        "                    \"e1_mask\": batch[4],\n",
        "                    \"e2_mask\": batch[5],\n",
        "                }\n",
        "                outputs = self.model(**inputs)\n",
        "                tmp_eval_loss, logits = outputs[:2]\n",
        "\n",
        "                eval_loss += tmp_eval_loss.mean().item()\n",
        "            nb_eval_steps += 1\n",
        "\n",
        "            if preds is None:\n",
        "                preds = logits.detach().cpu().numpy()\n",
        "                out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n",
        "            else:\n",
        "                preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
        "                out_label_ids = np.append(out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0)\n",
        "\n",
        "        eval_loss = eval_loss / nb_eval_steps\n",
        "        results = {\"loss\": eval_loss}\n",
        "        preds = np.argmax(preds, axis=1)\n",
        "        write_prediction(self.args, os.path.join(self.args.eval_dir, \"proposed_answers.txt\"), preds)\n",
        "\n",
        "        result = compute_metrics(preds, out_label_ids)\n",
        "        results.update(result)\n",
        "\n",
        "        logger.info(\"***** Eval results *****\")\n",
        "        for key in sorted(results.keys()):\n",
        "            logger.info(\"  {} = {:.4f}\".format(key, results[key]))\n",
        "\n",
        "        return results\n",
        "\n",
        "    def save_model(self):\n",
        "        # Save model checkpoint (Overwrite)\n",
        "        if not os.path.exists(self.args.model_dir):\n",
        "            os.makedirs(self.args.model_dir)\n",
        "        model_to_save = self.model.module if hasattr(self.model, \"module\") else self.model\n",
        "        model_to_save.save_pretrained(self.args.model_dir)\n",
        "\n",
        "        # Save training arguments together with the trained model\n",
        "        torch.save(self.args, os.path.join(self.args.model_dir, \"training_args.bin\"))\n",
        "        logger.info(\"Saving model checkpoint to %s\", self.args.model_dir)\n",
        "\n",
        "    def load_model(self):\n",
        "        # Check whether model exists\n",
        "        if not os.path.exists(self.args.model_dir):\n",
        "            raise Exception(\"Model doesn't exists! Train first!\")\n",
        "\n",
        "        self.args = torch.load(os.path.join(self.args.model_dir, \"training_args.bin\"))\n",
        "        self.model = RBERT.from_pretrained(self.args.model_dir, args=self.args)\n",
        "        self.model.to(self.device)\n",
        "        logger.info(\"***** Model Loaded *****\")"
      ],
      "metadata": {
        "id": "Ma-8AtEmGgW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predict"
      ],
      "metadata": {
        "id": "4FsKK4iyJ420"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import logging\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, SequentialSampler, TensorDataset\n",
        "from tqdm import tqdm\n",
        "import easydict\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "def get_device(pred_config):\n",
        "    return \"cuda\" if torch.cuda.is_available() and not pred_config.no_cuda else \"cpu\"\n",
        "\n",
        "\n",
        "def get_args(pred_config):\n",
        "    return torch.load(os.path.join(pred_config.model_dir, \"training_args.bin\"))\n",
        "\n",
        "\n",
        "def load_model(pred_config, args, device):\n",
        "    # Check whether model exists\n",
        "    if not os.path.exists(pred_config.model_dir):\n",
        "        raise Exception(\"Model doesn't exists! Train first!\")\n",
        "\n",
        "    try:\n",
        "        model = RBERT.from_pretrained(pred_config.model_dir, args=args)\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "        logger.info(\"***** Model Loaded *****\")\n",
        "    except:\n",
        "        raise Exception(\"Some model files might be missing...\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def convert_input_file_to_tensor_dataset(\n",
        "    pred_config,\n",
        "    args,\n",
        "    cls_token_segment_id=0,\n",
        "    pad_token_segment_id=0,\n",
        "    sequence_a_segment_id=0,\n",
        "    mask_padding_with_zero=True,\n",
        "):\n",
        "    tokenizer = load_tokenizer(args)\n",
        "\n",
        "    # Setting based on the current model type\n",
        "    cls_token = tokenizer.cls_token\n",
        "    sep_token = tokenizer.sep_token\n",
        "    pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "    all_input_ids = []\n",
        "    all_attention_mask = []\n",
        "    all_token_type_ids = []\n",
        "    all_e1_mask = []\n",
        "    all_e2_mask = []\n",
        "\n",
        "    with open(pred_config.input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            tokens = tokenizer.tokenize(line)\n",
        "\n",
        "            e11_p = tokens.index(\"<e1>\")  # the start position of entity1\n",
        "            e12_p = tokens.index(\"</e1>\")  # the end position of entity1\n",
        "            e21_p = tokens.index(\"<e2>\")  # the start position of entity2\n",
        "            e22_p = tokens.index(\"</e2>\")  # the end position of entity2\n",
        "\n",
        "            # Replace the token\n",
        "            tokens[e11_p] = \"$\"\n",
        "            tokens[e12_p] = \"$\"\n",
        "            tokens[e21_p] = \"#\"\n",
        "            tokens[e22_p] = \"#\"\n",
        "\n",
        "            # Add 1 because of the [CLS] token\n",
        "            e11_p += 1\n",
        "            e12_p += 1\n",
        "            e21_p += 1\n",
        "            e22_p += 1\n",
        "\n",
        "            # Account for [CLS] and [SEP] with \"- 2\" and with \"- 3\" for RoBERTa.\n",
        "            if args.add_sep_token:\n",
        "                special_tokens_count = 2\n",
        "            else:\n",
        "                special_tokens_count = 1\n",
        "            if len(tokens) > args.max_seq_len - special_tokens_count:\n",
        "                tokens = tokens[: (args.max_seq_len - special_tokens_count)]\n",
        "\n",
        "            # Add [SEP] token\n",
        "            if args.add_sep_token:\n",
        "                tokens += [sep_token]\n",
        "            token_type_ids = [sequence_a_segment_id] * len(tokens)\n",
        "\n",
        "            # Add [CLS] token\n",
        "            tokens = [cls_token] + tokens\n",
        "            token_type_ids = [cls_token_segment_id] + token_type_ids\n",
        "\n",
        "            input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "            # The mask has 1 for real tokens and 0 for padding tokens. Only real tokens are attended to.\n",
        "            attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
        "\n",
        "            # Zero-pad up to the sequence length.\n",
        "            padding_length = args.max_seq_len - len(input_ids)\n",
        "            input_ids = input_ids + ([pad_token_id] * padding_length)\n",
        "            attention_mask = attention_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n",
        "            token_type_ids = token_type_ids + ([pad_token_segment_id] * padding_length)\n",
        "\n",
        "            # e1 mask, e2 mask\n",
        "            e1_mask = [0] * len(attention_mask)\n",
        "            e2_mask = [0] * len(attention_mask)\n",
        "\n",
        "            for i in range(e11_p, e12_p + 1):\n",
        "                e1_mask[i] = 1\n",
        "            for i in range(e21_p, e22_p + 1):\n",
        "                e2_mask[i] = 1\n",
        "\n",
        "            all_input_ids.append(input_ids)\n",
        "            all_attention_mask.append(attention_mask)\n",
        "            all_token_type_ids.append(token_type_ids)\n",
        "            all_e1_mask.append(e1_mask)\n",
        "            all_e2_mask.append(e2_mask)\n",
        "\n",
        "    # Change to Tensor\n",
        "    all_input_ids = torch.tensor(all_input_ids, dtype=torch.long)\n",
        "    all_attention_mask = torch.tensor(all_attention_mask, dtype=torch.long)\n",
        "    all_token_type_ids = torch.tensor(all_token_type_ids, dtype=torch.long)\n",
        "    all_e1_mask = torch.tensor(all_e1_mask, dtype=torch.long)\n",
        "    all_e2_mask = torch.tensor(all_e2_mask, dtype=torch.long)\n",
        "\n",
        "    dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_e1_mask, all_e2_mask)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def predict(pred_config):\n",
        "    # load model and args\n",
        "    args = get_args(pred_config)\n",
        "    device = get_device(pred_config)\n",
        "    model = load_model(pred_config, args, device)\n",
        "    logger.info(args)\n",
        "\n",
        "    # Convert input file to TensorDataset\n",
        "    dataset = convert_input_file_to_tensor_dataset(pred_config, args)\n",
        "\n",
        "    # Predict\n",
        "    sampler = SequentialSampler(dataset)\n",
        "    data_loader = DataLoader(dataset, sampler=sampler, batch_size=pred_config.batch_size)\n",
        "\n",
        "    preds = None\n",
        "\n",
        "    for batch in tqdm(data_loader, desc=\"Predicting\"):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        with torch.no_grad():\n",
        "            inputs = {\n",
        "                \"input_ids\": batch[0],\n",
        "                \"attention_mask\": batch[1],\n",
        "                \"token_type_ids\": batch[2],\n",
        "                \"labels\": None,\n",
        "                \"e1_mask\": batch[3],\n",
        "                \"e2_mask\": batch[4],\n",
        "            }\n",
        "            outputs = model(**inputs)\n",
        "            logits = outputs[0]\n",
        "\n",
        "            if preds is None:\n",
        "                preds = logits.detach().cpu().numpy()\n",
        "            else:\n",
        "                preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
        "\n",
        "    preds = np.argmax(preds, axis=1)\n",
        "\n",
        "    # Write to output file\n",
        "    label_lst = get_label(args)\n",
        "    print(preds)\n",
        "    with open(pred_config.output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        for pred in preds:\n",
        "            f.write(\"{}\\n\".format(label_lst[pred]))\n",
        "\n",
        "    logger.info(\"Prediction Done!\")\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     init_logger()\n",
        "    \n",
        "#     pred_config = easydict.EasyDict({\n",
        "#         \"input_file\" : \"sample_pred_in.txt\",\n",
        "#         \"output_file\" : \"sample_pred_out.txt\",\n",
        "#         \"model_dir\" : \"./model\",\n",
        "#         \"batch_size\" : 32,\n",
        "#         \"no_cuda\" : \"store_true\"\n",
        "#     })\n",
        "\n",
        "#     predict(pred_config)"
      ],
      "metadata": {
        "id": "-fRNjGliJmCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Main"
      ],
      "metadata": {
        "id": "5HFKpiQXKPl1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "\n",
        "def main(args):\n",
        "    init_logger()\n",
        "    set_seed(args)\n",
        "    tokenizer = load_tokenizer(args)\n",
        "\n",
        "    train_dataset = load_and_cache_examples(args, tokenizer, mode=\"train\")\n",
        "    test_dataset = load_and_cache_examples(args, tokenizer, mode=\"test\")\n",
        "\n",
        "    trainer = Trainer(args, train_dataset=train_dataset, test_dataset=test_dataset)\n",
        "\n",
        "    if args.do_train:\n",
        "        trainer.train()\n",
        "        trainer.save_model()\n",
        "    if args.do_eval:\n",
        "        trainer.load_model()\n",
        "        trainer.evaluate(\"test\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    args = easydict.EasyDict({\n",
        "        \"task\" : \"semeval\",\n",
        "        \"data_dir\" : \"./data\",\n",
        "        \"model_dir\" : \"./model\",\n",
        "        \"eval_dir\" : \"./eval\",\n",
        "        \"train_file\" : \"train.tsv\",\n",
        "        \"test_file\" : \"test.tsv\",\n",
        "        \"label_file\" : \"label.txt\",\n",
        "        \"model_name_or_path\" : \"bert-base-uncased\",\n",
        "        \"seed\" : 77,\n",
        "        \"train_batch_size\" : 16,\n",
        "        \"eval_batch_size\" : 32,\n",
        "        \"max_seq_len\" : 384,\n",
        "        \"learning_rate\" : 2e-5,\n",
        "        \"num_train_epochs\" : 1.0,\n",
        "        \"weight_decay\" : 0.0,\n",
        "        \"gradient_accumulation_steps\" : 1,\n",
        "        \"adam_epsilon\" : 1e-8,\n",
        "        \"max_grad_norm\" : 1.0,\n",
        "        \"max_steps\" : -1,\n",
        "        \"warmup_steps\" : 0,\n",
        "        \"dropout_rate\" : 0.1,\n",
        "        \"logging_steps\" : 250,\n",
        "        \"save_steps\" : 250,\n",
        "        \"do_train\" : \"store_true\",\n",
        "        \"do_eval\" : \"store_true\",\n",
        "        \"no_cuda\" : \"store_true\",\n",
        "        \"add_sep_token\" : \"store_true\",\n",
        "    })\n",
        "\n",
        "    main(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNATPRATKRV8",
        "outputId": "63c90b54-773c-41db-905c-bd8cdef07cf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing RBERT: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing RBERT from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RBERT from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RBERT were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['entity_fc_layer.linear.weight', 'entity_fc_layer.linear.bias', 'cls_fc_layer.linear.weight', 'label_classifier.linear.weight', 'label_classifier.linear.bias', 'cls_fc_layer.linear.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "Iteration:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration: 100%|██████████| 1/1 [00:35<00:00, 35.19s/it]\n",
            "Epoch: 100%|██████████| 1/1 [00:35<00:00, 35.20s/it]\n",
            "Evaluating: 100%|██████████| 1/1 [00:06<00:00,  6.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " 0 predict result is  yz4ctet7cda(e_4|e_22)\n",
            "\n",
            " 1 predict result is  yz4ctet7cda(e_4|e_22)\n",
            "\n",
            " 2 predict result is  yz4ctet7cda(e_4|e_22)\n",
            "\n",
            " 3 predict result is  yz4ctet7cda(e_4|e_22)\n",
            "\n",
            " 4 predict result is  yz4ctet7cda(e_4|e_22)\n",
            "\n",
            " 5 predict result is  yz4ctet7cda(e_4|e_22)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}