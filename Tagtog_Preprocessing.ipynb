{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOWsO033BkD5xRIWuDmj6wp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/slayerzeroa/Railway_Accident_BERT/blob/main/Tagtog_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jmpus_i-7MJl"
      },
      "outputs": [],
      "source": [
        "# 라이브러리\n",
        "import torch\n",
        "from torch import nn, optim, tensor\n",
        "from torch.utils.data import DataLoader, Dataset,TensorDataset, random_split\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error\n",
        "\n",
        "# from transformers import BertTokenizer\n",
        "\n",
        "import json\n",
        "import glob\n",
        "import re\n",
        "import os\n",
        "import ast\n",
        "import html\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "\n",
        "#로그인 위치\n",
        "url = \"https://tagtog.com/-login\"\n",
        "\n",
        "#다운로드 위치\n",
        "file_url = 'https://www.tagtog.com/slayerzeroa/Railway_BERT/-downloads/dataset-as-anndoc' \n",
        "o_file = 'Railway_Annotation.zip'  \n",
        "if os.path.exists(o_file):\n",
        "    os.remove(o_file)\n",
        "\n",
        "#로그인 정보\n",
        "login_info = {\n",
        "    'loginid' : 'slayerzeroa@gmail.com',\n",
        "    'password' : 'gyysxw5rU3NzrYX',\n",
        "}\n",
        "\n",
        "#로그인\n",
        "with requests.Session() as s:\n",
        "    login_req = s.post(url, data=login_info)\n",
        "    r = s.get(file_url)\n",
        "\n",
        "    with open(o_file,\"wb\") as output:\n",
        "        output.write(r.content)"
      ],
      "metadata": {
        "id": "MjgJQ4xYvb_t"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#압축파일 풀기\n",
        "import zipfile\n",
        "import shutil\n",
        "folder_path = \"./tagtog_relation_extraction/\"\n",
        "\n",
        "zip_ = zipfile.ZipFile(\"Railway_Annotation.zip\")\n",
        "if os.path.exists(folder_path):\n",
        "    shutil.rmtree(folder_path)\n",
        "zip_.extractall(folder_path)"
      ],
      "metadata": {
        "id": "dcy6zlWXzPwx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import glob\n",
        "import re\n",
        "import os\n",
        "#폴더 경로\n",
        "folder_name = \"./tagtog_relation_extraction/Railway_BERT/\"\n",
        "\n",
        "#context list   # context == plain.html\n",
        "context_name_list = os.listdir(folder_name + \"plain.html/pool\")\n",
        "print(context_name_list)\n",
        "\n",
        "#relation 폴더 경로   # relation == ann.json\n",
        "relation_folder_paths = glob.glob(folder_name + \"ann.json/master/pool\")\n",
        "\n",
        "#context 폴더 경로\n",
        "# contexts_folders_paths = glob.glob(folder_name + \"plain.html/pool/*\")\n",
        "contexts_folders_paths = [folder_name + \"plain.html/pool/\" + c for c in context_name_list]\n",
        "print(contexts_folders_paths)\n",
        "\n",
        "#anntation_lenged 정보\n",
        "annotations_legend = folder_name + \"annotations-legend.json\"\n",
        "with open(annotations_legend,\"r\") as f:\n",
        "    annotations_legend = json.load(f)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjB4FYZNzWjd",
        "outputId": "d6c034b5-84b1-4535-99fd-b0fd4b49ef67"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a3DnzcVSvJKG25OeznRpiW5zLgi8-sample.pdf.plain.html']\n",
            "['./tagtog_relation_extraction/Railway_BERT/plain.html/pool/a3DnzcVSvJKG25OeznRpiW5zLgi8-sample.pdf.plain.html']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TagTog Annotation json 전처리"
      ],
      "metadata": {
        "id": "SySo6GhrvcXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_relation_start(text):\n",
        "  text = text.split('|')[-1]\n",
        "  start = text.split(',')[0]\n",
        "  start = int(start)\n",
        "  return start"
      ],
      "metadata": {
        "id": "_ZjHmbB95_F4"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for context_name, relation_folder, contexts_folder in zip(context_name_list, relation_folder_paths, contexts_folders_paths):\n",
        "    # relation files와 context files 리스트 출력\n",
        "    file_ids = [file_name for file_name in os.listdir(relation_folder)]\n",
        "    file_nums = [ids.split(\"-\")[1] for ids in file_ids]\n",
        "    relation_files = [relation_folder + \"/\"+ file_id for file_id in file_ids]\n",
        "    context_files = [contexts_folder for file_id in file_ids]\n",
        "\n",
        "    for relation_file, context_file, file_num in zip(relation_files, context_files, file_nums):\n",
        "      with open(relation_file,\"r\") as f:\n",
        "        relation = json.load(f)\n",
        "\n",
        "        idx_list = []\n",
        "        classid_list = []\n",
        "        part_list = []\n",
        "        text_list = []\n",
        "        start_list = []\n",
        "\n",
        "        for idx, content in enumerate(relation['entities']):\n",
        "          idx_list.append(idx)\n",
        "          classid_list.append(content['classId'])\n",
        "          part_list.append(content['part'])\n",
        "          text_list.append(content['offsets'][0]['text'])\n",
        "          start_list.append(content['offsets'][0]['start'])\n",
        "\n",
        "        df = pd.DataFrame(zip(idx_list, start_list, classid_list, part_list, text_list))\n",
        "        df.columns = ['Index', 'Start','Class', 'Part', 'Text']\n",
        "\n",
        "        doc_list = []\n",
        "        loc_list = []\n",
        "        cls_list = []\n",
        "        part_list = []\n",
        "        subject_list = []\n",
        "        object_list = []\n",
        "        label_list = []\n",
        "\n",
        "        for rel in relation['relations']:\n",
        "          subject = split_relation_start(rel['entities'][0])\n",
        "          subject_class = df.loc[df['Start']==subject]['Class'].values[0]\n",
        "          subject_part = df.loc[df['Start']==subject]['Part'].values[0]\n",
        "          subject_text = df.loc[df['Start']==subject]['Text'].values[0]\n",
        "          \n",
        "\n",
        "          object = split_relation_start(rel['entities'][1])\n",
        "          object_class = df.loc[df['Start']==object]['Class'].values[0]\n",
        "          object_part = df.loc[df['Start']==object]['Part'].values[0]\n",
        "          object_text = df.loc[df['Start']==object]['Text'].values[0]\n",
        "\n",
        "          loc = str(subject) + '_' + str(object)\n",
        "          cls = subject_class + '_' + object_class\n",
        "          part = subject_part + '_' + object_part\n",
        "\n",
        "          doc_list.append(str(file_nums)[2:-11])\n",
        "          loc_list.append(loc)\n",
        "          cls_list.append(cls)\n",
        "          part_list.append(part)\n",
        "          subject_list.append(subject_text)\n",
        "          object_list.append(object_text)\n",
        "          label_list.append(1)\n",
        "\n",
        "        Rel_df = pd.DataFrame(zip(doc_list, loc_list, cls_list, part_list, subject_list, object_list, label_list))\n",
        "        Rel_df.columns = ['Doc', 'Loc', 'Cls', 'Part', 'Sub', 'Obj', 'Label']"
      ],
      "metadata": {
        "id": "XIjJlVvp0yF2"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Rel_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdOmDXTY6nVa",
        "outputId": "94bcf08b-b9ee-4dcc-9b1f-4df6dacb7a13"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          Doc          Loc       Cls       Part  \\\n",
            "0  sample.pdf    3704_3607   e_6_e_4  s1p5_s1p5   \n",
            "1  sample.pdf    3741_3607   e_6_e_4  s1p5_s1p5   \n",
            "2  sample.pdf  13057_12960   e_6_e_4  s1p6_s1p6   \n",
            "3  sample.pdf  13094_12960   e_6_e_4  s1p6_s1p6   \n",
            "4  sample.pdf    3394_3607  e_22_e_4  s1p5_s1p5   \n",
            "5  sample.pdf    3434_3607  e_22_e_4  s1p5_s1p5   \n",
            "6  sample.pdf    3455_3607  e_22_e_4  s1p5_s1p5   \n",
            "7  sample.pdf  13368_13094  e_23_e_6  s1p6_s1p6   \n",
            "8  sample.pdf  13525_13057  e_23_e_6  s1p6_s1p6   \n",
            "\n",
            "                                                 Sub  \\\n",
            "0                화차에 컨테이너를 싣고 내릴 때 차축 베어링에 충격이 가해진 것   \n",
            "1  차축 베어링의 분해검수 주기와 교환 주기가 화차에 적재되는 화 물의 종류, 적재방법...   \n",
            "2                화차에 컨테이너를 싣고 내릴 때 차축 베어링에 충격이 가해진 것   \n",
            "3  차축 베어링의 분해검수 주기와 교환 주기가 화차에 적재되는 화 물의 종류, 적재방법...   \n",
            "4                뒷 대차 첫 번째 차축과 두 번째 차축이 진행방향 좌측으로 탈선   \n",
            "5                                    좌 측 세 번째 차축이 절손   \n",
            "6                    탈선화차와 14번째 화차 사이의 공기호스 가 빠져있는 것   \n",
            "7  화차 차축 베어링의 분해검수 주기와 교환 주기를 화차에 적재되는 화 물의 종류, 적...   \n",
            "8  컨테이너를 화차에서 들어 올릴 때 화차가 함께 들어 올려지다 떨어지지 않도록 작업 ...   \n",
            "\n",
            "                                                 Obj  Label  \n",
            "0  차축에 설치되어 있는 차축 베어링 외륜의 내면과 롤러에 결함이 있는 상태에서 운행 ...      1  \n",
            "1  차축에 설치되어 있는 차축 베어링 외륜의 내면과 롤러에 결함이 있는 상태에서 운행 ...      1  \n",
            "2  차축에 설치되어 있는 차축 베어링 외륜의 내면과 롤러에 결함이 있는 상태에서 운행 ...      1  \n",
            "3  차축에 설치되어 있는 차축 베어링 외륜의 내면과 롤러에 결함이 있는 상태에서 운행 ...      1  \n",
            "4  차축에 설치되어 있는 차축 베어링 외륜의 내면과 롤러에 결함이 있는 상태에서 운행 ...      1  \n",
            "5  차축에 설치되어 있는 차축 베어링 외륜의 내면과 롤러에 결함이 있는 상태에서 운행 ...      1  \n",
            "6  차축에 설치되어 있는 차축 베어링 외륜의 내면과 롤러에 결함이 있는 상태에서 운행 ...      1  \n",
            "7  차축 베어링의 분해검수 주기와 교환 주기가 화차에 적재되는 화 물의 종류, 적재방법...      1  \n",
            "8                화차에 컨테이너를 싣고 내릴 때 차축 베어링에 충격이 가해진 것      1  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(annotations_legend))\n",
        "\n",
        "doc_name = 'sample'\n",
        "with open(\"annotations-legend.json\",\"r\") as f:\n",
        "    annotations_legend = json.load(f)\n",
        "\n",
        "print(annotations_legend)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "NWhErqyY-H46",
        "outputId": "fad5cfa2-7feb-4d5a-8d16-afa4e90b53eb"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'dict'>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-15fe25b54460>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdoc_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'sample'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"annotations-legend.json\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mannotations_legend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'annotations-legend.json'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"sample.ann.json\",\"r\") as f:\n",
        "    relation = json.load(f)\n",
        "\n",
        "print(relation['annotatable'])\n",
        "print(relation['anncomplete'])\n",
        "print(len(relation['entities']))\n",
        "\n",
        "idx_list = []\n",
        "classid_list = []\n",
        "part_list = []\n",
        "text_list = []\n",
        "start_list = []\n",
        "\n",
        "for idx, content in enumerate(relation['entities']):\n",
        "  idx_list.append(idx)\n",
        "  classid_list.append(content['classId'])\n",
        "  part_list.append(content['part'])\n",
        "  text_list.append(content['offsets'][0]['text'])\n",
        "  start_list.append(content['offsets'][0]['start'])\n",
        "\n",
        "df = pd.DataFrame(zip(idx_list, start_list, classid_list, part_list, text_list))\n",
        "df.columns = ['Index', 'Start','Class', 'Part', 'Text']\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xe470wR-YxT",
        "outputId": "1a931c35-4cb2-40a5-ab33-7b5edc21774c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'parts': ['s1p1', 's1p2', 's1p3', 's1p4', 's1p5', 's1p6', 's1p7']}\n",
            "False\n",
            "14\n",
            "    Index  Start Class  Part  \\\n",
            "0       0   3394  e_22  s1p5   \n",
            "1       1   3434  e_22  s1p5   \n",
            "2       2   3455  e_22  s1p5   \n",
            "3       3   3607   e_4  s1p5   \n",
            "4       4   3704   e_6  s1p5   \n",
            "5       5   3741   e_6  s1p5   \n",
            "6       6  12960   e_4  s1p6   \n",
            "7       7  13057   e_6  s1p6   \n",
            "8       8  13094   e_6  s1p6   \n",
            "9       9  13368  e_23  s1p6   \n",
            "10     10  13525  e_23  s1p6   \n",
            "11     11  13608  e_23  s1p6   \n",
            "12     12  13689  e_23  s1p6   \n",
            "13     13  13763  e_23  s1p6   \n",
            "\n",
            "                                                 Text  \n",
            "0                 뒷 대차 첫 번째 차축과 두 번째 차축이 진행방향 좌측으로 탈선  \n",
            "1                                     좌 측 세 번째 차축이 절손  \n",
            "2                     탈선화차와 14번째 화차 사이의 공기호스 가 빠져있는 것  \n",
            "3   차축에 설치되어 있는 차축 베어링 외륜의 내면과 롤러에 결함이 있는 상태에서 운행 ...  \n",
            "4                 화차에 컨테이너를 싣고 내릴 때 차축 베어링에 충격이 가해진 것  \n",
            "5   차축 베어링의 분해검수 주기와 교환 주기가 화차에 적재되는 화 물의 종류, 적재방법...  \n",
            "6   차축에 설치되어 있는 차축 베어링 외륜의 내면과 롤러에 결함이 있는 상태에서 운행 ...  \n",
            "7                 화차에 컨테이너를 싣고 내릴 때 차축 베어링에 충격이 가해진 것  \n",
            "8   차축 베어링의 분해검수 주기와 교환 주기가 화차에 적재되는 화 물의 종류, 적재방법...  \n",
            "9   화차 차축 베어링의 분해검수 주기와 교환 주기를 화차에 적재되는 화 물의 종류, 적...  \n",
            "10  컨테이너를 화차에서 들어 올릴 때 화차가 함께 들어 올려지다 떨어지지 않도록 작업 ...  \n",
            "11  화차의 발열 여부를 발견하기 위해 차축에 부착한 온도 테이프는 효과가 없는 것으로 ...  \n",
            "12  역근무자가 열차 감시 업무를 적정하게 수행할 수 있도록 열차감시용 영상감시설비의 확...  \n",
            "13     신형기관차로 열차를 편성하여 운행시 열차 후부를 쉽게 감시할 수 있 도록 개선할 것  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-hrpfFXmzU2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(relation['relations'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IoyKuFPF_60x",
        "outputId": "407c95ab-31b3-465d-b4cc-610a3a7a0e23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'classId': 'r_19', 'type': 'linked', 'directed': False, 'entities': ['s1p5|e_6|3704,3738', 's1p5|e_4|3607,3683'], 'confidence': {'state': 'pre-added', 'who': ['user:slayerzeroa'], 'prob': 1}}, {'classId': 'r_19', 'type': 'linked', 'directed': False, 'entities': ['s1p5|e_6|3741,3820', 's1p5|e_4|3607,3683'], 'confidence': {'state': 'pre-added', 'who': ['user:slayerzeroa'], 'prob': 1}}, {'classId': 'r_19', 'type': 'linked', 'directed': False, 'entities': ['s1p6|e_6|13057,13091', 's1p6|e_4|12960,13036'], 'confidence': {'state': 'pre-added', 'who': ['user:slayerzeroa'], 'prob': 1}}, {'classId': 'r_19', 'type': 'linked', 'directed': False, 'entities': ['s1p6|e_6|13094,13173', 's1p6|e_4|12960,13036'], 'confidence': {'state': 'pre-added', 'who': ['user:slayerzeroa'], 'prob': 1}}, {'classId': 'r_24', 'type': 'linked', 'directed': False, 'entities': ['s1p5|e_22|3394,3428', 's1p5|e_4|3607,3683'], 'confidence': {'state': 'pre-added', 'who': ['user:slayerzeroa'], 'prob': 1}}, {'classId': 'r_24', 'type': 'linked', 'directed': False, 'entities': ['s1p5|e_22|3434,3448', 's1p5|e_4|3607,3683'], 'confidence': {'state': 'pre-added', 'who': ['user:slayerzeroa'], 'prob': 1}}, {'classId': 'r_24', 'type': 'linked', 'directed': False, 'entities': ['s1p5|e_22|3455,3485', 's1p5|e_4|3607,3683'], 'confidence': {'state': 'pre-added', 'who': ['user:slayerzeroa'], 'prob': 1}}, {'classId': 'r_26', 'type': 'linked', 'directed': False, 'entities': ['s1p6|e_23|13368,13517', 's1p6|e_6|13094,13173'], 'confidence': {'state': 'pre-added', 'who': ['user:slayerzeroa'], 'prob': 1}}, {'classId': 'r_26', 'type': 'linked', 'directed': False, 'entities': ['s1p6|e_23|13525,13600', 's1p6|e_6|13057,13091'], 'confidence': {'state': 'pre-added', 'who': ['user:slayerzeroa'], 'prob': 1}}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(annotations_legend)\n",
        "print(len(annotations_legend))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inWT5nIm6SVt",
        "outputId": "2fddcc3d-fa30-41f8-de3e-bb317f441234"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'e_23': 'SOLUTION', 'r_19': 'mz4f90xj280(e_6|e_4)', 'r_26': '5zjoi2gg5ps(e_6|e_23)', 'e_22': 'ACCIDENT', 'r_25': '6oyjvcblqfe(e_22|e_23)', 'e_6': 'IMPLIED', 'r_27': 'f19f8drqjkn(e_4|e_23)', 'e_4': 'CAUSE', 'r_24': 'yz4ctet7cda(e_4|e_22)'}\n",
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for rel in relation['relations']:\n",
        "  print(rel)"
      ],
      "metadata": {
        "id": "WRifGCdfiMTC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6996bd86-eefb-4c29-c7d9-82517aa7b818"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'classId': 'r_19', 'type': 'linked', 'directed': False, 'entities': ['s1p5|e_6|3704,3738', 's1p5|e_4|3607,3683'], 'confidence': {'state': 'pre-added', 'who': ['user:slayerzeroa'], 'prob': 1}}\n",
            "{'classId': 'r_19', 'type': 'linked', 'directed': False, 'entities': ['s1p5|e_6|3741,3820', 's1p5|e_4|3607,3683'], 'confidence': {'state': 'pre-added', 'who': ['user:slayerzeroa'], 'prob': 1}}\n",
            "{'classId': 'r_19', 'type': 'linked', 'directed': False, 'entities': ['s1p6|e_6|13057,13091', 's1p6|e_4|12960,13036'], 'confidence': {'state': 'pre-added', 'who': ['user:slayerzeroa'], 'prob': 1}}\n",
            "{'classId': 'r_19', 'type': 'linked', 'directed': False, 'entities': ['s1p6|e_6|13094,13173', 's1p6|e_4|12960,13036'], 'confidence': {'state': 'pre-added', 'who': ['user:slayerzeroa'], 'prob': 1}}\n",
            "{'classId': 'r_24', 'type': 'linked', 'directed': False, 'entities': ['s1p5|e_22|3394,3428', 's1p5|e_4|3607,3683'], 'confidence': {'state': 'pre-added', 'who': ['user:slayerzeroa'], 'prob': 1}}\n",
            "{'classId': 'r_24', 'type': 'linked', 'directed': False, 'entities': ['s1p5|e_22|3434,3448', 's1p5|e_4|3607,3683'], 'confidence': {'state': 'pre-added', 'who': ['user:slayerzeroa'], 'prob': 1}}\n",
            "{'classId': 'r_24', 'type': 'linked', 'directed': False, 'entities': ['s1p5|e_22|3455,3485', 's1p5|e_4|3607,3683'], 'confidence': {'state': 'pre-added', 'who': ['user:slayerzeroa'], 'prob': 1}}\n",
            "{'classId': 'r_26', 'type': 'linked', 'directed': False, 'entities': ['s1p6|e_23|13368,13517', 's1p6|e_6|13094,13173'], 'confidence': {'state': 'pre-added', 'who': ['user:slayerzeroa'], 'prob': 1}}\n",
            "{'classId': 'r_26', 'type': 'linked', 'directed': False, 'entities': ['s1p6|e_23|13525,13600', 's1p6|e_6|13057,13091'], 'confidence': {'state': 'pre-added', 'who': ['user:slayerzeroa'], 'prob': 1}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_relation_start(text):\n",
        "  text = text.split('|')[-1]\n",
        "  start = text.split(',')[0]\n",
        "  start = int(start)\n",
        "  return start"
      ],
      "metadata": {
        "id": "JFZwciSNIfr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_list = []\n",
        "loc_list = []\n",
        "cls_list = []\n",
        "part_list = []\n",
        "subject_list = []\n",
        "object_list = []\n",
        "label_list = []\n",
        "'''\n",
        "Loc : Sub, Obj 위치\n",
        "Cls : Sub, Obj 클래스\n",
        "Part : Sub, Obj html 상 문단\n",
        "Sub : Sub 내용\n",
        "Obj : Obj 내용\n",
        "Label : 관계 있으면 1 없으면 0\n",
        "Relation DataFrame Content\n",
        "'''\n",
        "for rel in relation['relations']:\n",
        "  subject = split_relation_start(rel['entities'][0])\n",
        "  subject_class = df.loc[df['Start']==subject]['Class'].values[0]\n",
        "  subject_part = df.loc[df['Start']==subject]['Part'].values[0]\n",
        "  subject_text = df.loc[df['Start']==subject]['Text'].values[0]\n",
        "  \n",
        "\n",
        "  object = split_relation_start(rel['entities'][1])\n",
        "  object_class = df.loc[df['Start']==object]['Class'].values[0]\n",
        "  object_part = df.loc[df['Start']==object]['Part'].values[0]\n",
        "  object_text = df.loc[df['Start']==object]['Text'].values[0]\n",
        "\n",
        "  loc = str(subject) + '_' + str(object)\n",
        "  cls = subject_class + '_' + object_class\n",
        "  part = subject_part + '_' + object_part\n",
        "\n",
        "  doc_list.append(doc_name)\n",
        "  loc_list.append(loc)\n",
        "  cls_list.append(cls)\n",
        "  part_list.append(part)\n",
        "  subject_list.append(subject_text)\n",
        "  object_list.append(object_text)\n",
        "  label_list.append(1)\n",
        "\n",
        "Rel_df = pd.DataFrame(zip(doc_list, loc_list, cls_list, part_list, subject_list, object_list, label_list))\n",
        "Rel_df.columns = ['Doc', 'Loc', 'Cls', 'Part', 'Sub', 'Obj', 'Label']"
      ],
      "metadata": {
        "id": "eQj7ZXmIbg_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Rel_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyrwunAPbwuF",
        "outputId": "9e967360-bb81-4c65-fccc-2e6a7c770e20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Doc          Loc       Cls       Part  \\\n",
            "0  sample    3704_3607   e_6_e_4  s1p5_s1p5   \n",
            "1  sample    3741_3607   e_6_e_4  s1p5_s1p5   \n",
            "2  sample  13057_12960   e_6_e_4  s1p6_s1p6   \n",
            "3  sample  13094_12960   e_6_e_4  s1p6_s1p6   \n",
            "4  sample    3394_3607  e_22_e_4  s1p5_s1p5   \n",
            "5  sample    3434_3607  e_22_e_4  s1p5_s1p5   \n",
            "6  sample    3455_3607  e_22_e_4  s1p5_s1p5   \n",
            "7  sample  13368_13094  e_23_e_6  s1p6_s1p6   \n",
            "8  sample  13525_13057  e_23_e_6  s1p6_s1p6   \n",
            "\n",
            "                                                 Sub  \\\n",
            "0                화차에 컨테이너를 싣고 내릴 때 차축 베어링에 충격이 가해진 것   \n",
            "1  차축 베어링의 분해검수 주기와 교환 주기가 화차에 적재되는 화 물의 종류, 적재방법...   \n",
            "2                화차에 컨테이너를 싣고 내릴 때 차축 베어링에 충격이 가해진 것   \n",
            "3  차축 베어링의 분해검수 주기와 교환 주기가 화차에 적재되는 화 물의 종류, 적재방법...   \n",
            "4                뒷 대차 첫 번째 차축과 두 번째 차축이 진행방향 좌측으로 탈선   \n",
            "5                                    좌 측 세 번째 차축이 절손   \n",
            "6                    탈선화차와 14번째 화차 사이의 공기호스 가 빠져있는 것   \n",
            "7  화차 차축 베어링의 분해검수 주기와 교환 주기를 화차에 적재되는 화 물의 종류, 적...   \n",
            "8  컨테이너를 화차에서 들어 올릴 때 화차가 함께 들어 올려지다 떨어지지 않도록 작업 ...   \n",
            "\n",
            "                                                 Obj  Label  \n",
            "0  차축에 설치되어 있는 차축 베어링 외륜의 내면과 롤러에 결함이 있는 상태에서 운행 ...      1  \n",
            "1  차축에 설치되어 있는 차축 베어링 외륜의 내면과 롤러에 결함이 있는 상태에서 운행 ...      1  \n",
            "2  차축에 설치되어 있는 차축 베어링 외륜의 내면과 롤러에 결함이 있는 상태에서 운행 ...      1  \n",
            "3  차축에 설치되어 있는 차축 베어링 외륜의 내면과 롤러에 결함이 있는 상태에서 운행 ...      1  \n",
            "4  차축에 설치되어 있는 차축 베어링 외륜의 내면과 롤러에 결함이 있는 상태에서 운행 ...      1  \n",
            "5  차축에 설치되어 있는 차축 베어링 외륜의 내면과 롤러에 결함이 있는 상태에서 운행 ...      1  \n",
            "6  차축에 설치되어 있는 차축 베어링 외륜의 내면과 롤러에 결함이 있는 상태에서 운행 ...      1  \n",
            "7  차축 베어링의 분해검수 주기와 교환 주기가 화차에 적재되는 화 물의 종류, 적재방법...      1  \n",
            "8                화차에 컨테이너를 싣고 내릴 때 차축 베어링에 충격이 가해진 것      1  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Google Spreadsheet 연동"
      ],
      "metadata": {
        "id": "W1maNDsXoMWm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NFvYoZtHkprb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gspread\n",
        "!pip install oauth2client"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o56W4VumoL1D",
        "outputId": "d932dab6-c22a-452a-d224-20d40357117b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gspread in /usr/local/lib/python3.7/dist-packages (3.4.2)\n",
            "Requirement already satisfied: requests>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from gspread) (2.23.0)\n",
            "Requirement already satisfied: google-auth in /usr/local/lib/python3.7/dist-packages (from gspread) (2.14.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.2.1->gspread) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.2.1->gspread) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.2.1->gspread) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.2.1->gspread) (3.0.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from google-auth->gspread) (1.15.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth->gspread) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth->gspread) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth->gspread) (5.2.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth->gspread) (0.4.8)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (4.1.3)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from oauth2client) (1.15.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from oauth2client) (4.9)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from oauth2client) (0.17.4)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client) (0.4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from oauth2client) (0.2.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "key = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "id": "CJyAMQE7obp2",
        "outputId": "c054f43a-658d-4700-a264-54b699e72ca2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ef79f9fe-b3c4-4490-bbfa-82a59ac36781\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ef79f9fe-b3c4-4490-bbfa-82a59ac36781\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving railway-tagtog-d24e6901ddb0.json to railway-tagtog-d24e6901ddb0.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gspread\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "\n",
        "scope = ['https://spreadsheets.google.com/feeds',\n",
        "         'https://www.googleapis.com/auth/drive']\n",
        "\n",
        "#json key file 위치\n",
        "json_file_name = './railway-tagtog-d24e6901ddb0.json'\n",
        "\n",
        "# json key file을 이용하여 접속\n",
        "credentials = ServiceAccountCredentials.from_json_keyfile_name(json_file_name, scope)\n",
        "gc = gspread.authorize(credentials)\n",
        "\n",
        "#구글 스프레드 시트 주소\n",
        "spreadsheet_url = \"https://docs.google.com/spreadsheets/d/1cpt2ad2pw0N2tC1H3MPtRAaZzFmgCh2If0gHOQKEIIo/edit#gid=0\"\n",
        "\n",
        "# 스프레드시트 문서 가져오기\n",
        "doc = gc.open_by_url(spreadsheet_url)\n",
        "## gc.create(spreadsheet_name) # 스프레드시트 생성\n",
        "\n",
        "\n",
        "#시트 선택하기\n",
        "sheet_name = \"annotation\"\n",
        "worksheet = doc.worksheet(sheet_name) #해당 시트가 있는 경우 불러오기\n",
        "## 403 error가 뜰 경우, google sheets API를 활성시켜줘야 함"
      ],
      "metadata": {
        "id": "sb4r11S1oUcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 시트의 모든 데이터 가져오기\n",
        "values = worksheet.get_all_values()\n",
        "header, rows = values[0], values[1:]\n",
        "data = pd.DataFrame(rows, columns=header)\n",
        "column_list = ['Doc', 'Loc', 'Cls', 'Part', 'Sub', 'Obj', 'Label']\n",
        "data = data[column_list]\n",
        "data.head()\n",
        "\n",
        "# 워크시트 범위 설정\n",
        "worksheet.resize(len(doc_list)+1,10)\n",
        "list_range = f\"a2:g{len(doc_list)+1}\"\n",
        "cell_list = worksheet.range(list_range)\n",
        "\n",
        "# 내용 작성\n",
        "for i in range(len(cell_list)//len(column_list)):\n",
        "    cell_list[(7*i)].value = doc_list[i]\n",
        "    cell_list[(7*i)+1].value = loc_list[i]\n",
        "    cell_list[(7*i)+2].value = cls_list[i]\n",
        "    cell_list[(7*i)+3].value = part_list[i]\n",
        "    cell_list[(7*i)+4].value = subject_list[i]\n",
        "    cell_list[(7*i)+5].value = object_list[i]\n",
        "    cell_list[(7*i)+6].value = label_list[i]\n",
        "    \n",
        "worksheet.update_cells(cell_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3BFRhYLoXmW",
        "outputId": "a02a3da6-7cdf-46e0-ada4-e348b9917caa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'spreadsheetId': '1cpt2ad2pw0N2tC1H3MPtRAaZzFmgCh2If0gHOQKEIIo',\n",
              " 'updatedRange': 'annotation!A2:G10',\n",
              " 'updatedRows': 9,\n",
              " 'updatedColumns': 7,\n",
              " 'updatedCells': 63}"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "70kubRiZpnhs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}